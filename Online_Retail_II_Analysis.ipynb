{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark Online Retail II Dataset Analysis\n",
        "\n",
        "This notebook demonstrates how to load and analyze the Online Retail II dataset using PySpark in Google Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Required Packages\n",
        "\n",
        "First, install PySpark and related dependencies in Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-4.0.1.tar.gz (434.2 MB)\n",
            "     ---------------------------------------- 0.0/434.2 MB ? eta -:--:--\n",
            "     --------------------------------------- 1.6/434.2 MB 10.3 MB/s eta 0:00:42\n",
            "     --------------------------------------- 3.9/434.2 MB 11.2 MB/s eta 0:00:39\n",
            "      -------------------------------------- 6.6/434.2 MB 11.4 MB/s eta 0:00:38\n",
            "      -------------------------------------- 8.9/434.2 MB 11.5 MB/s eta 0:00:37\n",
            "     - ------------------------------------ 11.5/434.2 MB 11.6 MB/s eta 0:00:37\n",
            "     - ------------------------------------ 13.9/434.2 MB 11.6 MB/s eta 0:00:37\n",
            "     - ------------------------------------ 16.3/434.2 MB 11.6 MB/s eta 0:00:36\n",
            "     - ------------------------------------ 18.6/434.2 MB 11.6 MB/s eta 0:00:36\n",
            "     - ------------------------------------ 21.2/434.2 MB 11.7 MB/s eta 0:00:36\n",
            "     -- ----------------------------------- 23.9/434.2 MB 11.7 MB/s eta 0:00:36\n",
            "     -- ----------------------------------- 26.2/434.2 MB 11.7 MB/s eta 0:00:35\n",
            "     -- ----------------------------------- 28.6/434.2 MB 11.7 MB/s eta 0:00:35\n",
            "     -- ----------------------------------- 30.9/434.2 MB 11.7 MB/s eta 0:00:35\n",
            "     -- ----------------------------------- 33.0/434.2 MB 11.7 MB/s eta 0:00:35\n",
            "     --- ---------------------------------- 35.7/434.2 MB 11.6 MB/s eta 0:00:35\n",
            "     --- ---------------------------------- 38.0/434.2 MB 11.6 MB/s eta 0:00:35\n",
            "     --- ---------------------------------- 40.6/434.2 MB 11.6 MB/s eta 0:00:34\n",
            "     --- ---------------------------------- 43.0/434.2 MB 11.6 MB/s eta 0:00:34\n",
            "     --- ---------------------------------- 45.4/434.2 MB 11.6 MB/s eta 0:00:34\n",
            "     ---- --------------------------------- 47.7/434.2 MB 11.6 MB/s eta 0:00:34\n",
            "     ---- --------------------------------- 50.3/434.2 MB 11.6 MB/s eta 0:00:34\n",
            "     ---- --------------------------------- 52.7/434.2 MB 11.6 MB/s eta 0:00:33\n",
            "     ---- --------------------------------- 55.3/434.2 MB 11.6 MB/s eta 0:00:33\n",
            "     ----- -------------------------------- 57.9/434.2 MB 11.6 MB/s eta 0:00:33\n",
            "     ----- -------------------------------- 60.3/434.2 MB 11.6 MB/s eta 0:00:33\n",
            "     ----- -------------------------------- 62.9/434.2 MB 11.6 MB/s eta 0:00:32\n",
            "     ----- -------------------------------- 65.3/434.2 MB 11.6 MB/s eta 0:00:32\n",
            "     ----- -------------------------------- 67.6/434.2 MB 11.6 MB/s eta 0:00:32\n",
            "     ------ ------------------------------- 70.0/434.2 MB 11.6 MB/s eta 0:00:32\n",
            "     ------ ------------------------------- 72.6/434.2 MB 11.6 MB/s eta 0:00:32\n",
            "     ------ ------------------------------- 75.0/434.2 MB 11.6 MB/s eta 0:00:31\n",
            "     ------ ------------------------------- 77.6/434.2 MB 11.6 MB/s eta 0:00:31\n",
            "     ------ ------------------------------- 80.0/434.2 MB 11.6 MB/s eta 0:00:31\n",
            "     ------- ------------------------------ 82.6/434.2 MB 11.6 MB/s eta 0:00:31\n",
            "     ------- ------------------------------ 84.9/434.2 MB 11.6 MB/s eta 0:00:31\n",
            "     ------- ------------------------------ 87.6/434.2 MB 11.6 MB/s eta 0:00:30\n",
            "     ------- ------------------------------ 89.9/434.2 MB 11.6 MB/s eta 0:00:30\n",
            "     -------- ----------------------------- 92.3/434.2 MB 11.6 MB/s eta 0:00:30\n",
            "     -------- ----------------------------- 94.9/434.2 MB 11.6 MB/s eta 0:00:30\n",
            "     -------- ----------------------------- 97.5/434.2 MB 11.6 MB/s eta 0:00:29\n",
            "     -------- ---------------------------- 100.1/434.2 MB 11.6 MB/s eta 0:00:29\n",
            "     -------- ---------------------------- 102.2/434.2 MB 11.7 MB/s eta 0:00:29\n",
            "     -------- ---------------------------- 104.9/434.2 MB 11.6 MB/s eta 0:00:29\n",
            "     --------- --------------------------- 107.5/434.2 MB 11.6 MB/s eta 0:00:29\n",
            "     --------- --------------------------- 109.8/434.2 MB 11.7 MB/s eta 0:00:28\n",
            "     --------- --------------------------- 112.5/434.2 MB 11.7 MB/s eta 0:00:28\n",
            "     --------- --------------------------- 114.8/434.2 MB 11.7 MB/s eta 0:00:28\n",
            "     --------- --------------------------- 117.2/434.2 MB 11.7 MB/s eta 0:00:28\n",
            "     ---------- -------------------------- 119.5/434.2 MB 11.7 MB/s eta 0:00:27\n",
            "     ---------- -------------------------- 122.2/434.2 MB 11.7 MB/s eta 0:00:27\n",
            "     ---------- -------------------------- 124.5/434.2 MB 11.7 MB/s eta 0:00:27\n",
            "     ---------- -------------------------- 127.1/434.2 MB 11.7 MB/s eta 0:00:27\n",
            "     ----------- ------------------------- 129.8/434.2 MB 11.7 MB/s eta 0:00:27\n",
            "     ----------- ------------------------- 132.1/434.2 MB 11.7 MB/s eta 0:00:26\n",
            "     ----------- ------------------------- 134.7/434.2 MB 11.7 MB/s eta 0:00:26\n",
            "     ----------- ------------------------- 137.1/434.2 MB 11.7 MB/s eta 0:00:26\n",
            "     ----------- ------------------------- 139.2/434.2 MB 11.7 MB/s eta 0:00:26\n",
            "     ------------ ------------------------ 141.6/434.2 MB 11.7 MB/s eta 0:00:26\n",
            "     ------------ ------------------------ 143.9/434.2 MB 11.7 MB/s eta 0:00:25\n",
            "     ------------ ------------------------ 146.5/434.2 MB 11.7 MB/s eta 0:00:25\n",
            "     ------------ ------------------------ 148.9/434.2 MB 11.6 MB/s eta 0:00:25\n",
            "     ------------ ------------------------ 151.0/434.2 MB 11.6 MB/s eta 0:00:25\n",
            "     ------------- ----------------------- 153.6/434.2 MB 11.6 MB/s eta 0:00:25\n",
            "     ------------- ----------------------- 156.0/434.2 MB 11.6 MB/s eta 0:00:24\n",
            "     ------------- ----------------------- 158.1/434.2 MB 11.6 MB/s eta 0:00:24\n",
            "     ------------- ----------------------- 160.7/434.2 MB 11.6 MB/s eta 0:00:24\n",
            "     ------------- ----------------------- 163.1/434.2 MB 11.6 MB/s eta 0:00:24\n",
            "     -------------- ---------------------- 165.4/434.2 MB 11.6 MB/s eta 0:00:24\n",
            "     -------------- ---------------------- 168.0/434.2 MB 11.6 MB/s eta 0:00:23\n",
            "     -------------- ---------------------- 170.4/434.2 MB 11.6 MB/s eta 0:00:23\n",
            "     -------------- ---------------------- 172.8/434.2 MB 11.6 MB/s eta 0:00:23\n",
            "     -------------- ---------------------- 175.4/434.2 MB 11.6 MB/s eta 0:00:23\n",
            "     --------------- --------------------- 177.7/434.2 MB 11.6 MB/s eta 0:00:23\n",
            "     --------------- --------------------- 180.4/434.2 MB 11.6 MB/s eta 0:00:22\n",
            "     --------------- --------------------- 182.7/434.2 MB 11.6 MB/s eta 0:00:22\n",
            "     --------------- --------------------- 185.3/434.2 MB 11.6 MB/s eta 0:00:22\n",
            "     --------------- --------------------- 187.7/434.2 MB 11.7 MB/s eta 0:00:22\n",
            "     ---------------- -------------------- 190.1/434.2 MB 11.7 MB/s eta 0:00:21\n",
            "     ---------------- -------------------- 192.7/434.2 MB 11.7 MB/s eta 0:00:21\n",
            "     ---------------- -------------------- 195.0/434.2 MB 11.7 MB/s eta 0:00:21\n",
            "     ---------------- -------------------- 197.7/434.2 MB 11.7 MB/s eta 0:00:21\n",
            "     ----------------- ------------------- 200.3/434.2 MB 11.7 MB/s eta 0:00:21\n",
            "     ----------------- ------------------- 202.9/434.2 MB 11.7 MB/s eta 0:00:20\n",
            "     ----------------- ------------------- 205.0/434.2 MB 11.6 MB/s eta 0:00:20\n",
            "     ----------------- ------------------- 207.6/434.2 MB 11.7 MB/s eta 0:00:20\n",
            "     ----------------- ------------------- 209.7/434.2 MB 11.6 MB/s eta 0:00:20\n",
            "     ------------------ ------------------ 212.3/434.2 MB 11.7 MB/s eta 0:00:20\n",
            "     ------------------ ------------------ 215.0/434.2 MB 11.7 MB/s eta 0:00:19\n",
            "     ------------------ ------------------ 217.3/434.2 MB 11.7 MB/s eta 0:00:19\n",
            "     ------------------ ------------------ 219.7/434.2 MB 11.7 MB/s eta 0:00:19\n",
            "     ------------------ ------------------ 222.3/434.2 MB 11.7 MB/s eta 0:00:19\n",
            "     ------------------- ----------------- 224.7/434.2 MB 11.7 MB/s eta 0:00:18\n",
            "     ------------------- ----------------- 227.3/434.2 MB 11.7 MB/s eta 0:00:18\n",
            "     ------------------- ----------------- 229.9/434.2 MB 11.7 MB/s eta 0:00:18\n",
            "     ------------------- ----------------- 232.3/434.2 MB 11.7 MB/s eta 0:00:18\n",
            "     ------------------- ----------------- 234.6/434.2 MB 11.7 MB/s eta 0:00:18\n",
            "     -------------------- ---------------- 237.2/434.2 MB 11.7 MB/s eta 0:00:17\n",
            "     -------------------- ---------------- 239.6/434.2 MB 11.7 MB/s eta 0:00:17\n",
            "     -------------------- ---------------- 242.2/434.2 MB 11.7 MB/s eta 0:00:17\n",
            "     -------------------- ---------------- 244.8/434.2 MB 11.7 MB/s eta 0:00:17\n",
            "     --------------------- --------------- 247.2/434.2 MB 11.7 MB/s eta 0:00:17\n",
            "     --------------------- --------------- 249.8/434.2 MB 11.7 MB/s eta 0:00:16\n",
            "     --------------------- --------------- 252.2/434.2 MB 11.7 MB/s eta 0:00:16\n",
            "     --------------------- --------------- 254.8/434.2 MB 11.7 MB/s eta 0:00:16\n",
            "     --------------------- --------------- 257.2/434.2 MB 11.7 MB/s eta 0:00:16\n",
            "     ---------------------- -------------- 259.5/434.2 MB 11.7 MB/s eta 0:00:15\n",
            "     ---------------------- -------------- 261.9/434.2 MB 11.7 MB/s eta 0:00:15\n",
            "     ---------------------- -------------- 264.5/434.2 MB 11.7 MB/s eta 0:00:15\n",
            "     ---------------------- -------------- 266.6/434.2 MB 11.7 MB/s eta 0:00:15\n",
            "     ---------------------- -------------- 269.0/434.2 MB 11.7 MB/s eta 0:00:15\n",
            "     ----------------------- ------------- 271.6/434.2 MB 11.7 MB/s eta 0:00:14\n",
            "     ----------------------- ------------- 273.9/434.2 MB 11.7 MB/s eta 0:00:14\n",
            "     ----------------------- ------------- 276.3/434.2 MB 11.6 MB/s eta 0:00:14\n",
            "     ----------------------- ------------- 278.7/434.2 MB 11.6 MB/s eta 0:00:14\n",
            "     ----------------------- ------------- 280.8/434.2 MB 11.6 MB/s eta 0:00:14\n",
            "     ------------------------ ------------ 283.4/434.2 MB 11.6 MB/s eta 0:00:13\n",
            "     ------------------------ ------------ 285.7/434.2 MB 11.6 MB/s eta 0:00:13\n",
            "     ------------------------ ------------ 288.1/434.2 MB 11.6 MB/s eta 0:00:13\n",
            "     ------------------------ ------------ 290.2/434.2 MB 11.6 MB/s eta 0:00:13\n",
            "     ------------------------ ------------ 292.8/434.2 MB 11.6 MB/s eta 0:00:13\n",
            "     ------------------------- ----------- 295.4/434.2 MB 11.6 MB/s eta 0:00:12\n",
            "     ------------------------- ----------- 297.8/434.2 MB 11.6 MB/s eta 0:00:12\n",
            "     ------------------------- ----------- 300.4/434.2 MB 11.6 MB/s eta 0:00:12\n",
            "     ------------------------- ----------- 302.8/434.2 MB 11.6 MB/s eta 0:00:12\n",
            "     -------------------------- ---------- 305.1/434.2 MB 11.6 MB/s eta 0:00:12\n",
            "     -------------------------- ---------- 307.8/434.2 MB 11.6 MB/s eta 0:00:11\n",
            "     -------------------------- ---------- 310.4/434.2 MB 11.6 MB/s eta 0:00:11\n",
            "     -------------------------- ---------- 312.5/434.2 MB 11.6 MB/s eta 0:00:11\n",
            "     -------------------------- ---------- 315.1/434.2 MB 11.6 MB/s eta 0:00:11\n",
            "     --------------------------- --------- 317.5/434.2 MB 11.6 MB/s eta 0:00:11\n",
            "     --------------------------- --------- 319.8/434.2 MB 11.6 MB/s eta 0:00:10\n",
            "     --------------------------- --------- 322.4/434.2 MB 11.6 MB/s eta 0:00:10\n",
            "     --------------------------- --------- 324.8/434.2 MB 11.6 MB/s eta 0:00:10\n",
            "     --------------------------- --------- 327.4/434.2 MB 11.6 MB/s eta 0:00:10\n",
            "     ---------------------------- -------- 329.8/434.2 MB 11.6 MB/s eta 0:00:09\n",
            "     ---------------------------- -------- 332.1/434.2 MB 11.6 MB/s eta 0:00:09\n",
            "     ---------------------------- -------- 334.8/434.2 MB 11.6 MB/s eta 0:00:09\n",
            "     ---------------------------- -------- 337.1/434.2 MB 11.6 MB/s eta 0:00:09\n",
            "     ---------------------------- -------- 339.5/434.2 MB 11.6 MB/s eta 0:00:09\n",
            "     ----------------------------- ------- 342.1/434.2 MB 11.6 MB/s eta 0:00:08\n",
            "     ----------------------------- ------- 344.5/434.2 MB 11.6 MB/s eta 0:00:08\n",
            "     ----------------------------- ------- 347.1/434.2 MB 11.6 MB/s eta 0:00:08\n",
            "     ----------------------------- ------- 349.4/434.2 MB 11.6 MB/s eta 0:00:08\n",
            "     ----------------------------- ------- 351.8/434.2 MB 11.6 MB/s eta 0:00:08\n",
            "     ------------------------------ ------ 354.2/434.2 MB 11.6 MB/s eta 0:00:07\n",
            "     ------------------------------ ------ 356.5/434.2 MB 11.6 MB/s eta 0:00:07\n",
            "     ------------------------------ ------ 358.9/434.2 MB 11.6 MB/s eta 0:00:07\n",
            "     ------------------------------ ------ 361.5/434.2 MB 11.6 MB/s eta 0:00:07\n",
            "     ------------------------------- ----- 363.9/434.2 MB 11.6 MB/s eta 0:00:07\n",
            "     ------------------------------- ----- 366.5/434.2 MB 11.6 MB/s eta 0:00:06\n",
            "     ------------------------------- ----- 368.8/434.2 MB 11.6 MB/s eta 0:00:06\n",
            "     ------------------------------- ----- 371.5/434.2 MB 11.6 MB/s eta 0:00:06\n",
            "     ------------------------------- ----- 373.8/434.2 MB 11.6 MB/s eta 0:00:06\n",
            "     -------------------------------- ---- 376.4/434.2 MB 11.6 MB/s eta 0:00:05\n",
            "     -------------------------------- ---- 378.8/434.2 MB 11.6 MB/s eta 0:00:05\n",
            "     -------------------------------- ---- 381.2/434.2 MB 11.6 MB/s eta 0:00:05\n",
            "     -------------------------------- ---- 383.8/434.2 MB 11.6 MB/s eta 0:00:05\n",
            "     -------------------------------- ---- 386.1/434.2 MB 11.6 MB/s eta 0:00:05\n",
            "     --------------------------------- --- 388.8/434.2 MB 11.6 MB/s eta 0:00:04\n",
            "     --------------------------------- --- 391.1/434.2 MB 11.6 MB/s eta 0:00:04\n",
            "     --------------------------------- --- 393.7/434.2 MB 11.6 MB/s eta 0:00:04\n",
            "     --------------------------------- --- 396.4/434.2 MB 11.6 MB/s eta 0:00:04\n",
            "     --------------------------------- --- 398.7/434.2 MB 11.6 MB/s eta 0:00:04\n",
            "     ---------------------------------- -- 401.1/434.2 MB 11.6 MB/s eta 0:00:03\n",
            "     ---------------------------------- -- 403.7/434.2 MB 11.6 MB/s eta 0:00:03\n",
            "     ---------------------------------- -- 406.3/434.2 MB 11.6 MB/s eta 0:00:03\n",
            "     ---------------------------------- -- 408.9/434.2 MB 11.6 MB/s eta 0:00:03\n",
            "     ----------------------------------- - 411.3/434.2 MB 11.6 MB/s eta 0:00:02\n",
            "     ----------------------------------- - 413.7/434.2 MB 11.6 MB/s eta 0:00:02\n",
            "     ----------------------------------- - 416.3/434.2 MB 11.6 MB/s eta 0:00:02\n",
            "     ----------------------------------- - 418.6/434.2 MB 11.6 MB/s eta 0:00:02\n",
            "     ----------------------------------- - 421.3/434.2 MB 11.6 MB/s eta 0:00:02\n",
            "     ------------------------------------  423.9/434.2 MB 11.6 MB/s eta 0:00:01\n",
            "     ------------------------------------  426.5/434.2 MB 11.6 MB/s eta 0:00:01\n",
            "     ------------------------------------  428.9/434.2 MB 11.6 MB/s eta 0:00:01\n",
            "     ------------------------------------  431.5/434.2 MB 11.6 MB/s eta 0:00:01\n",
            "     ------------------------------------  434.1/434.2 MB 11.6 MB/s eta 0:00:01\n",
            "     ------------------------------------  434.1/434.2 MB 11.6 MB/s eta 0:00:01\n",
            "     ------------------------------------  434.1/434.2 MB 11.6 MB/s eta 0:00:01\n",
            "     ------------------------------------- 434.2/434.2 MB 11.4 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: pandas in d:\\anaconda3\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: openpyxl in d:\\anaconda3\\lib\\site-packages (3.1.5)\n",
            "Collecting py4j==0.10.9.9 (from pyspark)\n",
            "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in d:\\anaconda3\\lib\\site-packages (from pandas) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in d:\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in d:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py): started\n",
            "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
            "  Created wheel for pyspark: filename=pyspark-4.0.1-py2.py3-none-any.whl size=434813833 sha256=3d425cf4983fecd74912d8ba6466cf7392e429af8ba44285ee278761ec6b7374\n",
            "  Stored in directory: c:\\users\\87190\\appdata\\local\\pip\\cache\\wheels\\00\\e3\\92\\8594f4cee2c9fd4ad82fe85e4bf2559ab8ea84ef19b1dd3d15\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   -------------------- ------------------- 1/2 [pyspark]\n",
            "   ---------------------------------------- 2/2 [pyspark]\n",
            "\n",
            "Successfully installed py4j-0.10.9.9 pyspark-4.0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  DEPRECATION: Building 'pyspark' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pyspark'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install pyspark pandas openpyxl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries and Initialize Spark Session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session initialized successfully!\n",
            "Spark version: 4.0.1\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col, sum as spark_sum, count, when, isnan, isnull, desc, min as spark_min, max as spark_max\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize Spark session\n",
        "# Configure Spark for both local and Colab environments\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"OnlineRetailAnalysis\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
        "    .config(\"spark.python.worker.timeout\", \"300\") \\\n",
        "    .config(\"spark.python.worker.reuse\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce output noise\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"Spark session initialized successfully!\")\n",
        "print(f\"Spark version: {spark.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Data from GitHub\n",
        "\n",
        "Since PySpark cannot directly read Excel files, we use pandas to read from GitHub and then convert to Spark DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading Excel file from GitHub...\n",
            "Data successfully loaded from GitHub into Spark DataFrame!\n"
          ]
        }
      ],
      "source": [
        "# Use pandas to read Excel file from GitHub\n",
        "print(\"Reading Excel file from GitHub...\")\n",
        "\n",
        "# GitHub repository information\n",
        "github_user = \"Hachi630\"\n",
        "github_repo = \"BDAS\"\n",
        "file_path = \"online_retail_II.xlsx\"\n",
        "\n",
        "# Construct GitHub raw URL\n",
        "github_url = f\"https://raw.githubusercontent.com/{github_user}/{github_repo}/main/{file_path}\"\n",
        "\n",
        "# Use pandas to read Excel file from GitHub\n",
        "pandas_df = pd.read_excel(github_url)\n",
        "\n",
        "# Convert pandas DataFrame to Spark DataFrame\n",
        "# Ensure DataFrame is named df for consistency\n",
        "df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "print(\"Data successfully loaded from GitHub into Spark DataFrame!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Check Data Dimensions\n",
        "\n",
        "Determine the number of rows and columns in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Data Dimension Information ===\n",
            "Error getting row count with Spark: An error occurred while calling o55.count.\n",
            ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 1.0 failed 1 times, most recent failure: Lost task 7.0 in stage 1.0 (TID 27) (windows10.microdone.cn executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\n",
            "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\n",
            "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\n",
            "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\n",
            "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\n",
            "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
            "Caused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\n",
            "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\n",
            "\t... 34 more\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n",
            "\tat scala.Option.getOrElse(Option.scala:201)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n",
            "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n",
            "\tat scala.Option.foreach(Option.scala:437)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
            "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\n",
            "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\n",
            "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\n",
            "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\n",
            "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\n",
            "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
            "Caused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\n",
            "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\n",
            "\t... 34 more\n",
            "\n",
            "Using pandas DataFrame for row count...\n",
            "Dataset row count (from pandas): 525,461\n",
            "Dataset column count: 8\n",
            "Column names: ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country']\n"
          ]
        }
      ],
      "source": [
        "# Check data dimensions\n",
        "print(\"=== Data Dimension Information ===\")\n",
        "\n",
        "# Get row count with error handling\n",
        "try:\n",
        "    row_count = df.count()\n",
        "    print(f\"Dataset row count: {row_count:,}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error getting row count with Spark: {e}\")\n",
        "    print(\"Using pandas DataFrame for row count...\")\n",
        "    row_count = len(pandas_df)\n",
        "    print(f\"Dataset row count (from pandas): {row_count:,}\")\n",
        "\n",
        "# Get column count\n",
        "column_count = len(df.columns)\n",
        "print(f\"Dataset column count: {column_count}\")\n",
        "\n",
        "# Display column names\n",
        "print(f\"Column names: {df.columns}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Preview Data\n",
        "\n",
        "Display the first few rows to understand the content structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview data - show first 5 rows\n",
        "print(\"=== Data Preview (First 5 Rows) ===\")\n",
        "df.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Schema\n",
        "\n",
        "Print the DataFrame schema to verify data types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print data schema to verify data types\n",
        "print(\"=== Data Schema ===\")\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Statistical Summary of Numeric Columns\n",
        "\n",
        "Get basic statistical information for numeric columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display basic statistical summary for numeric columns\n",
        "print(\"=== Numeric Columns Statistical Summary ===\")\n",
        "# Use describe() method to get statistical information for numeric columns\n",
        "df.describe().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional statistical information - use summary() method for more detailed statistics\n",
        "print(\"=== Detailed Statistical Summary ===\")\n",
        "df.summary().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Missing Values Check\n",
        "\n",
        "Check for missing values in the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"=== Missing Values Check ===\")\n",
        "\n",
        "# Calculate missing value count for each column\n",
        "missing_values = df.select([spark_sum(when(isnull(c) | isnan(c), 1).otherwise(0)).alias(c) for c in df.columns])\n",
        "missing_values.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Specific Column Analysis\n",
        "\n",
        "Analyze special cases in Quantity and UnitPrice columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check negative values in Quantity column (returns)\n",
        "print(\"=== Quantity Column Analysis ===\")\n",
        "\n",
        "quantity_stats = df.select(\n",
        "    spark_min(\"Quantity\").alias(\"Min Quantity\"),\n",
        "    spark_max(\"Quantity\").alias(\"Max Quantity\"),\n",
        "    count(when(col(\"Quantity\") < 0, 1)).alias(\"Return Records Count\"),\n",
        "    count(when(col(\"Quantity\") > 0, 1)).alias(\"Normal Sales Records Count\")\n",
        ")\n",
        "quantity_stats.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check UnitPrice column range\n",
        "print(\"=== UnitPrice Column Analysis ===\")\n",
        "\n",
        "price_stats = df.select(\n",
        "    spark_min(\"UnitPrice\").alias(\"Min Unit Price\"),\n",
        "    spark_max(\"UnitPrice\").alias(\"Max Unit Price\"),\n",
        "    count(when(col(\"UnitPrice\") < 0, 1)).alias(\"Negative Price Records Count\"),\n",
        "    count(when(col(\"UnitPrice\") == 0, 1)).alias(\"Zero Price Records Count\")\n",
        ")\n",
        "price_stats.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Group Analysis\n",
        "\n",
        "Perform group analysis by country and customer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display record counts by country\n",
        "print(\"=== Record Count by Country (Top 10) ===\")\n",
        "df.groupBy(\"Country\").count().orderBy(desc(\"count\")).show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display record counts by customer\n",
        "print(\"=== Record Count by Customer (Top 10) ===\")\n",
        "df.groupBy(\"Customer ID\").count().orderBy(desc(\"count\")).show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary\n",
        "\n",
        "Dataset basic information summary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Analysis Complete ===\")\n",
        "print(\"Dataset basic information summary:\")\n",
        "print(f\"- Total records: {row_count:,}\")\n",
        "print(f\"- Column count: {column_count}\")\n",
        "print(f\"- Main columns: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, Customer ID, Country\")\n",
        "print(\"- Data types verified through printSchema()\")\n",
        "print(\"- Statistical summary shows distribution of numeric columns\")\n",
        "print(\"- Missing values and anomalies checked\")\n",
        "\n",
        "# Stop Spark session (optional, usually not needed in Colab)\n",
        "# spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative: Pandas-Only Analysis\n",
        "\n",
        "If PySpark encounters issues in your local environment, you can use this pandas-only version for analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative pandas-only analysis (use if PySpark fails)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data using pandas from GitHub\n",
        "print(\"=== Loading Data with Pandas from GitHub ===\")\n",
        "\n",
        "# GitHub repository information\n",
        "github_user = \"Hachi630\"\n",
        "github_repo = \"BDAS\"\n",
        "file_path = \"online_retail_II.xlsx\"\n",
        "\n",
        "# Construct GitHub raw URL\n",
        "github_url = f\"https://raw.githubusercontent.com/{github_user}/{github_repo}/main/{file_path}\"\n",
        "\n",
        "# Use pandas to read Excel file from GitHub\n",
        "pandas_df = pd.read_excel(github_url)\n",
        "print(\"Data successfully loaded from GitHub!\")\n",
        "\n",
        "# Check data dimensions\n",
        "print(\"\\n=== Data Dimension Information ===\")\n",
        "row_count = len(pandas_df)\n",
        "column_count = len(pandas_df.columns)\n",
        "print(f\"Dataset row count: {row_count:,}\")\n",
        "print(f\"Dataset column count: {column_count}\")\n",
        "print(f\"Column names: {list(pandas_df.columns)}\")\n",
        "\n",
        "# Preview data\n",
        "print(\"\\n=== Data Preview (First 5 Rows) ===\")\n",
        "print(pandas_df.head())\n",
        "\n",
        "# Data types\n",
        "print(\"\\n=== Data Schema ===\")\n",
        "print(pandas_df.dtypes)\n",
        "\n",
        "# Statistical summary\n",
        "print(\"\\n=== Statistical Summary ===\")\n",
        "print(pandas_df.describe())\n",
        "\n",
        "# Missing values\n",
        "print(\"\\n=== Missing Values Check ===\")\n",
        "print(pandas_df.isnull().sum())\n",
        "\n",
        "# Quantity analysis\n",
        "print(\"\\n=== Quantity Column Analysis ===\")\n",
        "print(f\"Min Quantity: {pandas_df['Quantity'].min()}\")\n",
        "print(f\"Max Quantity: {pandas_df['Quantity'].max()}\")\n",
        "print(f\"Return Records Count: {(pandas_df['Quantity'] < 0).sum()}\")\n",
        "print(f\"Normal Sales Records Count: {(pandas_df['Quantity'] > 0).sum()}\")\n",
        "\n",
        "# UnitPrice analysis\n",
        "print(\"\\n=== UnitPrice Column Analysis ===\")\n",
        "print(f\"Min Unit Price: {pandas_df['UnitPrice'].min()}\")\n",
        "print(f\"Max Unit Price: {pandas_df['UnitPrice'].max()}\")\n",
        "print(f\"Negative Price Records Count: {(pandas_df['UnitPrice'] < 0).sum()}\")\n",
        "print(f\"Zero Price Records Count: {(pandas_df['UnitPrice'] == 0).sum()}\")\n",
        "\n",
        "# Group analysis\n",
        "print(\"\\n=== Record Count by Country (Top 10) ===\")\n",
        "print(pandas_df['Country'].value_counts().head(10))\n",
        "\n",
        "print(\"\\n=== Record Count by Customer (Top 10) ===\")\n",
        "print(pandas_df['Customer ID'].value_counts().head(10))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
