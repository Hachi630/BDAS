{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark Online Retail II Dataset Analysis\n",
        "\n",
        "This notebook demonstrates how to load and analyze the Online Retail II dataset using PySpark in Google Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Required Packages\n",
        "\n",
        "First, install PySpark and related dependencies in Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install pyspark pandas openpyxl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries and Initialize Spark Session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col, sum as spark_sum, count, when, isnan, isnull, desc, min as spark_min, max as spark_max\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize Spark session\n",
        "# In Google Colab, we need to set some configurations to ensure Spark works properly\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"OnlineRetailAnalysis\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce output noise\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"Spark session initialized successfully!\")\n",
        "print(f\"Spark version: {spark.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Data\n",
        "\n",
        "Since PySpark cannot directly read Excel files, we use pandas to read and then convert to Spark DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use pandas to read Excel file\n",
        "print(\"Reading Excel file...\")\n",
        "pandas_df = pd.read_excel('online_retail_II.xlsx')\n",
        "\n",
        "# Convert pandas DataFrame to Spark DataFrame\n",
        "# Ensure DataFrame is named df for consistency\n",
        "df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "print(\"Data successfully loaded into Spark DataFrame!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Check Data Dimensions\n",
        "\n",
        "Determine the number of rows and columns in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check data dimensions\n",
        "print(\"=== Data Dimension Information ===\")\n",
        "\n",
        "# Get row count\n",
        "row_count = df.count()\n",
        "print(f\"Dataset row count: {row_count:,}\")\n",
        "\n",
        "# Get column count\n",
        "column_count = len(df.columns)\n",
        "print(f\"Dataset column count: {column_count}\")\n",
        "\n",
        "# Display column names\n",
        "print(f\"Column names: {df.columns}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Preview Data\n",
        "\n",
        "Display the first few rows to understand the content structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview data - show first 5 rows\n",
        "print(\"=== Data Preview (First 5 Rows) ===\")\n",
        "df.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Schema\n",
        "\n",
        "Print the DataFrame schema to verify data types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print data schema to verify data types\n",
        "print(\"=== Data Schema ===\")\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Statistical Summary of Numeric Columns\n",
        "\n",
        "Get basic statistical information for numeric columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display basic statistical summary for numeric columns\n",
        "print(\"=== Numeric Columns Statistical Summary ===\")\n",
        "# Use describe() method to get statistical information for numeric columns\n",
        "df.describe().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional statistical information - use summary() method for more detailed statistics\n",
        "print(\"=== Detailed Statistical Summary ===\")\n",
        "df.summary().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Missing Values Check\n",
        "\n",
        "Check for missing values in the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"=== Missing Values Check ===\")\n",
        "\n",
        "# Calculate missing value count for each column\n",
        "missing_values = df.select([spark_sum(when(isnull(c) | isnan(c), 1).otherwise(0)).alias(c) for c in df.columns])\n",
        "missing_values.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Specific Column Analysis\n",
        "\n",
        "Analyze special cases in Quantity and UnitPrice columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check negative values in Quantity column (returns)\n",
        "print(\"=== Quantity Column Analysis ===\")\n",
        "\n",
        "quantity_stats = df.select(\n",
        "    spark_min(\"Quantity\").alias(\"Min Quantity\"),\n",
        "    spark_max(\"Quantity\").alias(\"Max Quantity\"),\n",
        "    count(when(col(\"Quantity\") < 0, 1)).alias(\"Return Records Count\"),\n",
        "    count(when(col(\"Quantity\") > 0, 1)).alias(\"Normal Sales Records Count\")\n",
        ")\n",
        "quantity_stats.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check UnitPrice column range\n",
        "print(\"=== UnitPrice Column Analysis ===\")\n",
        "\n",
        "price_stats = df.select(\n",
        "    spark_min(\"UnitPrice\").alias(\"Min Unit Price\"),\n",
        "    spark_max(\"UnitPrice\").alias(\"Max Unit Price\"),\n",
        "    count(when(col(\"UnitPrice\") < 0, 1)).alias(\"Negative Price Records Count\"),\n",
        "    count(when(col(\"UnitPrice\") == 0, 1)).alias(\"Zero Price Records Count\")\n",
        ")\n",
        "price_stats.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Group Analysis\n",
        "\n",
        "Perform group analysis by country and customer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display record counts by country\n",
        "print(\"=== Record Count by Country (Top 10) ===\")\n",
        "df.groupBy(\"Country\").count().orderBy(desc(\"count\")).show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display record counts by customer\n",
        "print(\"=== Record Count by Customer (Top 10) ===\")\n",
        "df.groupBy(\"Customer ID\").count().orderBy(desc(\"count\")).show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary\n",
        "\n",
        "Dataset basic information summary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Analysis Complete ===\")\n",
        "print(\"Dataset basic information summary:\")\n",
        "print(f\"- Total records: {row_count:,}\")\n",
        "print(f\"- Column count: {column_count}\")\n",
        "print(f\"- Main columns: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, Customer ID, Country\")\n",
        "print(\"- Data types verified through printSchema()\")\n",
        "print(\"- Statistical summary shows distribution of numeric columns\")\n",
        "print(\"- Missing values and anomalies checked\")\n",
        "\n",
        "# Stop Spark session (optional, usually not needed in Colab)\n",
        "# spark.stop()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
