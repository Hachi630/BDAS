{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd284f1",
   "metadata": {},
   "source": [
    "# PySpark Online Retail II Dataset Analysis\n",
    "\n",
    "This notebook demonstrates how to load and analyze the Online Retail II dataset using PySpark in Google Colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5c825",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n",
    "\n",
    "First, install PySpark and related dependencies in Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "862c8810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in d:\\python\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: pandas in d:\\python\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: openpyxl in d:\\python\\lib\\site-packages (3.1.3)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in d:\\python\\lib\\site-packages (from pyspark) (0.10.9.9)\n",
      "Requirement already satisfied: numpy>=1.22.4 in d:\\python\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\python\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\python\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in d:\\python\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install pyspark pandas openpyxl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa7021",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Initialize Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3af7c7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized successfully!\n",
      "Spark version: 4.0.1\n",
      "Spark UI: http://windows10.microdone.cn:4040\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, sum as spark_sum, count, when, isnan, isnull, desc, min as spark_min, max as spark_max\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set environment variables for Windows\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python'\n",
    "\n",
    "# Initialize Spark session with enhanced configuration for Windows\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OnlineRetailAnalysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"1200\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.pythonUDF.arrow.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.rpc.askTimeout\", \"800s\") \\\n",
    "    .config(\"spark.rpc.lookupTimeout\", \"800s\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"800s\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce output noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"Spark session initialized successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d416c",
   "metadata": {},
   "source": [
    "## 3. Load Data from GitHub\n",
    "\n",
    "Since PySpark cannot directly read Excel files, we use pandas to read from GitHub and then convert to Spark DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "332dfa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Excel file from GitHub...\n",
      "Loading data from both sheets (2009-2010 and 2010-2011)...\n",
      "2009-2010 data shape: (525461, 8)\n",
      "2010-2011 data shape: (541910, 8)\n",
      "Combined data shape: (1067371, 8)\n",
      "Data successfully loaded from GitHub into Spark DataFrame!\n",
      "Pandas objects cleaned up to free memory.\n"
     ]
    }
   ],
   "source": [
    "# Use pandas to read Excel file from GitHub (PySpark doesn't support Excel directly)\n",
    "import pandas as pd\n",
    "print(\"Reading Excel file from GitHub...\")\n",
    "\n",
    "# GitHub repository information\n",
    "github_user = \"Hachi630\"\n",
    "github_repo = \"BDAS\"\n",
    "file_path = \"online_retail_II.xlsx\"\n",
    "\n",
    "# Construct GitHub raw URL\n",
    "github_url = f\"https://raw.githubusercontent.com/{github_user}/{github_repo}/main/{file_path}\"\n",
    "\n",
    "# Read Excel file with multiple sheets\n",
    "print(\"Loading data from both sheets (2009-2010 and 2010-2011)...\")\n",
    "excel_data = pd.read_excel(github_url, sheet_name=None)  # Read all sheets\n",
    "\n",
    "# Get the two sheets\n",
    "sheet_2009_2010 = excel_data['Year 2009-2010']\n",
    "sheet_2010_2011 = excel_data['Year 2010-2011']\n",
    "\n",
    "print(f\"2009-2010 data shape: {sheet_2009_2010.shape}\")\n",
    "print(f\"2010-2011 data shape: {sheet_2010_2011.shape}\")\n",
    "\n",
    "# Combine both datasets\n",
    "pandas_df = pd.concat([sheet_2009_2010, sheet_2010_2011], ignore_index=True)\n",
    "print(f\"Combined data shape: {pandas_df.shape}\")\n",
    "\n",
    "# Convert pandas DataFrame to Spark DataFrame\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "# Clean up pandas objects to free memory\n",
    "del pandas_df, sheet_2009_2010, sheet_2010_2011, excel_data\n",
    "\n",
    "print(\"Data successfully loaded from GitHub into Spark DataFrame!\")\n",
    "print(\"Pandas objects cleaned up to free memory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d57336",
   "metadata": {},
   "source": [
    "## 4. Check Data Dimensions\n",
    "\n",
    "Determine the number of rows and columns in the combined dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53a08fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Dimension Information ===\n",
      "Counting rows...\n",
      "Dataset row count: 1,067,371\n",
      "Dataset column count: 8\n",
      "Column names: ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country']\n",
      "\n",
      "Dataset partitions: 20\n",
      "Dataset storage level: Serialized 1x Replicated\n",
      "‚úÖ Data dimension check completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Check data dimensions using PySpark with error handling\n",
    "print(\"=== Data Dimension Information ===\")\n",
    "\n",
    "try:\n",
    "    # Get row count with retry mechanism\n",
    "    print(\"Counting rows...\")\n",
    "    row_count = df.count()\n",
    "    print(f\"Dataset row count: {row_count:,}\")\n",
    "    \n",
    "    # Get column count and names\n",
    "    column_count = len(df.columns)\n",
    "    column_names = df.columns\n",
    "    \n",
    "    print(f\"Dataset column count: {column_count}\")\n",
    "    print(f\"Column names: {column_names}\")\n",
    "    \n",
    "    # Additional information\n",
    "    print(f\"\\nDataset partitions: {df.rdd.getNumPartitions()}\")\n",
    "    print(f\"Dataset storage level: {df.storageLevel}\")\n",
    "    \n",
    "    print(\"‚úÖ Data dimension check completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during data dimension check: {e}\")\n",
    "    print(\"\\nüîß Troubleshooting steps:\")\n",
    "    print(\"1. Restart the kernel and run all cells again\")\n",
    "    print(\"2. Check if Java is properly installed\")\n",
    "    print(\"3. Try reducing memory allocation in Spark config\")\n",
    "    print(\"4. Consider using pandas-only analysis for this dataset\")\n",
    "    \n",
    "    # Fallback: try to get basic info without count()\n",
    "    try:\n",
    "        print(\"\\nüîÑ Attempting fallback analysis...\")\n",
    "        column_count = len(df.columns)\n",
    "        column_names = df.columns\n",
    "        print(f\"Dataset column count: {column_count}\")\n",
    "        print(f\"Column names: {column_names}\")\n",
    "        print(\"Note: Row count unavailable due to Spark connection issues\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Fallback also failed: {e2}\")\n",
    "        print(\"Please restart the kernel and try again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d0df9",
   "metadata": {},
   "source": [
    "## 5. Data Quality Assessment\n",
    "\n",
    "This section performs comprehensive data quality checks to identify potential issues in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f711b0",
   "metadata": {},
   "source": [
    "### 5.1 Missing Values Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "889f315c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Missing Values Analysis ===\n",
      "Testing Spark connection...\n",
      "‚úÖ Spark connection successful! Dataset has 1,067,371 records\n",
      "Counting missing values using PySpark...\n",
      "Missing values per column:\n",
      "+-------+---------+-----------+--------+-----------+-----+-----------+-------+\n",
      "|Invoice|StockCode|Description|Quantity|InvoiceDate|Price|Customer ID|Country|\n",
      "+-------+---------+-----------+--------+-----------+-----+-----------+-------+\n",
      "|      0|        0|          0|       0|          0|    0|          0|      0|\n",
      "+-------+---------+-----------+--------+-----------+-----+-----------+-------+\n",
      "\n",
      "\n",
      "Total records: 1,067,371\n",
      "\n",
      "Missing values summary:\n",
      "Invoice: 0 (0.00%)\n",
      "  ‚Üí Invoice is complete (no missing values)\n",
      "StockCode: 0 (0.00%)\n",
      "  ‚Üí StockCode is complete (no missing values)\n",
      "Description: 0 (0.00%)\n",
      "  ‚Üí Description is complete (no missing values)\n",
      "Quantity: 0 (0.00%)\n",
      "  ‚Üí Quantity is complete (no missing values)\n",
      "InvoiceDate: 0 (0.00%)\n",
      "  ‚Üí InvoiceDate is complete (no missing values)\n",
      "Price: 0 (0.00%)\n",
      "  ‚Üí Price is complete (no missing values)\n",
      "Customer ID: 0 (0.00%)\n",
      "  ‚Üí Customer ID is complete (no missing values)\n",
      "Country: 0 (0.00%)\n",
      "  ‚Üí Country is complete (no missing values)\n",
      "‚úÖ Missing values analysis completed successfully with PySpark!\n"
     ]
    }
   ],
   "source": [
    "# Count nulls in each column with robust error handling\n",
    "print(\"=== Missing Values Analysis ===\")\n",
    "\n",
    "# Try PySpark first, fallback to pandas if needed\n",
    "USE_SPARK = True\n",
    "\n",
    "try:\n",
    "    # Test Spark connection first\n",
    "    print(\"Testing Spark connection...\")\n",
    "    test_count = df.count()\n",
    "    print(f\"‚úÖ Spark connection successful! Dataset has {test_count:,} records\")\n",
    "    \n",
    "    # Count missing values in each column using PySpark\n",
    "    print(\"Counting missing values using PySpark...\")\n",
    "    missing_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "    \n",
    "    print(\"Missing values per column:\")\n",
    "    missing_counts.show()\n",
    "    \n",
    "    # Get total count for percentage calculation\n",
    "    total_records = df.count()\n",
    "    print(f\"\\nTotal records: {total_records:,}\")\n",
    "    \n",
    "    # Calculate and display missing percentages\n",
    "    print(\"\\nMissing values summary:\")\n",
    "    missing_data = missing_counts.collect()[0]\n",
    "    for col_name in df.columns:\n",
    "        missing_count = missing_data[col_name]\n",
    "        missing_pct = (missing_count / total_records) * 100\n",
    "        print(f\"{col_name}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "        \n",
    "        # Add specific comment for CustomerID\n",
    "        if col_name == \"Customer ID\" and missing_count > 0:\n",
    "            print(f\"  ‚Üí CustomerID has many missing entries ({missing_count:,} records)\")\n",
    "        elif missing_count == 0:\n",
    "            print(f\"  ‚Üí {col_name} is complete (no missing values)\")\n",
    "        else:\n",
    "            print(f\"  ‚Üí {col_name} has some missing values\")\n",
    "    \n",
    "    print(\"‚úÖ Missing values analysis completed successfully with PySpark!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå PySpark analysis failed: {e}\")\n",
    "    print(\"\\nüîÑ Switching to pandas analysis...\")\n",
    "    USE_SPARK = False\n",
    "    \n",
    "    # Fallback to pandas analysis\n",
    "    try:\n",
    "        # Re-read data with pandas for analysis\n",
    "        print(\"Re-reading data with pandas for analysis...\")\n",
    "        github_url = \"https://raw.githubusercontent.com/Hachi630/BDAS/main/online_retail_II.xlsx\"\n",
    "        excel_data = pd.read_excel(github_url, sheet_name=None)\n",
    "        sheet_2009_2010 = excel_data['Year 2009-2010']\n",
    "        sheet_2010_2011 = excel_data['Year 2010-2011']\n",
    "        pandas_df = pd.concat([sheet_2009_2010, sheet_2010_2011], ignore_index=True)\n",
    "        \n",
    "        print(\"Missing values per column:\")\n",
    "        missing_counts = pandas_df.isnull().sum()\n",
    "        print(missing_counts)\n",
    "        \n",
    "        total_records = len(pandas_df)\n",
    "        print(f\"\\nTotal records: {total_records:,}\")\n",
    "        \n",
    "        # Calculate and display missing percentages\n",
    "        print(\"\\nMissing values summary:\")\n",
    "        for col_name in missing_counts.index:\n",
    "            missing_count = missing_counts[col_name]\n",
    "            missing_pct = (missing_count / total_records) * 100\n",
    "            print(f\"{col_name}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "            \n",
    "            # Add specific comment for CustomerID\n",
    "            if col_name == \"Customer ID\" and missing_count > 0:\n",
    "                print(f\"  ‚Üí CustomerID has many missing entries ({missing_count:,} records)\")\n",
    "            elif missing_count == 0:\n",
    "                print(f\"  ‚Üí {col_name} is complete (no missing values)\")\n",
    "            else:\n",
    "                print(f\"  ‚Üí {col_name} has some missing values\")\n",
    "        \n",
    "        print(\"‚úÖ Missing values analysis completed successfully with pandas!\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Pandas analysis also failed: {e2}\")\n",
    "        print(\"Please check your internet connection and try again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd6a02",
   "metadata": {},
   "source": [
    "### 5.2 Numeric Values Validity Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30c7af44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Numeric Values Validity Check ===\n",
      "Using PySpark for numeric validity analysis...\n",
      "Records with Price <= 0: 6,207\n",
      "\n",
      "Sample records with invalid prices:\n",
      "+-------+---------+--------+-----+\n",
      "|Invoice|StockCode|Quantity|Price|\n",
      "+-------+---------+--------+-----+\n",
      "| 489464|    21733|     -96|  0.0|\n",
      "| 489463|    71477|    -240|  0.0|\n",
      "| 489467|   85123A|    -192|  0.0|\n",
      "| 489521|    21646|     -50|  0.0|\n",
      "| 489655|    20683|     -44|  0.0|\n",
      "+-------+---------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Records with Quantity = 0: 0\n",
      "\n",
      "Records with Quantity < 0 (returns): 22,950\n",
      "‚úÖ PySpark numeric validity check completed!\n",
      "\n",
      "=== Validity Summary ===\n",
      "‚ö†Ô∏è  Found 6,207 records with non-positive prices (possibly freebies)\n",
      "‚úÖ No zero quantity records found\n",
      "‚ÑπÔ∏è  Found 22,950 return transactions (negative quantities)\n"
     ]
    }
   ],
   "source": [
    "# Identify records with non-positive prices or zero quantity with error handling\n",
    "print(\"=== Numeric Values Validity Check ===\")\n",
    "\n",
    "if USE_SPARK:\n",
    "    try:\n",
    "        print(\"Using PySpark for numeric validity analysis...\")\n",
    "        \n",
    "        # Check for non-positive Price values\n",
    "        invalid_price_count = df.filter(F.col(\"Price\") <= 0).count()\n",
    "        print(f\"Records with Price <= 0: {invalid_price_count:,}\")\n",
    "        \n",
    "        if invalid_price_count > 0:\n",
    "            print(\"\\nSample records with invalid prices:\")\n",
    "            df.filter(F.col(\"Price\") <= 0).select(\"Invoice\", \"StockCode\", \"Quantity\", \"Price\").show(5)\n",
    "        \n",
    "        # Check for zero Quantity values\n",
    "        zero_quantity_count = df.filter(F.col(\"Quantity\") == 0).count()\n",
    "        print(f\"\\nRecords with Quantity = 0: {zero_quantity_count:,}\")\n",
    "        \n",
    "        if zero_quantity_count > 0:\n",
    "            print(\"\\nSample records with zero quantity:\")\n",
    "            df.filter(F.col(\"Quantity\") == 0).show(5)\n",
    "        \n",
    "        # Check for negative quantities (returns)\n",
    "        negative_quantity_count = df.filter(F.col(\"Quantity\") < 0).count()\n",
    "        print(f\"\\nRecords with Quantity < 0 (returns): {negative_quantity_count:,}\")\n",
    "        \n",
    "        print(\"‚úÖ PySpark numeric validity check completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå PySpark numeric check failed: {e}\")\n",
    "        print(\"üîÑ Switching to pandas analysis...\")\n",
    "        USE_SPARK = False\n",
    "\n",
    "if not USE_SPARK:\n",
    "    try:\n",
    "        print(\"Using pandas for numeric validity analysis...\")\n",
    "        \n",
    "        # Check for non-positive Price values\n",
    "        invalid_price_mask = pandas_df[\"Price\"] <= 0\n",
    "        invalid_price_count = invalid_price_mask.sum()\n",
    "        print(f\"Records with Price <= 0: {invalid_price_count:,}\")\n",
    "        \n",
    "        if invalid_price_count > 0:\n",
    "            print(\"\\nSample records with invalid prices:\")\n",
    "            print(pandas_df[invalid_price_mask][[\"Invoice\", \"StockCode\", \"Quantity\", \"Price\"]].head())\n",
    "        \n",
    "        # Check for zero Quantity values\n",
    "        zero_quantity_mask = pandas_df[\"Quantity\"] == 0\n",
    "        zero_quantity_count = zero_quantity_mask.sum()\n",
    "        print(f\"\\nRecords with Quantity = 0: {zero_quantity_count:,}\")\n",
    "        \n",
    "        if zero_quantity_count > 0:\n",
    "            print(\"\\nSample records with zero quantity:\")\n",
    "            print(pandas_df[zero_quantity_mask].head())\n",
    "        \n",
    "        # Check for negative quantities (returns)\n",
    "        negative_quantity_mask = pandas_df[\"Quantity\"] < 0\n",
    "        negative_quantity_count = negative_quantity_mask.sum()\n",
    "        print(f\"\\nRecords with Quantity < 0 (returns): {negative_quantity_count:,}\")\n",
    "        \n",
    "        print(\"‚úÖ Pandas numeric validity check completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Pandas numeric check failed: {e}\")\n",
    "\n",
    "# Summary comments\n",
    "print(\"\\n=== Validity Summary ===\")\n",
    "if invalid_price_count > 0:\n",
    "    print(f\"‚ö†Ô∏è  Found {invalid_price_count:,} records with non-positive prices (possibly freebies)\")\n",
    "else:\n",
    "    print(\"‚úÖ All prices are positive\")\n",
    "    \n",
    "if zero_quantity_count > 0:\n",
    "    print(f\"‚ö†Ô∏è  Found {zero_quantity_count:,} records with zero quantity (unusual - could indicate data entry errors)\")\n",
    "else:\n",
    "    print(\"‚úÖ No zero quantity records found\")\n",
    "    \n",
    "if negative_quantity_count > 0:\n",
    "    print(f\"‚ÑπÔ∏è  Found {negative_quantity_count:,} return transactions (negative quantities)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d3a64",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Perform comprehensive exploratory data analysis using PySpark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5369c8",
   "metadata": {},
   "source": [
    "### 6.1 Key Dataset Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76ed05c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Key Dataset Metrics ===\n",
      "Total Transactions: 1,067,371\n",
      "Unique Products (StockCode): 5,305\n",
      "Unique Customers: 5,943\n",
      "Total Revenue: ¬£19,287,250.57\n",
      "Average Order Value: ¬£18.07\n",
      "\n",
      "=== Data Preview ===\n",
      "+-------+---------+-----------------------------------+--------+-------------------+-----+-----------+--------------+\n",
      "|Invoice|StockCode|Description                        |Quantity|InvoiceDate        |Price|Customer ID|Country       |\n",
      "+-------+---------+-----------------------------------+--------+-------------------+-----+-----------+--------------+\n",
      "|489434 |85048    |15CM CHRISTMAS GLASS BALL 20 LIGHTS|12      |2009-12-01 07:45:00|6.95 |13085.0    |United Kingdom|\n",
      "|489434 |79323P   |PINK CHERRY LIGHTS                 |12      |2009-12-01 07:45:00|6.75 |13085.0    |United Kingdom|\n",
      "|489434 |79323W   | WHITE CHERRY LIGHTS               |12      |2009-12-01 07:45:00|6.75 |13085.0    |United Kingdom|\n",
      "|489434 |22041    |RECORD FRAME 7\" SINGLE SIZE        |48      |2009-12-01 07:45:00|2.1  |13085.0    |United Kingdom|\n",
      "|489434 |21232    |STRAWBERRY CERAMIC TRINKET BOX     |24      |2009-12-01 07:45:00|1.25 |13085.0    |United Kingdom|\n",
      "+-------+---------+-----------------------------------+--------+-------------------+-----+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "=== Data Schema ===\n",
      "root\n",
      " |-- Invoice: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: long (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Customer ID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of distinct products and customers\n",
    "print(\"=== Key Dataset Metrics ===\")\n",
    "\n",
    "# Calculate distinct counts using Spark\n",
    "unique_products = df.select(\"StockCode\").distinct().count()\n",
    "unique_customers = df.select(\"Customer ID\").distinct().count()\n",
    "\n",
    "# Calculate total revenue\n",
    "total_revenue = df.agg(F.sum(F.col(\"Quantity\") * F.col(\"Price\")).alias(\"TotalRevenue\")).collect()[0][\"TotalRevenue\"]\n",
    "\n",
    "# Calculate total transactions\n",
    "total_transactions = df.count()\n",
    "\n",
    "print(f\"Total Transactions: {total_transactions:,}\")\n",
    "print(f\"Unique Products (StockCode): {unique_products:,}\")\n",
    "print(f\"Unique Customers: {unique_customers:,}\")\n",
    "print(f\"Total Revenue: ¬£{total_revenue:,.2f}\")\n",
    "\n",
    "# Additional metrics\n",
    "avg_order_value = df.agg(F.avg(F.col(\"Quantity\") * F.col(\"Price\")).alias(\"AvgOrderValue\")).collect()[0][\"AvgOrderValue\"]\n",
    "print(f\"Average Order Value: ¬£{avg_order_value:.2f}\")\n",
    "\n",
    "# Data preview\n",
    "print(\"\\n=== Data Preview ===\")\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "print(\"\\n=== Data Schema ===\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f744a0b9",
   "metadata": {},
   "source": [
    "## 7. Data Quality Summary Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a09708f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "           DATA QUALITY ASSESSMENT SUMMARY\n",
      "============================================================\n",
      "\n",
      "üìä DATASET OVERVIEW:\n",
      "   ‚Ä¢ Total Records: 1,067,371\n",
      "   ‚Ä¢ Total Columns: 8\n",
      "   ‚Ä¢ Analysis Engine: PySpark\n",
      "\n",
      "üîç DATA QUALITY ISSUES IDENTIFIED:\n",
      "\n",
      "1. MISSING VALUES:\n",
      "\n",
      "2. NUMERIC VALIDITY:\n",
      "   ‚Ä¢ Non-positive prices: 6,207\n",
      "   ‚Ä¢ Negative quantities (returns): 22,950\n",
      "\n",
      "3. RETURNS & CANCELLATIONS:\n",
      "   ‚Ä¢ Cancelled invoices: 19,494\n",
      "   ‚Ä¢ Return transactions: 22,950\n",
      "\n",
      "4. DUPLICATE RECORDS:\n",
      "   ‚Ä¢ Duplicate records: 34,335\n",
      "\n",
      "5. DATE RANGE CONSISTENCY:\n",
      "   ‚Ä¢ Date range: 2009-12-01 07:45:00 to 2011-12-09 12:50:00\n",
      "   ‚Ä¢ Expected range: 2009-12-01 to 2011-12-09\n",
      "\n",
      "üìã RECOMMENDATIONS:\n",
      "   ‚Ä¢ CustomerID missing values: Consider impact on customer analysis\n",
      "   ‚Ä¢ Return transactions: Account for net vs gross sales calculations\n",
      "   ‚Ä¢ Date range: Verify business context for partial years\n",
      "   ‚Ä¢ Duplicates: Consider removing for accurate analysis\n",
      "   ‚Ä¢ Invalid values: Review business rules for data cleaning\n",
      "\n",
      "============================================================\n",
      "           END OF DATA QUALITY ASSESSMENT\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Data Quality Summary Report\n",
    "print(\"=\" * 60)\n",
    "print(\"           DATA QUALITY ASSESSMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìä DATASET OVERVIEW:\")\n",
    "total_records = df.count()\n",
    "total_columns = len(df.columns)\n",
    "\n",
    "print(f\"   ‚Ä¢ Total Records: {total_records:,}\")\n",
    "print(f\"   ‚Ä¢ Total Columns: {total_columns}\")\n",
    "print(f\"   ‚Ä¢ Analysis Engine: PySpark\")\n",
    "\n",
    "print(\"\\nüîç DATA QUALITY ISSUES IDENTIFIED:\")\n",
    "\n",
    "# Missing Values Summary\n",
    "print(\"\\n1. MISSING VALUES:\")\n",
    "missing_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "missing_data = missing_counts.collect()[0]\n",
    "for col_name in df.columns:\n",
    "    missing_count = missing_data[col_name]\n",
    "    missing_pct = (missing_count / total_records) * 100\n",
    "    if missing_count > 0:\n",
    "        print(f\"   ‚Ä¢ {col_name}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "\n",
    "# Numeric Validity Summary\n",
    "print(\"\\n2. NUMERIC VALIDITY:\")\n",
    "invalid_price_count = df.filter(F.col(\"Price\") <= 0).count()\n",
    "zero_quantity_count = df.filter(F.col(\"Quantity\") == 0).count()\n",
    "negative_quantity_count = df.filter(F.col(\"Quantity\") < 0).count()\n",
    "\n",
    "if invalid_price_count > 0:\n",
    "    print(f\"   ‚Ä¢ Non-positive prices: {invalid_price_count:,}\")\n",
    "if zero_quantity_count > 0:\n",
    "    print(f\"   ‚Ä¢ Zero quantities: {zero_quantity_count:,}\")\n",
    "if negative_quantity_count > 0:\n",
    "    print(f\"   ‚Ä¢ Negative quantities (returns): {negative_quantity_count:,}\")\n",
    "\n",
    "# Returns and Cancellations Summary\n",
    "print(\"\\n3. RETURNS & CANCELLATIONS:\")\n",
    "num_cancelled = df.filter(F.col(\"Invoice\").startswith(\"C\")).count()\n",
    "num_returns = df.filter(F.col(\"Quantity\") < 0).count()\n",
    "\n",
    "print(f\"   ‚Ä¢ Cancelled invoices: {num_cancelled:,}\")\n",
    "print(f\"   ‚Ä¢ Return transactions: {num_returns:,}\")\n",
    "\n",
    "# Duplicates Summary\n",
    "print(\"\\n4. DUPLICATE RECORDS:\")\n",
    "unique_rows = df.dropDuplicates().count()\n",
    "num_duplicates = total_records - unique_rows\n",
    "if num_duplicates > 0:\n",
    "    print(f\"   ‚Ä¢ Duplicate records: {num_duplicates:,}\")\n",
    "else:\n",
    "    print(\"   ‚Ä¢ No duplicate records found\")\n",
    "\n",
    "# Date Range Summary\n",
    "print(\"\\n5. DATE RANGE CONSISTENCY:\")\n",
    "date_range = df.select(F.min(\"InvoiceDate\").alias(\"MinDate\"), F.max(\"InvoiceDate\").alias(\"MaxDate\"))\n",
    "date_info = date_range.collect()[0]\n",
    "min_date = date_info[\"MinDate\"]\n",
    "max_date = date_info[\"MaxDate\"]\n",
    "\n",
    "print(f\"   ‚Ä¢ Date range: {min_date} to {max_date}\")\n",
    "print(f\"   ‚Ä¢ Expected range: 2009-12-01 to 2011-12-09\")\n",
    "\n",
    "print(\"\\nüìã RECOMMENDATIONS:\")\n",
    "print(\"   ‚Ä¢ CustomerID missing values: Consider impact on customer analysis\")\n",
    "print(\"   ‚Ä¢ Return transactions: Account for net vs gross sales calculations\")\n",
    "print(\"   ‚Ä¢ Date range: Verify business context for partial years\")\n",
    "if num_duplicates > 0:\n",
    "    print(\"   ‚Ä¢ Duplicates: Consider removing for accurate analysis\")\n",
    "if invalid_price_count > 0 or zero_quantity_count > 0:\n",
    "    print(\"   ‚Ä¢ Invalid values: Review business rules for data cleaning\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"           END OF DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48274893",
   "metadata": {},
   "source": [
    "## 8. Troubleshooting: Restart Spark Session\n",
    "\n",
    "If you encounter connection timeout errors, run this cell to restart the Spark session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ba03bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Restarting Spark session...\n",
      "‚úÖ Previous Spark session stopped\n",
      "‚úÖ New Spark session initialized!\n",
      "Spark version: 4.0.1\n",
      "‚úÖ Connection test successful: 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Restart Spark session if needed\n",
    "print(\"üîÑ Restarting Spark session...\")\n",
    "\n",
    "try:\n",
    "    # Stop current session\n",
    "    spark.stop()\n",
    "    print(\"‚úÖ Previous Spark session stopped\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è  No previous session to stop\")\n",
    "\n",
    "# Wait a moment\n",
    "import time\n",
    "time.sleep(2)\n",
    "\n",
    "# Restart with optimized configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OnlineRetailAnalysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"1800\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "    .config(\"spark.network.timeout\", \"1200s\") \\\n",
    "    .config(\"spark.rpc.askTimeout\", \"1200s\") \\\n",
    "    .config(\"spark.rpc.lookupTimeout\", \"1200s\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"1200s\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"‚úÖ New Spark session initialized!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# Test the connection\n",
    "try:\n",
    "    test_df = spark.range(10)\n",
    "    test_count = test_df.count()\n",
    "    print(f\"‚úÖ Connection test successful: {test_count} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection test failed: {e}\")\n",
    "    print(\"Please check your Java installation and try again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d566bf",
   "metadata": {},
   "source": [
    "## Fix Spark on Windows: stable local dir + session rebuild\n",
    "\n",
    "This cell configures Spark to use a stable local directory on D: and rebuilds the session to avoid blockmgr/temp folder errors on Windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dd59a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark rebuilt with stable local dir: D:/spark-tmp\n",
      "Spark UI: http://windows10.microdone.cn:4040\n"
     ]
    }
   ],
   "source": [
    "# Configure stable local dirs and rebuild Spark session (Windows)\n",
    "import os, time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1) Create stable directories on D: (adjust if needed)\n",
    "stable_tmp = \"D:/spark-tmp\"\n",
    "stable_wh  = \"D:/spark-warehouse\"\n",
    "\n",
    "os.makedirs(stable_tmp, exist_ok=True)\n",
    "os.makedirs(stable_wh,  exist_ok=True)\n",
    "\n",
    "# 2) Environment vars so PySpark avoids system Temp\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\"\n",
    "os.environ[\"PYSPARK_TEMP_DIR\"] = stable_tmp\n",
    "os.environ[\"TMP\"] = stable_tmp\n",
    "os.environ[\"TEMP\"] = stable_tmp\n",
    "\n",
    "# 3) Stop previous session if any\n",
    "try:\n",
    "    spark.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 4) Rebuild Spark with stable local dir and conservative timeouts\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OnlineRetailAnalysis\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.local.dir\", stable_tmp) \\\n",
    "    .config(\"spark.sql.warehouse.dir\", stable_wh) \\\n",
    "    .config(\"spark.python.worker.timeout\", \"1800\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "    .config(\"spark.network.timeout\", \"1200s\") \\\n",
    "    .config(\"spark.rpc.askTimeout\", \"1200s\") \\\n",
    "    .config(\"spark.rpc.lookupTimeout\", \"1200s\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"1200s\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"‚úÖ Spark rebuilt with stable local dir:\", stable_tmp)\n",
    "print(\"Spark UI:\", spark.sparkContext.uiWebUrl)\n",
    "\n",
    "# Small sleep to let workers warm up\n",
    "time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b709b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Spark self-test...\n",
      "Count test: 10\n",
      "‚úÖ Spark basic count OK\n"
     ]
    }
   ],
   "source": [
    "# Connection self-test and safe repartition\n",
    "print(\"Running Spark self-test...\")\n",
    "try:\n",
    "    test = spark.range(10)\n",
    "    print(\"Count test:\", test.count())\n",
    "    print(\"‚úÖ Spark basic count OK\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Spark basic count failed:\", e)\n",
    "\n",
    "# If you already have df loaded, make it safer before heavy actions\n",
    "try:\n",
    "    from pyspark.storagelevel import StorageLevel\n",
    "    df = df.repartition(2).persist(StorageLevel.MEMORY_ONLY)\n",
    "    print(\"Repartitioned df to\", df.rdd.getNumPartitions(), \"partitions\")\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ddf6c6",
   "metadata": {},
   "source": [
    "## 9. Alternative: Pandas-Only Analysis\n",
    "\n",
    "If PySpark continues to have connection issues, use this pandas-only analysis as a complete alternative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5af15180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Complete Pandas Analysis ===\n",
      "This analysis uses pandas only, bypassing PySpark completely.\n",
      "Loading data with pandas...\n",
      "‚úÖ Data loaded successfully!\n",
      "Dataset shape: (1067371, 8)\n",
      "Columns: ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country']\n",
      "\n",
      "=== Dataset Overview ===\n",
      "Total records: 1,067,371\n",
      "Total columns: 8\n",
      "Memory usage: 266.48 MB\n",
      "\n",
      "=== Data Preview ===\n",
      "  Invoice StockCode                          Description  Quantity  \\\n",
      "0  489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
      "1  489434    79323P                   PINK CHERRY LIGHTS        12   \n",
      "2  489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
      "3  489434     22041         RECORD FRAME 7\" SINGLE SIZE         48   \n",
      "4  489434     21232       STRAWBERRY CERAMIC TRINKET BOX        24   \n",
      "\n",
      "          InvoiceDate  Price  Customer ID         Country  \n",
      "0 2009-12-01 07:45:00   6.95      13085.0  United Kingdom  \n",
      "1 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "2 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "3 2009-12-01 07:45:00   2.10      13085.0  United Kingdom  \n",
      "4 2009-12-01 07:45:00   1.25      13085.0  United Kingdom  \n",
      "\n",
      "=== Data Types ===\n",
      "Invoice                object\n",
      "StockCode              object\n",
      "Description            object\n",
      "Quantity                int64\n",
      "InvoiceDate    datetime64[ns]\n",
      "Price                 float64\n",
      "Customer ID           float64\n",
      "Country                object\n",
      "dtype: object\n",
      "\n",
      "=== Missing Values ===\n",
      "             Missing Count  Missing Percentage\n",
      "Invoice                  0            0.000000\n",
      "StockCode                0            0.000000\n",
      "Description           4382            0.410541\n",
      "Quantity                 0            0.000000\n",
      "InvoiceDate              0            0.000000\n",
      "Price                    0            0.000000\n",
      "Customer ID         243007           22.766873\n",
      "Country                  0            0.000000\n",
      "\n",
      "=== Basic Statistics ===\n",
      "           Quantity                    InvoiceDate         Price  \\\n",
      "count  1.067371e+06                        1067371  1.067371e+06   \n",
      "mean   9.938898e+00  2011-01-02 21:13:55.394028544  4.649388e+00   \n",
      "min   -8.099500e+04            2009-12-01 07:45:00 -5.359436e+04   \n",
      "25%    1.000000e+00            2010-07-09 09:46:00  1.250000e+00   \n",
      "50%    3.000000e+00            2010-12-07 15:28:00  2.100000e+00   \n",
      "75%    1.000000e+01            2011-07-22 10:23:00  4.150000e+00   \n",
      "max    8.099500e+04            2011-12-09 12:50:00  3.897000e+04   \n",
      "std    1.727058e+02                            NaN  1.235531e+02   \n",
      "\n",
      "         Customer ID  \n",
      "count  824364.000000  \n",
      "mean    15324.638504  \n",
      "min     12346.000000  \n",
      "25%     13975.000000  \n",
      "50%     15255.000000  \n",
      "75%     16797.000000  \n",
      "max     18287.000000  \n",
      "std      1697.464450  \n",
      "\n",
      "=== Key Metrics ===\n",
      "Unique Products: 5,305\n",
      "Unique Customers: 5,942\n",
      "Total Revenue: ¬£19,287,250.57\n",
      "Average Order Value: ¬£18.07\n",
      "\n",
      "=== Data Quality Checks ===\n",
      "Invalid prices (‚â§0): 6,207\n",
      "Zero quantities: 0\n",
      "Negative quantities (returns): 22,950\n",
      "Cancelled invoices: 19,494\n",
      "\n",
      "=== Date Range ===\n",
      "Date range: 2009-12-01 07:45:00 to 2011-12-09 12:50:00\n",
      "\n",
      "=== Top 10 Countries by Revenue ===\n",
      "                Quantity  TotalRevenue  TransactionCount\n",
      "Country                                                 \n",
      "United Kingdom   8692875  1.638258e+07            981330\n",
      "EIRE              331341  6.155195e+05             17866\n",
      "Netherlands       381951  5.485249e+05              5140\n",
      "Germany           224581  4.179886e+05             17624\n",
      "France            184952  3.281918e+05             14330\n",
      "Australia         103706  1.671291e+05              1913\n",
      "Switzerland        52378  9.972876e+04              3189\n",
      "Spain              45156  9.185948e+04              3811\n",
      "Sweden             87875  8.780942e+04              1364\n",
      "Denmark           235218  6.574109e+04               817\n",
      "\n",
      "‚úÖ Complete pandas analysis finished!\n",
      "All analysis completed successfully using pandas.\n"
     ]
    }
   ],
   "source": [
    "# Complete pandas-only analysis as fallback\n",
    "print(\"=== Complete Pandas Analysis ===\")\n",
    "print(\"This analysis uses pandas only, bypassing PySpark completely.\")\n",
    "\n",
    "# Load data with pandas\n",
    "print(\"Loading data with pandas...\")\n",
    "github_url = \"https://raw.githubusercontent.com/Hachi630/BDAS/main/online_retail_II.xlsx\"\n",
    "excel_data = pd.read_excel(github_url, sheet_name=None)\n",
    "sheet_2009_2010 = excel_data['Year 2009-2010']\n",
    "sheet_2010_2011 = excel_data['Year 2010-2011']\n",
    "df_pandas = pd.concat([sheet_2009_2010, sheet_2010_2011], ignore_index=True)\n",
    "\n",
    "print(f\"‚úÖ Data loaded successfully!\")\n",
    "print(f\"Dataset shape: {df_pandas.shape}\")\n",
    "print(f\"Columns: {list(df_pandas.columns)}\")\n",
    "\n",
    "# Basic info\n",
    "print(\"\\n=== Dataset Overview ===\")\n",
    "print(f\"Total records: {len(df_pandas):,}\")\n",
    "print(f\"Total columns: {len(df_pandas.columns)}\")\n",
    "print(f\"Memory usage: {df_pandas.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Data preview\n",
    "print(\"\\n=== Data Preview ===\")\n",
    "print(df_pandas.head())\n",
    "\n",
    "# Data types\n",
    "print(\"\\n=== Data Types ===\")\n",
    "print(df_pandas.dtypes)\n",
    "\n",
    "# Missing values\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "missing_counts = df_pandas.isnull().sum()\n",
    "missing_pct = (missing_counts / len(df_pandas)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Missing Percentage': missing_pct\n",
    "})\n",
    "print(missing_df)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n=== Basic Statistics ===\")\n",
    "print(df_pandas.describe())\n",
    "\n",
    "# Key metrics\n",
    "print(\"\\n=== Key Metrics ===\")\n",
    "unique_products = df_pandas['StockCode'].nunique()\n",
    "unique_customers = df_pandas['Customer ID'].nunique()\n",
    "total_revenue = (df_pandas['Quantity'] * df_pandas['Price']).sum()\n",
    "avg_order_value = (df_pandas['Quantity'] * df_pandas['Price']).mean()\n",
    "\n",
    "print(f\"Unique Products: {unique_products:,}\")\n",
    "print(f\"Unique Customers: {unique_customers:,}\")\n",
    "print(f\"Total Revenue: ¬£{total_revenue:,.2f}\")\n",
    "print(f\"Average Order Value: ¬£{avg_order_value:.2f}\")\n",
    "\n",
    "# Data quality checks\n",
    "print(\"\\n=== Data Quality Checks ===\")\n",
    "invalid_prices = (df_pandas['Price'] <= 0).sum()\n",
    "zero_quantities = (df_pandas['Quantity'] == 0).sum()\n",
    "negative_quantities = (df_pandas['Quantity'] < 0).sum()\n",
    "cancelled_invoices = df_pandas['Invoice'].str.startswith('C').sum()\n",
    "\n",
    "print(f\"Invalid prices (‚â§0): {invalid_prices:,}\")\n",
    "print(f\"Zero quantities: {zero_quantities:,}\")\n",
    "print(f\"Negative quantities (returns): {negative_quantities:,}\")\n",
    "print(f\"Cancelled invoices: {cancelled_invoices:,}\")\n",
    "\n",
    "# Date range\n",
    "print(\"\\n=== Date Range ===\")\n",
    "min_date = df_pandas['InvoiceDate'].min()\n",
    "max_date = df_pandas['InvoiceDate'].max()\n",
    "print(f\"Date range: {min_date} to {max_date}\")\n",
    "\n",
    "# Top countries\n",
    "print(\"\\n=== Top 10 Countries by Revenue ===\")\n",
    "country_revenue = df_pandas.groupby('Country').agg({\n",
    "    'Quantity': 'sum',\n",
    "    'Price': lambda x: (df_pandas.loc[x.index, 'Quantity'] * x).sum(),\n",
    "    'Invoice': 'count'\n",
    "}).rename(columns={'Price': 'TotalRevenue', 'Invoice': 'TransactionCount'})\n",
    "country_revenue = country_revenue.sort_values('TotalRevenue', ascending=False)\n",
    "print(country_revenue.head(10))\n",
    "\n",
    "print(\"\\n‚úÖ Complete pandas analysis finished!\")\n",
    "print(\"All analysis completed successfully using pandas.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
