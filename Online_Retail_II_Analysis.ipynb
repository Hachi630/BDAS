{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2fd284f1",
      "metadata": {
        "id": "2fd284f1"
      },
      "source": [
        "# PySpark Online Retail II Dataset Analysis\n",
        "\n",
        "This notebook demonstrates how to load and analyze the Online Retail II dataset using PySpark in Google Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BX8JRr3hg0WS",
      "metadata": {
        "id": "BX8JRr3hg0WS"
      },
      "source": [
        "# 2. Data Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4e5c825",
      "metadata": {
        "id": "d4e5c825"
      },
      "source": [
        "## 1. Install Required Packages\n",
        "\n",
        "First, install PySpark and related dependencies in Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "862c8810",
      "metadata": {
        "id": "862c8810",
        "outputId": "85525759-6d28-4299-f1e8-a0b64bc7dc42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in d:\\python\\lib\\site-packages (4.0.1)\n",
            "Requirement already satisfied: pandas in d:\\python\\lib\\site-packages (2.3.2)\n",
            "Requirement already satisfied: openpyxl in d:\\python\\lib\\site-packages (3.1.3)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in d:\\python\\lib\\site-packages (from pyspark) (0.10.9.9)\n",
            "Requirement already satisfied: numpy>=1.22.4 in d:\\python\\lib\\site-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\python\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\python\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\python\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in d:\\python\\lib\\site-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in d:\\python\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install pyspark pandas openpyxl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8fa7021",
      "metadata": {
        "id": "a8fa7021"
      },
      "source": [
        "## 2. Import Libraries and Initialize Spark Session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3af7c7f5",
      "metadata": {
        "id": "3af7c7f5",
        "outputId": "427409dd-2051-410b-ced7-069c2f04ebdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session initialized successfully!\n",
            "Spark version: 4.0.1\n",
            "Spark UI: http://windows10.microdone.cn:4040\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col, sum as spark_sum, count, when, isnan, isnull, desc, min as spark_min, max as spark_max\n",
        "from pyspark.sql import functions as F\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Set environment variables for Windows\n",
        "os.environ['PYSPARK_PYTHON'] = 'python'\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python'\n",
        "\n",
        "# Initialize Spark session with enhanced configuration for Windows\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"OnlineRetailAnalysis\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
        "    .config(\"spark.python.worker.timeout\", \"1200\") \\\n",
        "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.sql.execution.pythonUDF.arrow.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
        "    .config(\"spark.network.timeout\", \"800s\") \\\n",
        "    .config(\"spark.rpc.askTimeout\", \"800s\") \\\n",
        "    .config(\"spark.rpc.lookupTimeout\", \"800s\") \\\n",
        "    .config(\"spark.sql.broadcastTimeout\", \"800s\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce output noise\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"Spark session initialized successfully!\")\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b50d416c",
      "metadata": {
        "id": "b50d416c"
      },
      "source": [
        "## 3. Load Data from GitHub\n",
        "\n",
        "Since PySpark cannot directly read Excel files, we use pandas to read from GitHub and then convert to Spark DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "332dfa54",
      "metadata": {
        "id": "332dfa54",
        "outputId": "2135d0e1-0e8d-49d8-a708-b10bd85efc49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading Excel file from GitHub...\n",
            "Loading data from both sheets (2009-2010 and 2010-2011)...\n",
            "2009-2010 data shape: (525461, 8)\n",
            "2010-2011 data shape: (541910, 8)\n",
            "Combined data shape: (1067371, 8)\n",
            "Data successfully loaded from GitHub into Spark DataFrame!\n",
            "Pandas objects cleaned up to free memory.\n"
          ]
        }
      ],
      "source": [
        "# Use pandas to read Excel file from GitHub (PySpark doesn't support Excel directly)\n",
        "import pandas as pd\n",
        "print(\"Reading Excel file from GitHub...\")\n",
        "\n",
        "# GitHub repository information\n",
        "github_user = \"Hachi630\"\n",
        "github_repo = \"BDAS\"\n",
        "file_path = \"online_retail_II.xlsx\"\n",
        "\n",
        "# Construct GitHub raw URL\n",
        "github_url = f\"https://raw.githubusercontent.com/{github_user}/{github_repo}/main/{file_path}\"\n",
        "\n",
        "# Read Excel file with multiple sheets\n",
        "print(\"Loading data from both sheets (2009-2010 and 2010-2011)...\")\n",
        "excel_data = pd.read_excel(github_url, sheet_name=None)  # Read all sheets\n",
        "\n",
        "# Get the two sheets\n",
        "sheet_2009_2010 = excel_data['Year 2009-2010']\n",
        "sheet_2010_2011 = excel_data['Year 2010-2011']\n",
        "\n",
        "print(f\"2009-2010 data shape: {sheet_2009_2010.shape}\")\n",
        "print(f\"2010-2011 data shape: {sheet_2010_2011.shape}\")\n",
        "\n",
        "# Combine both datasets\n",
        "pandas_df = pd.concat([sheet_2009_2010, sheet_2010_2011], ignore_index=True)\n",
        "print(f\"Combined data shape: {pandas_df.shape}\")\n",
        "\n",
        "# Convert pandas DataFrame to Spark DataFrame\n",
        "df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "# Clean up pandas objects to free memory\n",
        "del pandas_df, sheet_2009_2010, sheet_2010_2011, excel_data\n",
        "\n",
        "print(\"Data successfully loaded from GitHub into Spark DataFrame!\")\n",
        "print(\"Pandas objects cleaned up to free memory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6d57336",
      "metadata": {
        "id": "b6d57336"
      },
      "source": [
        "## 4. Check Data Dimensions\n",
        "\n",
        "Determine the number of rows and columns in the combined dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "53a08fba",
      "metadata": {
        "id": "53a08fba",
        "outputId": "50681e1a-518d-46a1-bd70-8e23bded1c83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Data Dimension Information ===\n",
            "Counting rows...\n",
            "Dataset row count: 1,067,371\n",
            "Dataset column count: 8\n",
            "Column names: ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country']\n",
            "\n",
            "Dataset partitions: 20\n",
            "Dataset storage level: Serialized 1x Replicated\n",
            "✅ Data dimension check completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Check data dimensions using PySpark with error handling\n",
        "print(\"=== Data Dimension Information ===\")\n",
        "\n",
        "try:\n",
        "    # Get row count with retry mechanism\n",
        "    print(\"Counting rows...\")\n",
        "    row_count = df.count()\n",
        "    print(f\"Dataset row count: {row_count:,}\")\n",
        "\n",
        "    # Get column count and names\n",
        "    column_count = len(df.columns)\n",
        "    column_names = df.columns\n",
        "\n",
        "    print(f\"Dataset column count: {column_count}\")\n",
        "    print(f\"Column names: {column_names}\")\n",
        "\n",
        "    # Additional information\n",
        "    print(f\"\\nDataset partitions: {df.rdd.getNumPartitions()}\")\n",
        "    print(f\"Dataset storage level: {df.storageLevel}\")\n",
        "\n",
        "    print(\"✅ Data dimension check completed successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during data dimension check: {e}\")\n",
        "    print(\"\\n🔧 Troubleshooting steps:\")\n",
        "    print(\"1. Restart the kernel and run all cells again\")\n",
        "    print(\"2. Check if Java is properly installed\")\n",
        "    print(\"3. Try reducing memory allocation in Spark config\")\n",
        "    print(\"4. Consider using pandas-only analysis for this dataset\")\n",
        "\n",
        "    # Fallback: try to get basic info without count()\n",
        "    try:\n",
        "        print(\"\\n🔄 Attempting fallback analysis...\")\n",
        "        column_count = len(df.columns)\n",
        "        column_names = df.columns\n",
        "        print(f\"Dataset column count: {column_count}\")\n",
        "        print(f\"Column names: {column_names}\")\n",
        "        print(\"Note: Row count unavailable due to Spark connection issues\")\n",
        "    except Exception as e2:\n",
        "        print(f\"❌ Fallback also failed: {e2}\")\n",
        "        print(\"Please restart the kernel and try again.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c8d0df9",
      "metadata": {
        "id": "0c8d0df9"
      },
      "source": [
        "## 5. Data Quality Assessment\n",
        "\n",
        "This section performs comprehensive data quality checks to identify potential issues in the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22f711b0",
      "metadata": {
        "id": "22f711b0"
      },
      "source": [
        "### 5.1 Missing Values Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "889f315c",
      "metadata": {
        "id": "889f315c",
        "outputId": "36cc67fc-060b-4df6-a78e-5549bac27d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Missing Values Analysis ===\n",
            "Testing Spark connection...\n",
            "✅ Spark connection successful! Dataset has 1,067,371 records\n",
            "Counting missing values using PySpark...\n",
            "Missing values per column:\n",
            "+-------+---------+-----------+--------+-----------+-----+-----------+-------+\n",
            "|Invoice|StockCode|Description|Quantity|InvoiceDate|Price|Customer ID|Country|\n",
            "+-------+---------+-----------+--------+-----------+-----+-----------+-------+\n",
            "|      0|        0|          0|       0|          0|    0|          0|      0|\n",
            "+-------+---------+-----------+--------+-----------+-----+-----------+-------+\n",
            "\n",
            "\n",
            "Total records: 1,067,371\n",
            "\n",
            "Missing values summary:\n",
            "Invoice: 0 (0.00%)\n",
            "  → Invoice is complete (no missing values)\n",
            "StockCode: 0 (0.00%)\n",
            "  → StockCode is complete (no missing values)\n",
            "Description: 0 (0.00%)\n",
            "  → Description is complete (no missing values)\n",
            "Quantity: 0 (0.00%)\n",
            "  → Quantity is complete (no missing values)\n",
            "InvoiceDate: 0 (0.00%)\n",
            "  → InvoiceDate is complete (no missing values)\n",
            "Price: 0 (0.00%)\n",
            "  → Price is complete (no missing values)\n",
            "Customer ID: 0 (0.00%)\n",
            "  → Customer ID is complete (no missing values)\n",
            "Country: 0 (0.00%)\n",
            "  → Country is complete (no missing values)\n",
            "✅ Missing values analysis completed successfully with PySpark!\n"
          ]
        }
      ],
      "source": [
        "# Count nulls in each column with robust error handling\n",
        "print(\"=== Missing Values Analysis ===\")\n",
        "\n",
        "# Try PySpark first, fallback to pandas if needed\n",
        "USE_SPARK = True\n",
        "\n",
        "try:\n",
        "    # Test Spark connection first\n",
        "    print(\"Testing Spark connection...\")\n",
        "    test_count = df.count()\n",
        "    print(f\"✅ Spark connection successful! Dataset has {test_count:,} records\")\n",
        "\n",
        "    # Count missing values in each column using PySpark\n",
        "    print(\"Counting missing values using PySpark...\")\n",
        "    missing_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
        "\n",
        "    print(\"Missing values per column:\")\n",
        "    missing_counts.show()\n",
        "\n",
        "    # Get total count for percentage calculation\n",
        "    total_records = df.count()\n",
        "    print(f\"\\nTotal records: {total_records:,}\")\n",
        "\n",
        "    # Calculate and display missing percentages\n",
        "    print(\"\\nMissing values summary:\")\n",
        "    missing_data = missing_counts.collect()[0]\n",
        "    for col_name in df.columns:\n",
        "        missing_count = missing_data[col_name]\n",
        "        missing_pct = (missing_count / total_records) * 100\n",
        "        print(f\"{col_name}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
        "\n",
        "        # Add specific comment for CustomerID\n",
        "        if col_name == \"Customer ID\" and missing_count > 0:\n",
        "            print(f\"  → CustomerID has many missing entries ({missing_count:,} records)\")\n",
        "        elif missing_count == 0:\n",
        "            print(f\"  → {col_name} is complete (no missing values)\")\n",
        "        else:\n",
        "            print(f\"  → {col_name} has some missing values\")\n",
        "\n",
        "    print(\"✅ Missing values analysis completed successfully with PySpark!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ PySpark analysis failed: {e}\")\n",
        "    print(\"\\n🔄 Switching to pandas analysis...\")\n",
        "    USE_SPARK = False\n",
        "\n",
        "    # Fallback to pandas analysis\n",
        "    try:\n",
        "        # Re-read data with pandas for analysis\n",
        "        print(\"Re-reading data with pandas for analysis...\")\n",
        "        github_url = \"https://raw.githubusercontent.com/Hachi630/BDAS/main/online_retail_II.xlsx\"\n",
        "        excel_data = pd.read_excel(github_url, sheet_name=None)\n",
        "        sheet_2009_2010 = excel_data['Year 2009-2010']\n",
        "        sheet_2010_2011 = excel_data['Year 2010-2011']\n",
        "        pandas_df = pd.concat([sheet_2009_2010, sheet_2010_2011], ignore_index=True)\n",
        "\n",
        "        print(\"Missing values per column:\")\n",
        "        missing_counts = pandas_df.isnull().sum()\n",
        "        print(missing_counts)\n",
        "\n",
        "        total_records = len(pandas_df)\n",
        "        print(f\"\\nTotal records: {total_records:,}\")\n",
        "\n",
        "        # Calculate and display missing percentages\n",
        "        print(\"\\nMissing values summary:\")\n",
        "        for col_name in missing_counts.index:\n",
        "            missing_count = missing_counts[col_name]\n",
        "            missing_pct = (missing_count / total_records) * 100\n",
        "            print(f\"{col_name}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
        "\n",
        "            # Add specific comment for CustomerID\n",
        "            if col_name == \"Customer ID\" and missing_count > 0:\n",
        "                print(f\"  → CustomerID has many missing entries ({missing_count:,} records)\")\n",
        "            elif missing_count == 0:\n",
        "                print(f\"  → {col_name} is complete (no missing values)\")\n",
        "            else:\n",
        "                print(f\"  → {col_name} has some missing values\")\n",
        "\n",
        "        print(\"✅ Missing values analysis completed successfully with pandas!\")\n",
        "\n",
        "    except Exception as e2:\n",
        "        print(f\"❌ Pandas analysis also failed: {e2}\")\n",
        "        print(\"Please check your internet connection and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "017cef73",
      "metadata": {
        "id": "017cef73",
        "outputId": "6be0e5fa-f0de-47e3-abc2-9de3453b8f94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Missing Values Analysis (Robust) ===\n",
            "Missing values per column (robust):\n",
            "+-------+---------+-----------+--------+-----------+-----+-----------+-------+\n",
            "|Invoice|StockCode|Description|Quantity|InvoiceDate|Price|Customer ID|Country|\n",
            "+-------+---------+-----------+--------+-----------+-----+-----------+-------+\n",
            "|0      |0        |0          |0       |0          |0    |243007     |0      |\n",
            "+-------+---------+-----------+--------+-----------+-----+-----------+-------+\n",
            "\n",
            "\n",
            "Missing values summary (count and %):\n",
            "Invoice: 0 (0.00%)  → complete\n",
            "StockCode: 0 (0.00%)  → complete\n",
            "Description: 0 (0.00%)  → complete\n",
            "Quantity: 0 (0.00%)  → complete\n",
            "InvoiceDate: 0 (0.00%)  → complete\n",
            "Price: 0 (0.00%)  → complete\n",
            "Customer ID: 243,007 (22.77%)  → many missing\n",
            "  Note: 'Customer ID' often has many missing entries in this dataset.\n",
            "Country: 0 (0.00%)  → complete\n"
          ]
        }
      ],
      "source": [
        "# Missing Values (robust check: null, NaN, empty strings, 'null'-like)\n",
        "from pyspark.sql.types import StringType, NumericType, TimestampType, DateType\n",
        "\n",
        "print(\"=== Missing Values Analysis (Robust) ===\")\n",
        "\n",
        "schema_fields = {f.name: f.dataType for f in df.schema.fields}\n",
        "\n",
        "def missing_agg_for(col_name: str):\n",
        "\tdtype = schema_fields[col_name]\n",
        "\tc = F.col(col_name)\n",
        "\tif isinstance(dtype, NumericType):\n",
        "\t\tcond = c.isNull() | F.isnan(c)\n",
        "\telif isinstance(dtype, StringType):\n",
        "\t\tcond = c.isNull() | (F.trim(c) == \"\") | (F.lower(F.trim(c)).isin(\"na\", \"n/a\", \"null\", \"none\"))\n",
        "\telse:\n",
        "\t\tcond = c.isNull()\n",
        "\treturn F.sum(F.when(cond, 1).otherwise(0)).alias(col_name)\n",
        "\n",
        "exprs = [missing_agg_for(c) for c in df.columns]\n",
        "robust_missing = df.agg(*exprs)\n",
        "\n",
        "print(\"Missing values per column (robust):\")\n",
        "robust_missing.show(truncate=False)\n",
        "\n",
        "# Percentages\n",
        "total_records = df.count()\n",
        "row = robust_missing.collect()[0]\n",
        "print(\"\\nMissing values summary (count and %):\")\n",
        "for c in df.columns:\n",
        "\tmc = int(row[c])\n",
        "\tpct = (mc / total_records) * 100 if total_records else 0\n",
        "\tstatus = \"complete\" if mc == 0 else (\"many missing\" if mc > 0.05 * total_records else \"some missing\")\n",
        "\tprint(f\"{c}: {mc:,} ({pct:.2f}%)  → {status}\")\n",
        "\tif c == \"Customer ID\" and mc > 0:\n",
        "\t\tprint(f\"  Note: 'Customer ID' often has many missing entries in this dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38dd6a02",
      "metadata": {
        "id": "38dd6a02"
      },
      "source": [
        "### 5.2 Numeric Values Validity Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "30c7af44",
      "metadata": {
        "id": "30c7af44",
        "outputId": "da335d97-54d4-46fd-a74c-ddbca4e60b29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Numeric Values Validity Check ===\n",
            "Using PySpark for numeric validity analysis...\n",
            "Records with Price <= 0: 6,207\n",
            "\n",
            "Sample records with invalid prices:\n",
            "+-------+---------+--------+-----+\n",
            "|Invoice|StockCode|Quantity|Price|\n",
            "+-------+---------+--------+-----+\n",
            "| 489464|    21733|     -96|  0.0|\n",
            "| 489463|    71477|    -240|  0.0|\n",
            "| 489467|   85123A|    -192|  0.0|\n",
            "| 489521|    21646|     -50|  0.0|\n",
            "| 489655|    20683|     -44|  0.0|\n",
            "+-------+---------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "Records with Quantity = 0: 0\n",
            "\n",
            "Records with Quantity < 0 (returns): 22,950\n",
            "✅ PySpark numeric validity check completed!\n",
            "\n",
            "=== Validity Summary ===\n",
            "⚠️  Found 6,207 records with non-positive prices (possibly freebies)\n",
            "✅ No zero quantity records found\n",
            "ℹ️  Found 22,950 return transactions (negative quantities)\n"
          ]
        }
      ],
      "source": [
        "# Identify records with non-positive prices or zero quantity with error handling\n",
        "print(\"=== Numeric Values Validity Check ===\")\n",
        "\n",
        "if USE_SPARK:\n",
        "    try:\n",
        "        print(\"Using PySpark for numeric validity analysis...\")\n",
        "\n",
        "        # Check for non-positive Price values\n",
        "        invalid_price_count = df.filter(F.col(\"Price\") <= 0).count()\n",
        "        print(f\"Records with Price <= 0: {invalid_price_count:,}\")\n",
        "\n",
        "        if invalid_price_count > 0:\n",
        "            print(\"\\nSample records with invalid prices:\")\n",
        "            df.filter(F.col(\"Price\") <= 0).select(\"Invoice\", \"StockCode\", \"Quantity\", \"Price\").show(5)\n",
        "\n",
        "        # Check for zero Quantity values\n",
        "        zero_quantity_count = df.filter(F.col(\"Quantity\") == 0).count()\n",
        "        print(f\"\\nRecords with Quantity = 0: {zero_quantity_count:,}\")\n",
        "\n",
        "        if zero_quantity_count > 0:\n",
        "            print(\"\\nSample records with zero quantity:\")\n",
        "            df.filter(F.col(\"Quantity\") == 0).show(5)\n",
        "\n",
        "        # Check for negative quantities (returns)\n",
        "        negative_quantity_count = df.filter(F.col(\"Quantity\") < 0).count()\n",
        "        print(f\"\\nRecords with Quantity < 0 (returns): {negative_quantity_count:,}\")\n",
        "\n",
        "        print(\"✅ PySpark numeric validity check completed!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ PySpark numeric check failed: {e}\")\n",
        "        print(\"🔄 Switching to pandas analysis...\")\n",
        "        USE_SPARK = False\n",
        "\n",
        "if not USE_SPARK:\n",
        "    try:\n",
        "        print(\"Using pandas for numeric validity analysis...\")\n",
        "\n",
        "        # Check for non-positive Price values\n",
        "        invalid_price_mask = pandas_df[\"Price\"] <= 0\n",
        "        invalid_price_count = invalid_price_mask.sum()\n",
        "        print(f\"Records with Price <= 0: {invalid_price_count:,}\")\n",
        "\n",
        "        if invalid_price_count > 0:\n",
        "            print(\"\\nSample records with invalid prices:\")\n",
        "            print(pandas_df[invalid_price_mask][[\"Invoice\", \"StockCode\", \"Quantity\", \"Price\"]].head())\n",
        "\n",
        "        # Check for zero Quantity values\n",
        "        zero_quantity_mask = pandas_df[\"Quantity\"] == 0\n",
        "        zero_quantity_count = zero_quantity_mask.sum()\n",
        "        print(f\"\\nRecords with Quantity = 0: {zero_quantity_count:,}\")\n",
        "\n",
        "        if zero_quantity_count > 0:\n",
        "            print(\"\\nSample records with zero quantity:\")\n",
        "            print(pandas_df[zero_quantity_mask].head())\n",
        "\n",
        "        # Check for negative quantities (returns)\n",
        "        negative_quantity_mask = pandas_df[\"Quantity\"] < 0\n",
        "        negative_quantity_count = negative_quantity_mask.sum()\n",
        "        print(f\"\\nRecords with Quantity < 0 (returns): {negative_quantity_count:,}\")\n",
        "\n",
        "        print(\"✅ Pandas numeric validity check completed!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Pandas numeric check failed: {e}\")\n",
        "\n",
        "# Summary comments\n",
        "print(\"\\n=== Validity Summary ===\")\n",
        "if invalid_price_count > 0:\n",
        "    print(f\"⚠️  Found {invalid_price_count:,} records with non-positive prices (possibly freebies)\")\n",
        "else:\n",
        "    print(\"✅ All prices are positive\")\n",
        "\n",
        "if zero_quantity_count > 0:\n",
        "    print(f\"⚠️  Found {zero_quantity_count:,} records with zero quantity (unusual - could indicate data entry errors)\")\n",
        "else:\n",
        "    print(\"✅ No zero quantity records found\")\n",
        "\n",
        "if negative_quantity_count > 0:\n",
        "    print(f\"ℹ️  Found {negative_quantity_count:,} return transactions (negative quantities)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "915d3a64",
      "metadata": {
        "id": "915d3a64"
      },
      "source": [
        "## 6. Exploratory Data Analysis (EDA)\n",
        "\n",
        "Perform comprehensive exploratory data analysis using PySpark.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b5369c8",
      "metadata": {
        "id": "6b5369c8"
      },
      "source": [
        "### 6.1 Key Dataset Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "76ed05c9",
      "metadata": {
        "id": "76ed05c9",
        "outputId": "ae1138f8-4dd4-46db-9333-b64ebfa84c59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Key Dataset Metrics ===\n",
            "Total Transactions: 1,067,371\n",
            "Unique Products (StockCode): 5,305\n",
            "Unique Customers: 5,943\n",
            "Total Revenue: £19,287,250.57\n",
            "Average Order Value: £18.07\n",
            "\n",
            "=== Data Preview ===\n",
            "+-------+---------+-----------------------------------+--------+-------------------+-----+-----------+--------------+\n",
            "|Invoice|StockCode|Description                        |Quantity|InvoiceDate        |Price|Customer ID|Country       |\n",
            "+-------+---------+-----------------------------------+--------+-------------------+-----+-----------+--------------+\n",
            "|489434 |85048    |15CM CHRISTMAS GLASS BALL 20 LIGHTS|12      |2009-12-01 07:45:00|6.95 |13085.0    |United Kingdom|\n",
            "|489434 |79323P   |PINK CHERRY LIGHTS                 |12      |2009-12-01 07:45:00|6.75 |13085.0    |United Kingdom|\n",
            "|489434 |79323W   | WHITE CHERRY LIGHTS               |12      |2009-12-01 07:45:00|6.75 |13085.0    |United Kingdom|\n",
            "|489434 |22041    |RECORD FRAME 7\" SINGLE SIZE        |48      |2009-12-01 07:45:00|2.1  |13085.0    |United Kingdom|\n",
            "|489434 |21232    |STRAWBERRY CERAMIC TRINKET BOX     |24      |2009-12-01 07:45:00|1.25 |13085.0    |United Kingdom|\n",
            "+-------+---------+-----------------------------------+--------+-------------------+-----+-----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "=== Data Schema ===\n",
            "root\n",
            " |-- Invoice: string (nullable = true)\n",
            " |-- StockCode: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Quantity: long (nullable = true)\n",
            " |-- InvoiceDate: timestamp (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- Customer ID: double (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate number of distinct products and customers\n",
        "print(\"=== Key Dataset Metrics ===\")\n",
        "\n",
        "# Calculate distinct counts using Spark\n",
        "unique_products = df.select(\"StockCode\").distinct().count()\n",
        "unique_customers = df.select(\"Customer ID\").distinct().count()\n",
        "\n",
        "# Calculate total revenue\n",
        "total_revenue = df.agg(F.sum(F.col(\"Quantity\") * F.col(\"Price\")).alias(\"TotalRevenue\")).collect()[0][\"TotalRevenue\"]\n",
        "\n",
        "# Calculate total transactions\n",
        "total_transactions = df.count()\n",
        "\n",
        "print(f\"Total Transactions: {total_transactions:,}\")\n",
        "print(f\"Unique Products (StockCode): {unique_products:,}\")\n",
        "print(f\"Unique Customers: {unique_customers:,}\")\n",
        "print(f\"Total Revenue: £{total_revenue:,.2f}\")\n",
        "\n",
        "# Additional metrics\n",
        "avg_order_value = df.agg(F.avg(F.col(\"Quantity\") * F.col(\"Price\")).alias(\"AvgOrderValue\")).collect()[0][\"AvgOrderValue\"]\n",
        "print(f\"Average Order Value: £{avg_order_value:.2f}\")\n",
        "\n",
        "# Data preview\n",
        "print(\"\\n=== Data Preview ===\")\n",
        "df.show(5, truncate=False)\n",
        "\n",
        "print(\"\\n=== Data Schema ===\")\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f744a0b9",
      "metadata": {
        "id": "f744a0b9"
      },
      "source": [
        "## 7. Data Quality Summary Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1a09708f",
      "metadata": {
        "id": "1a09708f",
        "outputId": "215ea21a-d7cb-49a2-80a0-b7e6ec38cbba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "           DATA QUALITY ASSESSMENT SUMMARY\n",
            "============================================================\n",
            "\n",
            "📊 DATASET OVERVIEW:\n",
            "   • Total Records: 1,067,371\n",
            "   • Total Columns: 8\n",
            "   • Analysis Engine: PySpark\n",
            "\n",
            "🔍 DATA QUALITY ISSUES IDENTIFIED:\n",
            "\n",
            "1. MISSING VALUES:\n",
            "\n",
            "2. NUMERIC VALIDITY:\n",
            "   • Non-positive prices: 6,207\n",
            "   • Negative quantities (returns): 22,950\n",
            "\n",
            "3. RETURNS & CANCELLATIONS:\n",
            "   • Cancelled invoices: 19,494\n",
            "   • Return transactions: 22,950\n",
            "\n",
            "4. DUPLICATE RECORDS:\n",
            "   • Duplicate records: 34,335\n",
            "\n",
            "5. DATE RANGE CONSISTENCY:\n",
            "   • Date range: 2009-12-01 07:45:00 to 2011-12-09 12:50:00\n",
            "   • Expected range: 2009-12-01 to 2011-12-09\n",
            "\n",
            "📋 RECOMMENDATIONS:\n",
            "   • CustomerID missing values: Consider impact on customer analysis\n",
            "   • Return transactions: Account for net vs gross sales calculations\n",
            "   • Date range: Verify business context for partial years\n",
            "   • Duplicates: Consider removing for accurate analysis\n",
            "   • Invalid values: Review business rules for data cleaning\n",
            "\n",
            "============================================================\n",
            "           END OF DATA QUALITY ASSESSMENT\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Data Quality Summary Report\n",
        "print(\"=\" * 60)\n",
        "print(\"           DATA QUALITY ASSESSMENT SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n📊 DATASET OVERVIEW:\")\n",
        "total_records = df.count()\n",
        "total_columns = len(df.columns)\n",
        "\n",
        "print(f\"   • Total Records: {total_records:,}\")\n",
        "print(f\"   • Total Columns: {total_columns}\")\n",
        "print(f\"   • Analysis Engine: PySpark\")\n",
        "\n",
        "print(\"\\n🔍 DATA QUALITY ISSUES IDENTIFIED:\")\n",
        "\n",
        "# Missing Values Summary\n",
        "print(\"\\n1. MISSING VALUES:\")\n",
        "missing_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
        "missing_data = missing_counts.collect()[0]\n",
        "for col_name in df.columns:\n",
        "    missing_count = missing_data[col_name]\n",
        "    missing_pct = (missing_count / total_records) * 100\n",
        "    if missing_count > 0:\n",
        "        print(f\"   • {col_name}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
        "\n",
        "# Numeric Validity Summary\n",
        "print(\"\\n2. NUMERIC VALIDITY:\")\n",
        "invalid_price_count = df.filter(F.col(\"Price\") <= 0).count()\n",
        "zero_quantity_count = df.filter(F.col(\"Quantity\") == 0).count()\n",
        "negative_quantity_count = df.filter(F.col(\"Quantity\") < 0).count()\n",
        "\n",
        "if invalid_price_count > 0:\n",
        "    print(f\"   • Non-positive prices: {invalid_price_count:,}\")\n",
        "if zero_quantity_count > 0:\n",
        "    print(f\"   • Zero quantities: {zero_quantity_count:,}\")\n",
        "if negative_quantity_count > 0:\n",
        "    print(f\"   • Negative quantities (returns): {negative_quantity_count:,}\")\n",
        "\n",
        "# Returns and Cancellations Summary\n",
        "print(\"\\n3. RETURNS & CANCELLATIONS:\")\n",
        "num_cancelled = df.filter(F.col(\"Invoice\").startswith(\"C\")).count()\n",
        "num_returns = df.filter(F.col(\"Quantity\") < 0).count()\n",
        "\n",
        "print(f\"   • Cancelled invoices: {num_cancelled:,}\")\n",
        "print(f\"   • Return transactions: {num_returns:,}\")\n",
        "\n",
        "# Duplicates Summary\n",
        "print(\"\\n4. DUPLICATE RECORDS:\")\n",
        "unique_rows = df.dropDuplicates().count()\n",
        "num_duplicates = total_records - unique_rows\n",
        "if num_duplicates > 0:\n",
        "    print(f\"   • Duplicate records: {num_duplicates:,}\")\n",
        "else:\n",
        "    print(\"   • No duplicate records found\")\n",
        "\n",
        "# Date Range Summary\n",
        "print(\"\\n5. DATE RANGE CONSISTENCY:\")\n",
        "date_range = df.select(F.min(\"InvoiceDate\").alias(\"MinDate\"), F.max(\"InvoiceDate\").alias(\"MaxDate\"))\n",
        "date_info = date_range.collect()[0]\n",
        "min_date = date_info[\"MinDate\"]\n",
        "max_date = date_info[\"MaxDate\"]\n",
        "\n",
        "print(f\"   • Date range: {min_date} to {max_date}\")\n",
        "print(f\"   • Expected range: 2009-12-01 to 2011-12-09\")\n",
        "\n",
        "print(\"\\n📋 RECOMMENDATIONS:\")\n",
        "print(\"   • CustomerID missing values: Consider impact on customer analysis\")\n",
        "print(\"   • Return transactions: Account for net vs gross sales calculations\")\n",
        "print(\"   • Date range: Verify business context for partial years\")\n",
        "if num_duplicates > 0:\n",
        "    print(\"   • Duplicates: Consider removing for accurate analysis\")\n",
        "if invalid_price_count > 0 or zero_quantity_count > 0:\n",
        "    print(\"   • Invalid values: Review business rules for data cleaning\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"           END OF DATA QUALITY ASSESSMENT\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48274893",
      "metadata": {
        "id": "48274893"
      },
      "source": [
        "## 8. Troubleshooting: Restart Spark Session\n",
        "\n",
        "If you encounter connection timeout errors, run this cell to restart the Spark session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1ba03bcc",
      "metadata": {
        "id": "1ba03bcc",
        "outputId": "c386b2e7-3882-4f66-f92c-a337ad04eb20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Restarting Spark session...\n",
            "✅ Previous Spark session stopped\n",
            "✅ New Spark session initialized!\n",
            "Spark version: 4.0.1\n",
            "✅ Connection test successful: 10 rows\n"
          ]
        }
      ],
      "source": [
        "# Restart Spark session if needed\n",
        "print(\"🔄 Restarting Spark session...\")\n",
        "\n",
        "try:\n",
        "    # Stop current session\n",
        "    spark.stop()\n",
        "    print(\"✅ Previous Spark session stopped\")\n",
        "except:\n",
        "    print(\"ℹ️  No previous session to stop\")\n",
        "\n",
        "# Wait a moment\n",
        "import time\n",
        "time.sleep(2)\n",
        "\n",
        "# Restart with optimized configuration\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"OnlineRetailAnalysis\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.driver.memory\", \"1g\") \\\n",
        "    .config(\"spark.executor.memory\", \"1g\") \\\n",
        "    .config(\"spark.python.worker.timeout\", \"1800\") \\\n",
        "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
        "    .config(\"spark.network.timeout\", \"1200s\") \\\n",
        "    .config(\"spark.rpc.askTimeout\", \"1200s\") \\\n",
        "    .config(\"spark.rpc.lookupTimeout\", \"1200s\") \\\n",
        "    .config(\"spark.sql.broadcastTimeout\", \"1200s\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
        "    .master(\"local[2]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"✅ New Spark session initialized!\")\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "\n",
        "# Test the connection\n",
        "try:\n",
        "    test_df = spark.range(10)\n",
        "    test_count = test_df.count()\n",
        "    print(f\"✅ Connection test successful: {test_count} rows\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Connection test failed: {e}\")\n",
        "    print(\"Please check your Java installation and try again.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6d566bf",
      "metadata": {
        "id": "c6d566bf"
      },
      "source": [
        "### 8.1 Fix Spark on Windows: stable local dir + session rebuild\n",
        "\n",
        "This cell configures Spark to use a stable local directory on D: and rebuilds the session to avoid blockmgr/temp folder errors on Windows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5dd59a08",
      "metadata": {
        "id": "5dd59a08",
        "outputId": "cf03b193-ec32-4a82-b112-0896b3a56231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Spark rebuilt with stable local dir: D:/spark-tmp\n",
            "Spark UI: http://windows10.microdone.cn:4040\n"
          ]
        }
      ],
      "source": [
        "# Configure stable local dirs and rebuild Spark session (Windows)\n",
        "import os, time\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# 1) Create stable directories on D: (adjust if needed)\n",
        "stable_tmp = \"D:/spark-tmp\"\n",
        "stable_wh  = \"D:/spark-warehouse\"\n",
        "\n",
        "os.makedirs(stable_tmp, exist_ok=True)\n",
        "os.makedirs(stable_wh,  exist_ok=True)\n",
        "\n",
        "# 2) Environment vars so PySpark avoids system Temp\n",
        "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\"\n",
        "os.environ[\"PYSPARK_TEMP_DIR\"] = stable_tmp\n",
        "os.environ[\"TMP\"] = stable_tmp\n",
        "os.environ[\"TEMP\"] = stable_tmp\n",
        "\n",
        "# 3) Stop previous session if any\n",
        "try:\n",
        "    spark.stop()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# 4) Rebuild Spark with stable local dir and conservative timeouts\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"OnlineRetailAnalysis\") \\\n",
        "    .master(\"local[2]\") \\\n",
        "    .config(\"spark.local.dir\", stable_tmp) \\\n",
        "    .config(\"spark.sql.warehouse.dir\", stable_wh) \\\n",
        "    .config(\"spark.python.worker.timeout\", \"1800\") \\\n",
        "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
        "    .config(\"spark.network.timeout\", \"1200s\") \\\n",
        "    .config(\"spark.rpc.askTimeout\", \"1200s\") \\\n",
        "    .config(\"spark.rpc.lookupTimeout\", \"1200s\") \\\n",
        "    .config(\"spark.sql.broadcastTimeout\", \"1200s\") \\\n",
        "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "print(\"✅ Spark rebuilt with stable local dir:\", stable_tmp)\n",
        "print(\"Spark UI:\", spark.sparkContext.uiWebUrl)\n",
        "\n",
        "# Small sleep to let workers warm up\n",
        "time.sleep(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2b709b6d",
      "metadata": {
        "id": "2b709b6d",
        "outputId": "4ad26300-1be0-4348-def7-18b582235586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Spark self-test...\n",
            "Count test: 10\n",
            "✅ Spark basic count OK\n"
          ]
        }
      ],
      "source": [
        "# Connection self-test and safe repartition\n",
        "print(\"Running Spark self-test...\")\n",
        "try:\n",
        "    test = spark.range(10)\n",
        "    print(\"Count test:\", test.count())\n",
        "    print(\"✅ Spark basic count OK\")\n",
        "except Exception as e:\n",
        "    print(\"❌ Spark basic count failed:\", e)\n",
        "\n",
        "# If you already have df loaded, make it safer before heavy actions\n",
        "try:\n",
        "    from pyspark.storagelevel import StorageLevel\n",
        "    df = df.repartition(2).persist(StorageLevel.MEMORY_ONLY)\n",
        "    print(\"Repartitioned df to\", df.rdd.getNumPartitions(), \"partitions\")\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b93c816",
      "metadata": {
        "id": "6b93c816"
      },
      "source": [
        "## 9.Data Quality Summary Report (Robust)\n",
        "\n",
        "This summary uses robust missing value rules (null/NaN/empty/\"null\"-like) and consolidates key data quality indicators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ac11d196",
      "metadata": {
        "id": "ac11d196",
        "outputId": "a069f7bf-2d41-4f1c-c7c9-e81ddae4ad4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "         DATA QUALITY ASSESSMENT SUMMARY (ROBUST)\n",
            "============================================================\n",
            "❌ Spark failed in summary: An error occurred while calling o627.persist.\n",
            ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
            "This stopped SparkContext was created at:\n",
            "\n",
            "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
            "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
            "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
            "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
            "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
            "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
            "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
            "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "py4j.Gateway.invoke(Gateway.java:238)\n",
            "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
            "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
            "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
            "py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
            "java.base/java.lang.Thread.run(Thread.java:833)\n",
            "\n",
            "And it was stopped at:\n",
            "\n",
            "org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:552)\n",
            "java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
            "java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
            "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "py4j.Gateway.invoke(Gateway.java:282)\n",
            "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
            "py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
            "java.base/java.lang.Thread.run(Thread.java:833)\n",
            "\n",
            "The currently active SparkContext was created at:\n",
            "\n",
            "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n",
            "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
            "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
            "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
            "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
            "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
            "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
            "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "py4j.Gateway.invoke(Gateway.java:238)\n",
            "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
            "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
            "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
            "py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
            "java.base/java.lang.Thread.run(Thread.java:833)\n",
            "         \n",
            "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:128)\n",
            "\tat org.apache.spark.sql.classic.SparkSession.<init>(SparkSession.scala:124)\n",
            "\tat org.apache.spark.sql.classic.SparkSession.cloneSession(SparkSession.scala:265)\n",
            "\tat org.apache.spark.sql.classic.SparkSession$.getOrCloneSessionWithConfigsOff(SparkSession.scala:937)\n",
            "\tat org.apache.spark.sql.execution.CacheManager.getOrCloneSessionWithConfigsOff(CacheManager.scala:483)\n",
            "\tat org.apache.spark.sql.execution.CacheManager.cacheQueryInternal(CacheManager.scala:131)\n",
            "\tat org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:102)\n",
            "\tat org.apache.spark.sql.classic.Dataset.persist(Dataset.scala:1550)\n",
            "\tat org.apache.spark.sql.classic.Dataset.persist(Dataset.scala:232)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
            "\n",
            "🔄 Falling back to pandas for DATA QUALITY SUMMARY...\n",
            "\n",
            "📊 DATASET OVERVIEW:\n",
            "   • Total Records: 1,067,371\n",
            "   • Total Columns: 8\n",
            "\n",
            "1. MISSING VALUES (Robust):\n",
            "   • Description: 4,382 (0.41%)\n",
            "   • Customer ID: 243,007 (22.77%)\n",
            "\n",
            "2. NUMERIC VALIDITY:\n",
            "   • Non-positive prices: 6,207\n",
            "   • Negative quantities (returns): 22,950\n",
            "\n",
            "3. RETURNS & CANCELLATIONS:\n",
            "   • Cancelled invoices: 19,494\n",
            "   • Return transactions: 22,950\n",
            "\n",
            "4. DUPLICATE RECORDS:\n",
            "   • Duplicate records: 34,335\n",
            "\n",
            "5. DATE RANGE CONSISTENCY:\n",
            "   • Date range: 2009-12-01 07:45:00 to 2011-12-09 12:50:00\n",
            "   • Expected range: 2009-12-01 to 2011-12-09\n",
            "\n",
            "📋 RECOMMENDATIONS:\n",
            "   • Address missing Customer IDs before customer-level analysis\n",
            "   • Account for returns/cancellations in net sales\n",
            "   • Review non-positive prices with business rules\n",
            "   • Remove duplicates to avoid double counting\n",
            "\n",
            "============================================================\n",
            "         END OF DATA QUALITY ASSESSMENT (ROBUST)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Robust Data Quality Summary (with Spark fallback handling)\n",
        "from pyspark.sql.types import StringType, NumericType\n",
        "from pyspark.storagelevel import StorageLevel\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"         DATA QUALITY ASSESSMENT SUMMARY (ROBUST)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "USE_SPARK = True\n",
        "\n",
        "try:\n",
        "\t# Persist lightly to stabilize actions\n",
        "\tdf_cached = df.repartition(2).persist(StorageLevel.MEMORY_ONLY)\n",
        "\t# Dataset overview (avoid heavy transformations)\n",
        "\ttotal_records = df_cached.selectExpr('count(*) as c').collect()[0]['c']\n",
        "\tprint(\"\\n📊 DATASET OVERVIEW:\")\n",
        "\tprint(f\"   • Total Records: {total_records:,}\")\n",
        "\tprint(f\"   • Total Columns: {len(df_cached.columns)}\")\n",
        "\n",
        "\t# Robust missing values\n",
        "\tprint(\"\\n1. MISSING VALUES (Robust):\")\n",
        "\tschema_fields = {f.name: f.dataType for f in df_cached.schema.fields}\n",
        "\tdef miss_expr(cn):\n",
        "\t\tcolx = F.col(cn)\n",
        "\t\tdt = schema_fields[cn]\n",
        "\t\tif isinstance(dt, NumericType):\n",
        "\t\t\tcond = colx.isNull() | F.isnan(colx)\n",
        "\t\telif isinstance(dt, StringType):\n",
        "\t\t\tcond = colx.isNull() | (F.trim(colx) == \"\") | (F.lower(F.trim(colx)).isin(\"na\",\"n/a\",\"null\",\"none\"))\n",
        "\t\telse:\n",
        "\t\t\tcond = colx.isNull()\n",
        "\t\treturn F.sum(F.when(cond, 1).otherwise(0)).alias(cn)\n",
        "\n",
        "\trobust_missing_df = df_cached.agg(*[miss_expr(c) for c in df_cached.columns])\n",
        "\trobust_row = robust_missing_df.collect()[0]\n",
        "\tany_missing = False\n",
        "\tfor cn in df_cached.columns:\n",
        "\t\tmc = int(robust_row[cn])\n",
        "\t\tif mc > 0:\n",
        "\t\t\tany_missing = True\n",
        "\t\t\tpct = (mc / total_records) * 100\n",
        "\t\t\tprint(f\"   • {cn}: {mc:,} ({pct:.2f}%)\")\n",
        "\tif not any_missing:\n",
        "\t\tprint(\"   • No missing values detected under robust rules\")\n",
        "\n",
        "\t# Numeric validity\n",
        "\tprint(\"\\n2. NUMERIC VALIDITY:\")\n",
        "\tinvalid_price_count = df_cached.filter(F.col(\"Price\") <= 0).count()\n",
        "\tzero_quantity_count = df_cached.filter(F.col(\"Quantity\") == 0).count()\n",
        "\tnegative_quantity_count = df_cached.filter(F.col(\"Quantity\") < 0).count()\n",
        "\tif invalid_price_count > 0:\n",
        "\t\tprint(f\"   • Non-positive prices: {invalid_price_count:,}\")\n",
        "\tif zero_quantity_count > 0:\n",
        "\t\tprint(f\"   • Zero quantities: {zero_quantity_count:,}\")\n",
        "\tif negative_quantity_count > 0:\n",
        "\t\tprint(f\"   • Negative quantities (returns): {negative_quantity_count:,}\")\n",
        "\n",
        "\t# Returns and cancellations\n",
        "\tprint(\"\\n3. RETURNS & CANCELLATIONS:\")\n",
        "\tnum_cancelled = df_cached.filter(F.col(\"Invoice\").startswith(\"C\")).count()\n",
        "\tnum_returns = df_cached.filter(F.col(\"Quantity\") < 0).count()\n",
        "\tprint(f\"   • Cancelled invoices: {num_cancelled:,}\")\n",
        "\tprint(f\"   • Return transactions: {num_returns:,}\")\n",
        "\n",
        "\t# Duplicates\n",
        "\tprint(\"\\n4. DUPLICATE RECORDS:\")\n",
        "\tunique_rows = df_cached.dropDuplicates().count()\n",
        "\tnum_duplicates = total_records - unique_rows\n",
        "\tif num_duplicates > 0:\n",
        "\t\tprint(f\"   • Duplicate records: {num_duplicates:,}\")\n",
        "\telse:\n",
        "\t\tprint(\"   • No duplicate records found\")\n",
        "\n",
        "\t# Date range\n",
        "\tprint(\"\\n5. DATE RANGE CONSISTENCY:\")\n",
        "\tinfo = df_cached.select(F.min(\"InvoiceDate\").alias(\"MinDate\"), F.max(\"InvoiceDate\").alias(\"MaxDate\")).collect()[0]\n",
        "\tprint(f\"   • Date range: {info['MinDate']} to {info['MaxDate']}\")\n",
        "\tprint(f\"   • Expected range: 2009-12-01 to 2011-12-09\")\n",
        "\n",
        "except Exception as e:\n",
        "\t# Spark failed → fallback to pandas so the section is never empty\n",
        "\tUSE_SPARK = False\n",
        "\tprint(f\"❌ Spark failed in summary: {e}\")\n",
        "\tprint(\"🔄 Falling back to pandas for DATA QUALITY SUMMARY...\")\n",
        "\tgithub_url = \"https://raw.githubusercontent.com/Hachi630/BDAS/main/online_retail_II.xlsx\"\n",
        "\texcel_data = pd.read_excel(github_url, sheet_name=None)\n",
        "\tsheet_2009_2010 = excel_data['Year 2009-2010']\n",
        "\tsheet_2010_2011 = excel_data['Year 2010-2011']\n",
        "\tpandas_df = pd.concat([sheet_2009_2010, sheet_2010_2011], ignore_index=True)\n",
        "\n",
        "\ttotal_records = len(pandas_df)\n",
        "\tprint(\"\\n📊 DATASET OVERVIEW:\")\n",
        "\tprint(f\"   • Total Records: {total_records:,}\")\n",
        "\tprint(f\"   • Total Columns: {len(pandas_df.columns)}\")\n",
        "\n",
        "\tprint(\"\\n1. MISSING VALUES (Robust):\")\n",
        "\tdef _norm_str(x):\n",
        "\t\treturn str(x).strip().lower() if isinstance(x, str) else x\n",
        "\tmissing_counts = {}\n",
        "\tfor c in pandas_df.columns:\n",
        "\t\tcol = pandas_df[c]\n",
        "\t\tif col.dtype.kind in 'biufc':\n",
        "\t\t\tmc = int(col.isna().sum())\n",
        "\t\telse:\n",
        "\t\t\tlower = col.astype('string').str.strip().str.lower()\n",
        "\t\t\tmc = int(col.isna().sum() + (lower.isin(['','na','n/a','null','none']).sum()))\n",
        "\t\tmissing_counts[c] = mc\n",
        "\tany_missing = False\n",
        "\tfor c, mc in missing_counts.items():\n",
        "\t\tif mc > 0:\n",
        "\t\t\tany_missing = True\n",
        "\t\t\tpct = (mc / total_records) * 100\n",
        "\t\t\tprint(f\"   • {c}: {mc:,} ({pct:.2f}%)\")\n",
        "\tif not any_missing:\n",
        "\t\tprint(\"   • No missing values detected under robust rules (pandas)\")\n",
        "\n",
        "\tprint(\"\\n2. NUMERIC VALIDITY:\")\n",
        "\tinv = int((pandas_df['Price'] <= 0).sum()) if 'Price' in pandas_df.columns else 0\n",
        "\tzq = int((pandas_df['Quantity'] == 0).sum()) if 'Quantity' in pandas_df.columns else 0\n",
        "\tneg = int((pandas_df['Quantity'] < 0).sum()) if 'Quantity' in pandas_df.columns else 0\n",
        "\tif inv>0: print(f\"   • Non-positive prices: {inv:,}\")\n",
        "\tif zq>0: print(f\"   • Zero quantities: {zq:,}\")\n",
        "\tif neg>0: print(f\"   • Negative quantities (returns): {neg:,}\")\n",
        "\n",
        "\tprint(\"\\n3. RETURNS & CANCELLATIONS:\")\n",
        "\tcancelled = int(pandas_df['Invoice'].astype(str).str.startswith('C').sum()) if 'Invoice' in pandas_df.columns else 0\n",
        "\treturns = neg\n",
        "\tprint(f\"   • Cancelled invoices: {cancelled:,}\")\n",
        "\tprint(f\"   • Return transactions: {returns:,}\")\n",
        "\n",
        "\tprint(\"\\n4. DUPLICATE RECORDS:\")\n",
        "\tunique_rows = len(pandas_df.drop_duplicates())\n",
        "\tnum_duplicates = total_records - unique_rows\n",
        "\tif num_duplicates > 0:\n",
        "\t\tprint(f\"   • Duplicate records: {num_duplicates:,}\")\n",
        "\telse:\n",
        "\t\tprint(\"   • No duplicate records found\")\n",
        "\n",
        "\tprint(\"\\n5. DATE RANGE CONSISTENCY:\")\n",
        "\tprint(f\"   • Date range: {pandas_df['InvoiceDate'].min()} to {pandas_df['InvoiceDate'].max()}\")\n",
        "\tprint(f\"   • Expected range: 2009-12-01 to 2011-12-09\")\n",
        "\n",
        "finally:\n",
        "\ttry:\n",
        "\t\t# Clean up cache if used\n",
        "\t\tdf_cached.unpersist()\n",
        "\texcept Exception:\n",
        "\t\tpass\n",
        "\n",
        "print(\"\\n📋 RECOMMENDATIONS:\")\n",
        "print(\"   • Address missing Customer IDs before customer-level analysis\")\n",
        "print(\"   • Account for returns/cancellations in net sales\")\n",
        "print(\"   • Review non-positive prices with business rules\")\n",
        "try:\n",
        "\tif USE_SPARK:\n",
        "\t\tif num_duplicates > 0:\n",
        "\t\t\tprint(\"   • Remove duplicates to avoid double counting\")\n",
        "\telse:\n",
        "\t\tif num_duplicates > 0:\n",
        "\t\t\tprint(\"   • Remove duplicates to avoid double counting\")\n",
        "except Exception:\n",
        "\tpass\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"         END OF DATA QUALITY ASSESSMENT (ROBUST)\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5763227",
      "metadata": {
        "id": "b5763227"
      },
      "source": [
        "## 10.EDA with PySpark (+pandas/Matplotlib for plots)\n",
        "\n",
        "This section summarizes key counts, sales over time, quantity distribution, top/bottom products, and geographical analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1747175e",
      "metadata": {
        "id": "1747175e",
        "outputId": "6a42d85a-6c5d-401d-fc55-7cac03ac3c3e"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o648.count.\n: org.apache.spark.SparkException: [INTERNAL_ERROR] The \"count\" action failed. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace. SQLSTATE: XX000\r\n\tat org.apache.spark.SparkException$.internalError(SparkException.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:643)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:656)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.count(Dataset.scala:1499)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor78.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.sql.classic.SparkSession.sparkContext()\" because the return value of \"org.apache.spark.sql.execution.SparkPlan.session()\" is null\r\n\tat org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:68)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.metrics$lzycompute(HashAggregateExec.scala:71)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.metrics(HashAggregateExec.scala:70)\r\n\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:147)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.resetMetrics(AdaptiveSparkPlanExec.scala:245)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2233)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t... 27 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 1) Key counts and total revenue\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Calculate number of distinct products and customers\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m unique_products \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mStockCode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistinct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m unique_customers \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomer ID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdistinct()\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Compute total revenue (Quantity * Price)\u001b[39;00m\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:439\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o648.count.\n: org.apache.spark.SparkException: [INTERNAL_ERROR] The \"count\" action failed. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace. SQLSTATE: XX000\r\n\tat org.apache.spark.SparkException$.internalError(SparkException.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:643)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:656)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.count(Dataset.scala:1499)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor78.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.sql.classic.SparkSession.sparkContext()\" because the return value of \"org.apache.spark.sql.execution.SparkPlan.session()\" is null\r\n\tat org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:68)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.metrics$lzycompute(HashAggregateExec.scala:71)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.metrics(HashAggregateExec.scala:70)\r\n\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:147)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.resetMetrics(AdaptiveSparkPlanExec.scala:245)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2233)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t... 27 more\r\n"
          ]
        }
      ],
      "source": [
        "# 1) Key counts and total revenue\n",
        "# Calculate number of distinct products and customers\n",
        "unique_products = df.select(\"StockCode\").distinct().count()\n",
        "unique_customers = df.select(\"Customer ID\").distinct().count()\n",
        "\n",
        "# Compute total revenue (Quantity * Price)\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "total_revenue = df.agg(F.sum(F.col(\"Quantity\") * F.col(\"Price\")).alias(\"TotalRevenue\")).collect()[0][\"TotalRevenue\"]\n",
        "\n",
        "print(\"=== Key Dataset Metrics ===\")\n",
        "print(f\"Unique products (StockCode): {unique_products:,}\")\n",
        "print(f\"Unique customers (Customer ID): {unique_customers:,}\")\n",
        "print(f\"Total revenue: £{total_revenue:,.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e05e4183",
      "metadata": {
        "id": "e05e4183",
        "outputId": "dc15a35d-3ce1-45e4-abd0-7f27882007e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Monthly Sales (first 12 rows) ===\n",
            "+----+-----+-------------+------------------+\n",
            "|Year|Month|TotalQuantity|      TotalRevenue|\n",
            "+----+-----+-------------+------------------+\n",
            "|2009|   12|       420088| 799847.1100000143|\n",
            "|2010|    1|       375363| 624032.8919999956|\n",
            "|2010|    2|       368402| 533091.4260000042|\n",
            "|2010|    3|       489370| 765848.7609999765|\n",
            "|2010|    4|       351971| 590580.4319999823|\n",
            "|2010|    5|       364095| 615322.8300000005|\n",
            "|2010|    6|       388253| 679786.6099999842|\n",
            "|2010|    7|       302201|  575236.359999999|\n",
            "|2010|    8|       451803| 656776.3399999854|\n",
            "|2010|    9|       478262| 853650.4309999745|\n",
            "|2010|   10|       601729|1045168.3499998983|\n",
            "|2010|   11|       673856|1422654.6419998251|\n",
            "+----+-----+-------------+------------------+\n",
            "only showing top 12 rows\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"d:\\python\\lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"d:\\python\\lib\\site-packages\\py4j\\clientserver.py\", line 535, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"d:\\python\\lib\\socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[68], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m monthly_sales\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Plot line chart\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m pdf \u001b[38;5;241m=\u001b[39m \u001b[43mmonthly_sales\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m pdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(pdf\u001b[38;5;241m.\u001b[39massign(day\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[0;32m     20\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:1792\u001b[0m, in \u001b[0;36mDataFrame.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtoPandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandasDataFrameLike\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1792\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPandasConversionMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:197\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    199\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[0;32m    200\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     )\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:443\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Row]:\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m--> 443\u001b[0m         sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\py4j\\java_gateway.py:1361\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\py4j\\clientserver.py:535\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 535\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    536\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[0;32m    537\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
            "File \u001b[1;32md:\\python\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 2) Sales over time (monthly)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "monthly_sales = df.groupBy(F.year(\"InvoiceDate\").alias(\"Year\"),\n",
        "                           F.month(\"InvoiceDate\").alias(\"Month\")) \\\n",
        "                  .agg(F.sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
        "                       F.sum(F.col(\"Quantity\") * F.col(\"Price\")).alias(\"TotalRevenue\")) \\\n",
        "                  .orderBy(\"Year\", \"Month\")\n",
        "\n",
        "print(\"=== Monthly Sales (first 12 rows) ===\")\n",
        "monthly_sales.show(12)\n",
        "\n",
        "# Plot line chart\n",
        "pdf = monthly_sales.toPandas()\n",
        "pdf[\"Date\"] = pd.to_datetime(pdf.assign(day=1)[[\"Year\",\"Month\",\"day\"]])\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(14,5))\n",
        "ax[0].plot(pdf[\"Date\"], pdf[\"TotalQuantity\"], marker='o')\n",
        "ax[0].set_title(\"Monthly Total Quantity\")\n",
        "ax[0].set_xlabel(\"Month (Year-Month)\")\n",
        "ax[0].set_ylabel(\"Total Quantity\")\n",
        "ax[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "ax[1].plot(pdf[\"Date\"], pdf[\"TotalRevenue\"], color='green', marker='o')\n",
        "ax[1].set_title(\"Monthly Total Revenue\")\n",
        "ax[1].set_xlabel(\"Month (Year-Month)\")\n",
        "ax[1].set_ylabel(\"Total Revenue (£)\")\n",
        "ax[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89015bd9",
      "metadata": {
        "id": "89015bd9",
        "outputId": "df290e5a-f5ed-4c70-bf47-81fdfa0a6eb4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Quantity Quartiles (Q1, Median, Q3, P95) ===\n",
            "[1.0, 3.0, 12.0, 36.0]\n",
            "Return lines: 22,950 (2.15%)\n"
          ]
        }
      ],
      "source": [
        "# 3) Quantity distribution (sample to pandas for histogram)\n",
        "quant_sample = df.select(\"Quantity\").sample(fraction=0.1, seed=42).toPandas()\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
        "ax[0].hist(quant_sample[\"Quantity\"], bins=50, edgecolor='black', alpha=0.7)\n",
        "ax[0].set_title(\"Quantity Distribution (sample)\")\n",
        "ax[0].set_xlabel(\"Quantity\")\n",
        "ax[0].set_ylabel(\"Frequency\")\n",
        "\n",
        "# Boxplot\n",
        "ax[1].boxplot(quant_sample[\"Quantity\"], vert=True)\n",
        "ax[1].set_title(\"Quantity Boxplot (sample)\")\n",
        "ax[1].set_ylabel(\"Quantity\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Quartiles using Spark (no collect of all data)\n",
        "quartiles = df.approxQuantile(\"Quantity\", [0.25,0.5,0.75,0.95], 0.01)\n",
        "print(\"=== Quantity Quartiles (Q1, Median, Q3, P95) ===\")\n",
        "print(quartiles)\n",
        "\n",
        "# Returns share\n",
        "returns_count = df.filter(F.col(\"Quantity\") < 0).count()\n",
        "returns_pct = returns_count / df.count() * 100\n",
        "print(f\"Return lines: {returns_count:,} ({returns_pct:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "882eab76",
      "metadata": {
        "id": "882eab76",
        "outputId": "d0cd8ccc-9a41-4bdd-d222-026e297fc624"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Top 10 Products by Quantity ===\n",
            "+---------+-------------+------------------+\n",
            "|StockCode|TotalQuantity|      TotalRevenue|\n",
            "+---------+-------------+------------------+\n",
            "|    84077|       108545| 24898.22000000003|\n",
            "|   85123A|        96066|253720.01999999926|\n",
            "|   85099B|        95739|181278.51000000164|\n",
            "|    21212|        95450|  52447.5400000005|\n",
            "|    84879|        80705| 131413.8499999994|\n",
            "|    22197|        79363| 80300.07000000014|\n",
            "|    17003|        70700|14743.410000000003|\n",
            "|    21977|        56575|28373.680000000095|\n",
            "|    84991|        54366| 27216.27000000011|\n",
            "|    22492|        45384|28863.539999999855|\n",
            "+---------+-------------+------------------+\n",
            "\n",
            "\n",
            "=== Bottom 5 Products by Quantity (>0) ===\n",
            "+---------+-------------+------------+\n",
            "|StockCode|TotalQuantity|TotalRevenue|\n",
            "+---------+-------------+------------+\n",
            "|   46138D|            1|        1.95|\n",
            "|    21860|            1|        5.17|\n",
            "|    21148|            1|         0.0|\n",
            "|   85024A|            1|        1.65|\n",
            "|   84877C|            1|         0.0|\n",
            "+---------+-------------+------------+\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 4) Top and bottom products\n",
        "product_sales = df.groupBy(\"StockCode\").agg(\n",
        "    F.sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
        "    F.sum(F.col(\"Quantity\") * F.col(\"Price\")).alias(\"TotalRevenue\")\n",
        ")\n",
        "\n",
        "# Top 10 by quantity\n",
        "top_products = product_sales.orderBy(F.desc(\"TotalQuantity\")).limit(10)\n",
        "print(\"=== Top 10 Products by Quantity ===\")\n",
        "top_products.show()\n",
        "\n",
        "# Bottom 5 (exclude negative or zero)\n",
        "bottom_products = product_sales.filter(F.col(\"TotalQuantity\") > 0) \\\n",
        "                              .orderBy(F.asc(\"TotalQuantity\")).limit(5)\n",
        "print(\"\\n=== Bottom 5 Products by Quantity (>0) ===\")\n",
        "bottom_products.show()\n",
        "\n",
        "# Plot top 10 bar chart\n",
        "pdf_top = top_products.toPandas()\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.barh(pdf_top[\"StockCode\"].astype(str), pdf_top[\"TotalQuantity\"])\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel(\"Total Quantity\")\n",
        "plt.title(\"Top 10 Products by Quantity\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e3ef3cc",
      "metadata": {},
      "source": [
        "# 3. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4ef755c",
      "metadata": {},
      "source": [
        "## 3.0 Setup: Spark Initialization and Data Loading\n",
        "\n",
        "Initialize Spark session and load the Online Retail II dataset for data preparation steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "40900696",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Initializing Spark session for data preparation...\n",
            "ℹ️ No existing Spark session to stop\n",
            "✅ Spark session initialized successfully!\n",
            "📥 Loading Online Retail II dataset from GitHub...\n",
            "2009-2010 data shape: (525461, 8)\n",
            "2010-2011 data shape: (541910, 8)\n",
            "Combined data shape: (1067371, 8)\n",
            "❌ Error testing Spark connection: An error occurred while calling o76.count.\n",
            ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (windows10.microdone.cn executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\n",
            "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
            "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
            "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
            "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
            "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
            "Caused by: java.io.EOFException\n",
            "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\n",
            "\t... 25 more\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n",
            "\tat scala.Option.getOrElse(Option.scala:201)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n",
            "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n",
            "\tat scala.Option.foreach(Option.scala:437)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
            "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\n",
            "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
            "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
            "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
            "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
            "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
            "Caused by: java.io.EOFException\n",
            "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\n",
            "\t... 25 more\n",
            "\n",
            "💡 Try restarting Jupyter kernel and running this cell again\n",
            "🎯 Ready for data preparation steps!\n"
          ]
        }
      ],
      "source": [
        "# 3.0 Setup: Spark Initialization and Data Loading\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.storagelevel import StorageLevel\n",
        "\n",
        "print(\"🔄 Initializing Spark session for data preparation...\")\n",
        "\n",
        "# 1) Force stop any existing Spark session\n",
        "try:\n",
        "    spark.stop()\n",
        "    print(\"✅ Stopped existing Spark session\")\n",
        "except:\n",
        "    print(\"ℹ️ No existing Spark session to stop\")\n",
        "\n",
        "time.sleep(2)  # Wait for cleanup\n",
        "\n",
        "# 2) Create stable directories for Windows\n",
        "stable_tmp = \"D:/spark-tmp\"\n",
        "stable_wh = \"D:/spark-warehouse\"\n",
        "os.makedirs(stable_tmp, exist_ok=True)\n",
        "os.makedirs(stable_wh, exist_ok=True)\n",
        "\n",
        "# 3) Set environment variables\n",
        "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\"\n",
        "os.environ[\"PYSPARK_TEMP_DIR\"] = stable_tmp\n",
        "os.environ[\"TMP\"] = stable_tmp\n",
        "os.environ[\"TEMP\"] = stable_tmp\n",
        "\n",
        "# 4) Initialize Spark session with stable configuration\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"OnlineRetailAnalysis\") \\\n",
        "    .config(\"spark.local.dir\", stable_tmp) \\\n",
        "    .config(\"spark.sql.warehouse.dir\", stable_wh) \\\n",
        "    .config(\"spark.python.worker.timeout\", \"1200\") \\\n",
        "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
        "    .config(\"spark.network.timeout\", \"800s\") \\\n",
        "    .config(\"spark.rpc.askTimeout\", \"800s\") \\\n",
        "    .config(\"spark.rpc.lookupTimeout\", \"800s\") \\\n",
        "    .config(\"spark.sql.broadcastTimeout\", \"800s\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
        "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .master(\"local[2]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "print(\"✅ Spark session initialized successfully!\")\n",
        "\n",
        "# 5) Load data from GitHub\n",
        "print(\"📥 Loading Online Retail II dataset from GitHub...\")\n",
        "\n",
        "github_url = \"https://raw.githubusercontent.com/Hachi630/BDAS/main/online_retail_II.xlsx\"\n",
        "excel_data = pd.read_excel(github_url, sheet_name=None)\n",
        "\n",
        "# Get both sheets\n",
        "sheet_2009_2010 = excel_data['Year 2009-2010']\n",
        "sheet_2010_2011 = excel_data['Year 2010-2011']\n",
        "\n",
        "print(f\"2009-2010 data shape: {sheet_2009_2010.shape}\")\n",
        "print(f\"2010-2011 data shape: {sheet_2010_2011.shape}\")\n",
        "\n",
        "# Combine datasets\n",
        "pandas_df = pd.concat([sheet_2009_2010, sheet_2010_2011], ignore_index=True)\n",
        "print(f\"Combined data shape: {pandas_df.shape}\")\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "# Clean up pandas objects to free memory\n",
        "del pandas_df, sheet_2009_2010, sheet_2010_2011, excel_data\n",
        "\n",
        "# 6) Test Spark connection and cache DataFrame for stability\n",
        "try:\n",
        "    total_records = df.count()\n",
        "    print(f\"✅ Data loaded successfully: {total_records:,} records\")\n",
        "    \n",
        "    # Cache DataFrame for better performance in subsequent operations\n",
        "    df = df.repartition(2).persist(StorageLevel.MEMORY_ONLY)\n",
        "    print(\"✅ DataFrame cached for better performance\")\n",
        "    \n",
        "    # Show basic info\n",
        "    print(f\"Columns: {df.columns}\")\n",
        "    print(\"Sample data:\")\n",
        "    df.show(3, truncate=False)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error testing Spark connection: {e}\")\n",
        "    print(\"💡 Try restarting Jupyter kernel and running this cell again\")\n",
        "\n",
        "print(\"🎯 Ready for data preparation steps!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4454c57",
      "metadata": {},
      "source": [
        "## 3.1 Select the Data\n",
        "\n",
        "Apply explicit selection criteria to create the modeling/analysis subset: country filter, remove returns/cancellations, drop irrelevant columns, and optional timeframe filter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "34729ab0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Initial shape ===\n"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o454.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 2) (windows10.microdone.cn executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\t... 25 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Start from the current full DataFrame `df`\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Initial shape ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m init_rows \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m      8\u001b[0m init_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRows, Columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minit_rows\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minit_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:439\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcount())\n",
            "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o454.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 2) (windows10.microdone.cn executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\t... 25 more\r\n"
          ]
        }
      ],
      "source": [
        "# 3.1 Select the Data (PySpark)\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import year\n",
        "\n",
        "# Start from the current full DataFrame `df`\n",
        "print(\"=== Initial shape ===\")\n",
        "init_rows = df.count()\n",
        "init_cols = len(df.columns)\n",
        "print(f\"Rows, Columns: {init_rows:,}, {init_cols}\")\n",
        "\n",
        "# 1) Filter data to only include United Kingdom transactions\n",
        "print(\"\\n# Filter: Country == 'United Kingdom'\")\n",
        "df = df.filter(F.col(\"Country\") == \"United Kingdom\")\n",
        "print(\"After UK filter:\", df.count())\n",
        "\n",
        "# 2) Remove all returned/cancelled transactions\n",
        "#    - Exclude invoices that start with 'C'\n",
        "#    - Ensure no negative quantities (returns)\n",
        "print(\"\\n# Remove returns/cancellations\")\n",
        "if \"Invoice\" in df.columns:\n",
        "    df = df.filter(~F.col(\"Invoice\").startswith(\"C\"))\n",
        "elif \"InvoiceNo\" in df.columns:\n",
        "    df = df.filter(~F.col(\"InvoiceNo\").startswith(\"C\"))\n",
        "else:\n",
        "    print(\"Warning: No Invoice/InvoiceNo column found; cannot filter cancellations\")\n",
        "\n",
        "# Remove negative quantities\n",
        "before_neg = df.filter(F.col(\"Quantity\") < 0).count()\n",
        "df = df.filter(F.col(\"Quantity\") >= 0)\n",
        "after_neg = df.filter(F.col(\"Quantity\") < 0).count()\n",
        "print(f\"Removed returns (Quantity<0): before={before_neg:,}, after={after_neg:,}\")\n",
        "\n",
        "# 3) Drop fields not needed for modeling\n",
        "#    Adjust the drop list to your analysis scope\n",
        "print(\"\\n# Drop irrelevant columns\")\n",
        "drop_cols = []\n",
        "for c in [\"InvoiceNo\", \"Invoice\", \"Description\"]:\n",
        "    if c in df.columns:\n",
        "        drop_cols.append(c)\n",
        "if drop_cols:\n",
        "    df = df.drop(*drop_cols)\n",
        "print(\"Dropped columns:\", drop_cols)\n",
        "\n",
        "# 4) (Optional) Restrict data to 2011 only\n",
        "#    Comment out if using full range\n",
        "print(\"\\n# Optional: Restrict to year 2011\")\n",
        "df = df.filter(year(F.col(\"InvoiceDate\")) == 2011)\n",
        "print(\"After year==2011 filter:\", df.count())\n",
        "\n",
        "# Final shape and quick preview\n",
        "print(\"\\n=== Final shape ===\")\n",
        "print(\"Rows, Columns:\", df.count(), len(df.columns))\n",
        "df.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de1d189c",
      "metadata": {},
      "source": [
        "## 3.2 Data Cleaning\n",
        "\n",
        "Apply cleaning steps aligned with the analysis rationale: drop/impute missing IDs (if chosen), remove cancellations/returns to focus on actual sales, optionally remove extreme outliers, and perform light text tidying.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92f33c4d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Data Cleaning Start ===\n"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o86.count.\n: org.apache.spark.SparkException: [INTERNAL_ERROR] The \"count\" action failed. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace. SQLSTATE: XX000\r\n\tat org.apache.spark.SparkException$.internalError(SparkException.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:643)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:656)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.count(Dataset.scala:1499)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor78.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.sql.classic.SparkSession.sparkContext()\" because the return value of \"org.apache.spark.sql.execution.SparkPlan.session()\" is null\r\n\tat org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:68)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.metrics$lzycompute(HashAggregateExec.scala:71)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.metrics(HashAggregateExec.scala:70)\r\n\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:147)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.resetMetrics(AdaptiveSparkPlanExec.scala:245)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2233)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t... 27 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trim\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Data Cleaning Start ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m start_rows \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRows before cleaning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_rows\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 1) Remove transactions with missing Customer ID (optional)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#    Rationale: anonymous customers cannot be attributed in downstream analysis.\u001b[39;00m\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:439\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o86.count.\n: org.apache.spark.SparkException: [INTERNAL_ERROR] The \"count\" action failed. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace. SQLSTATE: XX000\r\n\tat org.apache.spark.SparkException$.internalError(SparkException.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:643)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:656)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.count(Dataset.scala:1499)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor78.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.sql.classic.SparkSession.sparkContext()\" because the return value of \"org.apache.spark.sql.execution.SparkPlan.session()\" is null\r\n\tat org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:68)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.metrics$lzycompute(HashAggregateExec.scala:71)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.metrics(HashAggregateExec.scala:70)\r\n\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:147)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.resetMetrics(AdaptiveSparkPlanExec.scala:245)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2233)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t... 27 more\r\n"
          ]
        }
      ],
      "source": [
        "# 3.2 Data Cleaning (PySpark)\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import trim\n",
        "\n",
        "print(\"=== Data Cleaning Start ===\")\n",
        "start_rows = df.count()\n",
        "print(f\"Rows before cleaning: {start_rows:,}\")\n",
        "\n",
        "# 1) Remove transactions with missing Customer ID (optional)\n",
        "#    Rationale: anonymous customers cannot be attributed in downstream analysis.\n",
        "missing_cust_before = df.filter(F.col(\"Customer ID\").isNull()).count() if \"Customer ID\" in df.columns else 0\n",
        "print(f\"Missing Customer ID before: {missing_cust_before:,}\")\n",
        "if \"Customer ID\" in df.columns:\n",
        "\t# Choice A (drop missing IDs) — typical if we will do any customer-level aggregations\n",
        "\tdf = df.filter(F.col(\"Customer ID\").isNotNull())\n",
        "\t# Choice B (alternative if keeping):\n",
        "\t# df = df.fillna({\"Customer ID\": -1})\n",
        "\n",
        "# 2) Exclude cancelled orders and returned items\n",
        "#    Rationale: focus on final sales that can contribute to waste.\n",
        "if \"Invoice\" in df.columns:\n",
        "\tdf = df.filter(~F.col(\"Invoice\").startswith(\"C\"))\n",
        "elif \"InvoiceNo\" in df.columns:\n",
        "\tdf = df.filter(~F.col(\"InvoiceNo\").startswith(\"C\"))\n",
        "\n",
        "neg_before = df.filter(F.col(\"Quantity\") < 0).count()\n",
        "df = df.filter(F.col(\"Quantity\") >= 0)\n",
        "print(f\"Negative quantities removed: {neg_before:,}\")\n",
        "\n",
        "# 3) (Optional) Remove extreme outliers by Quantity threshold\n",
        "#    Rationale: avoid a handful of bulk orders dominating statistics (tune threshold per EDA).\n",
        "#    Uncomment to apply, and print how many would be removed.\n",
        "# outlier_threshold = 5000\n",
        "# out_before = df.filter(F.col(\"Quantity\") >= outlier_threshold).count()\n",
        "# df = df.filter(F.col(\"Quantity\") < outlier_threshold)\n",
        "# print(f\"Removed extreme outliers (Quantity >= {outlier_threshold}): {out_before:,}\")\n",
        "\n",
        "# 4) Light text tidying (strip Description whitespace)\n",
        "if \"Description\" in df.columns:\n",
        "\tdf = df.withColumn(\"Description\", trim(F.col(\"Description\")))\n",
        "\n",
        "# 5) (Optional) Drop unneeded text columns to reduce width\n",
        "# if \"Description\" in df.columns:\n",
        "# \tdf = df.drop(\"Description\")\n",
        "\n",
        "# Sanity checks after cleaning\n",
        "missing_cust_after = df.filter(F.col(\"Customer ID\").isNull()).count() if \"Customer ID\" in df.columns else 0\n",
        "neg_after = df.filter(F.col(\"Quantity\") < 0).count()\n",
        "\n",
        "print(\"\\n=== Cleaning Summary ===\")\n",
        "print(f\"Missing Customer ID after: {missing_cust_after:,}\")\n",
        "print(f\"Negative quantities after: {neg_after:,}\")\n",
        "\n",
        "print(\"\\n=== Final shape ===\")\n",
        "print(\"Rows, Columns:\", df.count(), len(df.columns))\n",
        "df.show(5, truncate=False)\n",
        "print(\"=== Data Cleaning Done ===\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1878bcbc",
      "metadata": {},
      "source": [
        "## 3.3 Feature Engineering\n",
        "\n",
        "Engineer transaction-level and product-level features to support downstream modeling and risk analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9df3c7ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.3 Feature Engineering (PySpark)\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "print(\"=== Feature Engineering Start ===\")\n",
        "\n",
        "# 1) TotalPrice of each line (quantity * unit price)\n",
        "#    Note: column names might be Price or UnitPrice depending on sheet; handle both.\n",
        "unit_col = \"UnitPrice\" if \"UnitPrice\" in df.columns else (\"Price\" if \"Price\" in df.columns else None)\n",
        "if unit_col is None:\n",
        "\traise ValueError(\"Neither 'UnitPrice' nor 'Price' column found.\")\n",
        "\n",
        "df = df.withColumn(\"TotalPrice\", F.col(\"Quantity\") * F.col(unit_col))\n",
        "\n",
        "# 2) Ensure InvoiceDate is timestamp and derive date features\n",
        "if dict(df.dtypes).get(\"InvoiceDate\") != \"timestamp\":\n",
        "\tdf = df.withColumn(\"InvoiceDate\", F.to_timestamp(\"InvoiceDate\"))\n",
        "\n",
        "# InvoiceMonth as yyyy-MM string\n",
        "df = df.withColumn(\"InvoiceMonth\", F.date_format(\"InvoiceDate\", \"yyyy-MM\"))  # e.g., 2011-12\n",
        "# Numeric day of week (1=Sunday ... 7=Saturday)\n",
        "df = df.withColumn(\"DayOfWeekNum\", F.dayofweek(\"InvoiceDate\"))\n",
        "# Holiday season flag: November or December\n",
        "df = df.withColumn(\"IsHolidaySeason\", F.when(F.month(\"InvoiceDate\").isin([11,12]), 1).otherwise(0))\n",
        "\n",
        "print(\"Added: TotalPrice, InvoiceMonth, DayOfWeekNum, IsHolidaySeason\")\n",
        "\n",
        "# 3) Product-level aggregation (StockCode)\n",
        "#    Aggregate total quantity, revenue, number of distinct invoices, first/last sale dates\n",
        "invoice_col = \"InvoiceNo\" if \"InvoiceNo\" in df.columns else (\"Invoice\" if \"Invoice\" in df.columns else None)\n",
        "if invoice_col is None:\n",
        "\traise ValueError(\"Neither 'InvoiceNo' nor 'Invoice' column found for distinct invoice counts.\")\n",
        "\n",
        "product_stats = df.groupBy(\"StockCode\").agg(\n",
        "\tF.sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
        "\tF.count_distinct(F.col(invoice_col)).alias(\"NumInvoices\"),\n",
        "\tF.sum(\"TotalPrice\").alias(\"TotalRevenue\"),\n",
        "\tF.min(\"InvoiceDate\").alias(\"FirstSaleDate\"),\n",
        "\tF.max(\"InvoiceDate\").alias(\"LastSaleDate\")\n",
        ")\n",
        "\n",
        "print(\"Product-level features computed: TotalQuantity, NumInvoices, TotalRevenue, First/LastSaleDate\")\n",
        "\n",
        "# 4) Recency: days since last sale (relative to max date in dataset)\n",
        "max_date = df.agg(F.max(\"InvoiceDate\").alias(\"maxDate\")).collect()[0][\"maxDate\"]\n",
        "product_stats = product_stats.withColumn(\"DaysSinceLastSale\", F.datediff(F.lit(max_date), F.col(\"LastSaleDate\")))\n",
        "print(\"Added: DaysSinceLastSale (recency)\")\n",
        "\n",
        "# 5) WasteRisk label: low sales and stale recency → high risk\n",
        "#    Simple rule; adjust thresholds based on your analysis\n",
        "product_stats = product_stats.withColumn(\n",
        "\t\"WasteRisk\",\n",
        "\tF.when((F.col(\"TotalQuantity\") < 5) & (F.col(\"DaysSinceLastSale\") > 180), 1).otherwise(0)\n",
        ")\n",
        "print(\"Added: WasteRisk (1 = low sales & no sale in >180 days)\")\n",
        "\n",
        "# Preview results\n",
        "print(\"\\n=== Transaction-level columns added ===\")\n",
        "df.select(\"StockCode\", unit_col, \"Quantity\", \"TotalPrice\", \"InvoiceDate\", \"InvoiceMonth\", \"DayOfWeekNum\", \"IsHolidaySeason\").show(5, truncate=False)\n",
        "\n",
        "print(\"\\n=== Product-level features (sample) ===\")\n",
        "product_stats.select(\"StockCode\", \"TotalQuantity\", \"NumInvoices\", \"TotalRevenue\", \"DaysSinceLastSale\", \"WasteRisk\").orderBy(F.desc(\"TotalQuantity\")).show(10, truncate=False)\n",
        "\n",
        "print(\"=== Feature Engineering Done ===\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4095b574",
      "metadata": {},
      "source": [
        "## 3.4 Integrate Various Data Sources\n",
        "\n",
        "Read and combine multiple files (if any), join extra product information, and verify the integrated dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "895854ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.4 Integrate Various Data Sources (PySpark)\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "print(\"=== Data Source Integration ===\")\n",
        "\n",
        "# A) Read and combine multiple files (if provided separately)\n",
        "#    Example for two yearly CSVs. Commented out by default — uncomment if used.\n",
        "# df1 = spark.read.csv(\"online_retail_II_2009_2010.csv\", header=True, inferSchema=True)\n",
        "# df2 = spark.read.csv(\"online_retail_II_2010_2011.csv\", header=True, inferSchema=True)\n",
        "# df = df1.unionByName(df2)  # Combine two yearly datasets into one DataFrame\n",
        "# print(\"After union:\", df.count(), \"rows |\", len(df.columns), \"columns\")\n",
        "# df.printSchema()\n",
        "\n",
        "# B) Join additional product info (e.g., categories) keyed by StockCode\n",
        "#    Example: join transaction-level df with a product catalog\n",
        "# product_info = spark.read.csv(\"product_catalog.csv\", header=True, inferSchema=True)\n",
        "# df = df.join(product_info, on=\"StockCode\", how=\"left\")  # Add product category information by StockCode\n",
        "# print(\"After left join with product_info:\", df.count(), \"rows |\", len(df.columns), \"columns\")\n",
        "\n",
        "# C) Alternatively, join at product level after aggregation\n",
        "#    Example: product-level catalog joined to product_stats\n",
        "# product_catalog = spark.read.csv(\"product_catalog.csv\", header=True, inferSchema=True)\n",
        "# product_stats = product_stats.join(product_catalog, on=\"StockCode\", how=\"left\")\n",
        "# print(\"product_stats columns after join:\", product_stats.columns)\n",
        "\n",
        "# D) Verify combined dataset (min/max date, sample)\n",
        "try:\n",
        "\tmin_max = df.select(F.min(\"InvoiceDate\").alias(\"minDate\"), F.max(\"InvoiceDate\").alias(\"maxDate\")).collect()[0]\n",
        "\tprint(f\"Date range after integration: {min_max['minDate']} → {min_max['maxDate']}\")\n",
        "except Exception as e:\n",
        "\tprint(\"Date verification skipped:\", e)\n",
        "\n",
        "print(\"Sample rows:\")\n",
        "df.limit(5).show(truncate=False)\n",
        "\n",
        "print(\"=== Integration Section Complete ===\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdcf32bf",
      "metadata": {},
      "source": [
        "## 3.5 Format the Data as Required\n",
        "\n",
        "Convert data types, optionally encode categorical variables, assemble and scale features for modeling, and (optionally) split into train/test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2881fd38",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.5 Format the Data as Required (PySpark)\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "\n",
        "print(\"=== Data Formatting Start ===\")\n",
        "\n",
        "# A) Convert data types (InvoiceDate to timestamp; Quantity to int; UnitPrice to double)\n",
        "if dict(df.dtypes).get(\"InvoiceDate\") != \"timestamp\":\n",
        "\tdf = df.withColumn(\"InvoiceDate\", F.to_timestamp(\"InvoiceDate\"))\n",
        "\n",
        "# If fields came as strings, cast them (guard with existence checks)\n",
        "if dict(df.dtypes).get(\"Quantity\") != \"int\":\n",
        "\tdf = df.withColumn(\"Quantity\", F.col(\"Quantity\").cast(\"int\"))\n",
        "\n",
        "if \"UnitPrice\" in df.columns and dict(df.dtypes).get(\"UnitPrice\") != \"double\":\n",
        "\tdf = df.withColumn(\"UnitPrice\", F.col(\"UnitPrice\").cast(\"double\"))\n",
        "elif \"Price\" in df.columns and dict(df.dtypes).get(\"Price\") != \"double\":\n",
        "\tdf = df.withColumn(\"Price\", F.col(\"Price\").cast(\"double\"))\n",
        "\n",
        "print(\"Schema after type fixes:\")\n",
        "df.printSchema()\n",
        "\n",
        "# B) (Optional) Encode categorical variables\n",
        "# Example: Country (if retained) → index + one-hot (commented out by default)\n",
        "# if \"Country\" in df.columns:\n",
        "# \tindexer = StringIndexer(inputCol=\"Country\", outputCol=\"CountryIndex\", handleInvalid=\"keep\")\n",
        "# \tdf_indexed = indexer.fit(df).transform(df)\n",
        "# \tencoder = OneHotEncoder(inputCol=\"CountryIndex\", outputCol=\"CountryVec\")\n",
        "# \tdf = encoder.fit(df_indexed).transform(df_indexed)\n",
        "# \tprint(\"Added encoded Country features (CountryIndex, CountryVec)\")\n",
        "\n",
        "# C) Assemble and scale product-level features (for modeling)\n",
        "#    Use product_stats created in Feature Engineering\n",
        "assembler = VectorAssembler(\n",
        "\tinputCols=[\"TotalQuantity\", \"TotalRevenue\", \"DaysSinceLastSale\"],\n",
        "\toutputCol=\"features\"\n",
        ")\n",
        "product_vector_df = assembler.transform(product_stats)\n",
        "\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withMean=True, withStd=True)\n",
        "scalerModel = scaler.fit(product_vector_df)\n",
        "product_scaled_df = scalerModel.transform(product_vector_df)\n",
        "\n",
        "print(\"Sample scaled features:\")\n",
        "product_scaled_df.select(\"StockCode\", \"features\", \"scaledFeatures\", \"WasteRisk\").show(5, truncate=False)\n",
        "\n",
        "# D) (Optional) Split into train/test sets for modeling\n",
        "train_df, test_df = product_scaled_df.randomSplit([0.8, 0.2], seed=42)\n",
        "print(f\"Train size: {train_df.count()} | Test size: {test_df.count()}\")\n",
        "\n",
        "print(f\"Final product dataset count: {product_stats.count()} products\")\n",
        "product_stats.select(\"StockCode\",\"TotalQuantity\",\"WasteRisk\").show(5, truncate=False)\n",
        "\n",
        "print(\"=== Data Formatting Done ===\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb0b4d6",
      "metadata": {},
      "source": [
        "# 4. Data transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91a75f15",
      "metadata": {},
      "source": [
        "## 4.1 Reduce the Data\n",
        "\n",
        "Drop redundant features, optionally apply PCA to reduce dimensionality, and (optionally) sample for quick prototyping. Show the effect after reduction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c270e9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.1 Reduce the Data (PySpark)\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import PCA as PCAml\n",
        "\n",
        "print(\"=== Data Reduction Start ===\")\n",
        "\n",
        "# A) Feature selection: drop redundant/less-informative columns\n",
        "#    Example: drop NumInvoices if highly correlated with TotalQuantity\n",
        "if \"NumInvoices\" in product_stats.columns:\n",
        "\tproduct_stats = product_stats.drop(\"NumInvoices\")\n",
        "\tprint(\"Dropped feature: NumInvoices (assumed correlated with TotalQuantity)\")\n",
        "\n",
        "print(\"Remaining features in product_stats:\", product_stats.columns)\n",
        "\n",
        "# B) (Optional) PCA to reduce dimensionality\n",
        "#    Assemble selected numeric features into a vector for PCA\n",
        "pca_input_cols = [\"TotalQuantity\", \"TotalRevenue\", \"DaysSinceLastSale\"]\n",
        "assembler = VectorAssembler(inputCols=pca_input_cols, outputCol=\"featVec\")\n",
        "prod_vec = assembler.transform(product_stats)\n",
        "\n",
        "# Keep 2 principal components (adjust k as needed)\n",
        "pca = PCAml(k=2, inputCol=\"featVec\", outputCol=\"pcaFeatures\")\n",
        "pca_model = pca.fit(prod_vec)\n",
        "prod_pca = pca_model.transform(prod_vec)\n",
        "\n",
        "print(\"Explained variance by PCs:\", pca_model.explainedVariance.toArray())\n",
        "print(\"Example pcaFeatures:\")\n",
        "prod_pca.select(\"StockCode\", \"pcaFeatures\", \"WasteRisk\").show(5, truncate=False)\n",
        "\n",
        "# C) (Optional) Sampling for quick prototyping\n",
        "# sample_df = product_stats.sample(withReplacement=False, fraction=0.5, seed=1)\n",
        "# print(f\"Sample size (50%): {sample_df.count()}\")\n",
        "\n",
        "print(f\"Number of products considered: {product_stats.count()}\")\n",
        "print(\"=== Data Reduction Done ===\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bab49947",
      "metadata": {},
      "source": [
        "## 4.2 Project the Data\n",
        "\n",
        "Apply advanced projections/transformations (PCA, logs, interactions, optional clustering) to prepare data for modeling or visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1937e3b7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Data Projection Start ===\n",
            "PCA demo skipped (train_df not available?): name 'train_df' is not defined\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'product_stats' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA demo skipped (train_df not available?):\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# B) Log-transform skewed features (example)\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m product_stats \u001b[38;5;241m=\u001b[39m product_stats\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogTotalSales\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mlog1p(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotalQuantity\u001b[39m\u001b[38;5;124m\"\u001b[39m)))  \u001b[38;5;66;03m# log(1+x)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdded LogTotalSales (log1p of TotalQuantity)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# C) Example interaction feature\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'product_stats' is not defined"
          ]
        }
      ],
      "source": [
        "# 4.2 Project the Data (PySpark)\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import PCA as PCAml\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "print(\"=== Data Projection Start ===\")\n",
        "\n",
        "# A) PCA on training set (if already split in 3.5)\n",
        "try:\n",
        "\tassembler = VectorAssembler(inputCols=[\"TotalQuantity\", \"TotalRevenue\", \"DaysSinceLastSale\"], outputCol=\"featVec\")\n",
        "\ttrain_vec = assembler.transform(train_df)\n",
        "\tpca = PCAml(k=3, inputCol=\"featVec\", outputCol=\"pcaFeatures\")\n",
        "\tpca_model = pca.fit(train_vec)\n",
        "\ttrain_pca = pca_model.transform(train_vec)\n",
        "\tprint(\"Explained variance by 3 PCs:\", pca_model.explainedVariance.toArray())\n",
        "\ttrain_pca.select(\"pcaFeatures\").show(5, truncate=False)\n",
        "except Exception as e:\n",
        "\tprint(\"PCA demo skipped (train_df not available?):\", e)\n",
        "\n",
        "# B) Log-transform skewed features (example)\n",
        "product_stats = product_stats.withColumn(\"LogTotalSales\", F.log1p(F.col(\"TotalQuantity\")))  # log(1+x)\n",
        "print(\"Added LogTotalSales (log1p of TotalQuantity)\")\n",
        "\n",
        "# C) Example interaction feature\n",
        "product_stats = product_stats.withColumn(\"Revenue_x_Recency\", F.col(\"TotalRevenue\") * F.col(\"DaysSinceLastSale\"))\n",
        "print(\"Added Revenue_x_Recency interaction feature\")\n",
        "\n",
        "# D) (Optional) KMeans clustering on product features\n",
        "try:\n",
        "\tvec_for_cluster = VectorAssembler(inputCols=[\"TotalQuantity\", \"TotalRevenue\", \"DaysSinceLastSale\"], outputCol=\"featVec\")\n",
        "\tprod_vec2 = vec_for_cluster.transform(product_stats)\n",
        "\tkmeans = KMeans(k=3, featuresCol=\"featVec\", predictionCol=\"ClusterID\", seed=42)\n",
        "\tkmodel = kmeans.fit(prod_vec2)\n",
        "\tproduct_stats = kmodel.transform(prod_vec2)\n",
        "\tprint(\"Cluster sizes:\")\n",
        "\tproduct_stats.groupBy(\"ClusterID\").count().orderBy(\"ClusterID\").show()\n",
        "except Exception as e:\n",
        "\tprint(\"KMeans demo skipped:\", e)\n",
        "\n",
        "print(\"Columns now available in product_stats:\")\n",
        "print(product_stats.columns)\n",
        "print(\"=== Data Projection Done ===\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149b06a7",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8663fe3c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
