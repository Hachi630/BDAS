{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd284f1",
   "metadata": {},
   "source": [
    "# PySpark Online Retail II Dataset Analysis\n",
    "\n",
    "This notebook demonstrates how to load and analyze the Online Retail II dataset using PySpark in Google Colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5c825",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n",
    "\n",
    "First, install PySpark and related dependencies in Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "862c8810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in d:\\anaconda3\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: pandas in d:\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: openpyxl in d:\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in d:\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.9)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\anaconda3\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in d:\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install pyspark pandas openpyxl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa7021",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Initialize Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af7c7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized successfully!\n",
      "Spark version: 4.0.1\n",
      "Spark UI: http://windows10.microdone.cn:4040\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, sum as spark_sum, count, when, isnan, isnull, desc, min as spark_min, max as spark_max\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set environment variables for Windows\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python'\n",
    "\n",
    "# Initialize Spark session with enhanced configuration for Windows\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OnlineRetailAnalysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"1200\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.pythonUDF.arrow.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.rpc.askTimeout\", \"800s\") \\\n",
    "    .config(\"spark.rpc.lookupTimeout\", \"800s\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"800s\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce output noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"Spark session initialized successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d416c",
   "metadata": {},
   "source": [
    "## 3. Load Data from GitHub\n",
    "\n",
    "Since PySpark cannot directly read Excel files, we use pandas to read from GitHub and then convert to Spark DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332dfa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Excel file from GitHub...\n",
      "Loading data from both sheets (2009-2010 and 2010-2011)...\n",
      "2009-2010 data shape: (525461, 8)\n",
      "2010-2011 data shape: (541910, 8)\n",
      "Combined data shape: (1067371, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:425: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Could not convert 'C489449' with type str: tried to convert to int64\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from GitHub into Spark DataFrame!\n",
      "Pandas objects cleaned up to free memory.\n"
     ]
    }
   ],
   "source": [
    "# Use pandas to read Excel file from GitHub (PySpark doesn't support Excel directly)\n",
    "print(\"Reading Excel file from GitHub...\")\n",
    "\n",
    "# GitHub repository information\n",
    "github_user = \"Hachi630\"\n",
    "github_repo = \"BDAS\"\n",
    "file_path = \"online_retail_II.xlsx\"\n",
    "\n",
    "# Construct GitHub raw URL\n",
    "github_url = f\"https://raw.githubusercontent.com/{github_user}/{github_repo}/main/{file_path}\"\n",
    "\n",
    "# Read Excel file with multiple sheets\n",
    "print(\"Loading data from both sheets (2009-2010 and 2010-2011)...\")\n",
    "excel_data = pd.read_excel(github_url, sheet_name=None)  # Read all sheets\n",
    "\n",
    "# Get the two sheets\n",
    "sheet_2009_2010 = excel_data['Year 2009-2010']\n",
    "sheet_2010_2011 = excel_data['Year 2010-2011']\n",
    "\n",
    "print(f\"2009-2010 data shape: {sheet_2009_2010.shape}\")\n",
    "print(f\"2010-2011 data shape: {sheet_2010_2011.shape}\")\n",
    "\n",
    "# Combine both datasets\n",
    "pandas_df = pd.concat([sheet_2009_2010, sheet_2010_2011], ignore_index=True)\n",
    "print(f\"Combined data shape: {pandas_df.shape}\")\n",
    "\n",
    "# Convert pandas DataFrame to Spark DataFrame\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "# Clean up pandas objects to free memory\n",
    "del pandas_df, sheet_2009_2010, sheet_2010_2011, excel_data\n",
    "\n",
    "print(\"Data successfully loaded from GitHub into Spark DataFrame!\")\n",
    "print(\"Pandas objects cleaned up to free memory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d57336",
   "metadata": {},
   "source": [
    "## 4. Check Data Dimensions\n",
    "\n",
    "Determine the number of rows and columns in the combined dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53a08fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Dimension Information ===\n",
      "Counting rows...\n",
      "‚ùå Error during data dimension check: An error occurred while calling o80.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.nio.file.NoSuchFileException: C:\\Users\\87190\\AppData\\Local\\Temp\\blockmgr-036add69-0553-4f5f-b972-4fc7a2c3463f\\05\n",
      "java.nio.file.NoSuchFileException: C:\\Users\\87190\\AppData\\Local\\Temp\\blockmgr-036add69-0553-4f5f-b972-4fc7a2c3463f\\05\n",
      "\tat java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:85)\n",
      "\tat java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)\n",
      "\tat java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:108)\n",
      "\tat java.base/sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:521)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Files.java:700)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:110)\n",
      "\tat org.apache.spark.storage.DiskStore.remove(DiskStore.scala:134)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2119)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:1460)\n",
      "\tat org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1965)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:155)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:100)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n",
      "\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n",
      "\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1728)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1710)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1676)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1470)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1443)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3154)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1686)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1470)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1443)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3154)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "Caused by: java.nio.file.NoSuchFileException: C:\\Users\\87190\\AppData\\Local\\Temp\\blockmgr-036add69-0553-4f5f-b972-4fc7a2c3463f\\05\n",
      "\tat java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:85)\n",
      "\tat java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)\n",
      "\tat java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:108)\n",
      "\tat java.base/sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:521)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Files.java:700)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:110)\n",
      "\tat org.apache.spark.storage.DiskStore.remove(DiskStore.scala:134)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2119)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:1460)\n",
      "\tat org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1965)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:155)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:100)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n",
      "\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n",
      "\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1728)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1710)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1676)\n",
      "\t... 6 more\n",
      "\n",
      "\n",
      "üîß Troubleshooting steps:\n",
      "1. Restart the kernel and run all cells again\n",
      "2. Check if Java is properly installed\n",
      "3. Try reducing memory allocation in Spark config\n",
      "4. Consider using pandas-only analysis for this dataset\n",
      "\n",
      "üîÑ Attempting fallback analysis...\n",
      "Dataset column count: 8\n",
      "Column names: ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country']\n",
      "Note: Row count unavailable due to Spark connection issues\n"
     ]
    }
   ],
   "source": [
    "# Check data dimensions using PySpark with error handling\n",
    "print(\"=== Data Dimension Information ===\")\n",
    "\n",
    "try:\n",
    "    # Get row count with retry mechanism\n",
    "    print(\"Counting rows...\")\n",
    "    row_count = df.count()\n",
    "    print(f\"Dataset row count: {row_count:,}\")\n",
    "    \n",
    "    # Get column count and names\n",
    "    column_count = len(df.columns)\n",
    "    column_names = df.columns\n",
    "    \n",
    "    print(f\"Dataset column count: {column_count}\")\n",
    "    print(f\"Column names: {column_names}\")\n",
    "    \n",
    "    # Additional information\n",
    "    print(f\"\\nDataset partitions: {df.rdd.getNumPartitions()}\")\n",
    "    print(f\"Dataset storage level: {df.storageLevel}\")\n",
    "    \n",
    "    print(\"‚úÖ Data dimension check completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during data dimension check: {e}\")\n",
    "    print(\"\\nüîß Troubleshooting steps:\")\n",
    "    print(\"1. Restart the kernel and run all cells again\")\n",
    "    print(\"2. Check if Java is properly installed\")\n",
    "    print(\"3. Try reducing memory allocation in Spark config\")\n",
    "    print(\"4. Consider using pandas-only analysis for this dataset\")\n",
    "    \n",
    "    # Fallback: try to get basic info without count()\n",
    "    try:\n",
    "        print(\"\\nüîÑ Attempting fallback analysis...\")\n",
    "        column_count = len(df.columns)\n",
    "        column_names = df.columns\n",
    "        print(f\"Dataset column count: {column_count}\")\n",
    "        print(f\"Column names: {column_names}\")\n",
    "        print(\"Note: Row count unavailable due to Spark connection issues\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Fallback also failed: {e2}\")\n",
    "        print(\"Please restart the kernel and try again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d0df9",
   "metadata": {},
   "source": [
    "## 5. Data Quality Assessment\n",
    "\n",
    "This section performs comprehensive data quality checks to identify potential issues in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f711b0",
   "metadata": {},
   "source": [
    "### 5.1 Missing Values Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "889f315c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Missing Values Analysis ===\n",
      "Testing Spark connection...\n",
      "‚ùå PySpark analysis failed: An error occurred while calling o80.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.nio.file.NoSuchFileException: C:\\Users\\87190\\AppData\\Local\\Temp\\blockmgr-036add69-0553-4f5f-b972-4fc7a2c3463f\\08\n",
      "java.nio.file.NoSuchFileException: C:\\Users\\87190\\AppData\\Local\\Temp\\blockmgr-036add69-0553-4f5f-b972-4fc7a2c3463f\\08\n",
      "\tat java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:85)\n",
      "\tat java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)\n",
      "\tat java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:108)\n",
      "\tat java.base/sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:521)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Files.java:700)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:110)\n",
      "\tat org.apache.spark.storage.DiskStore.remove(DiskStore.scala:134)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2119)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:1460)\n",
      "\tat org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1965)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:155)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:100)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n",
      "\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n",
      "\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1728)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1710)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1676)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1470)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1443)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3154)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1686)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1470)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1443)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3154)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "Caused by: java.nio.file.NoSuchFileException: C:\\Users\\87190\\AppData\\Local\\Temp\\blockmgr-036add69-0553-4f5f-b972-4fc7a2c3463f\\08\n",
      "\tat java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:85)\n",
      "\tat java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)\n",
      "\tat java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:108)\n",
      "\tat java.base/sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:521)\n",
      "\tat java.base/java.nio.file.Files.createDirectory(Files.java:700)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:110)\n",
      "\tat org.apache.spark.storage.DiskStore.remove(DiskStore.scala:134)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2119)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:1460)\n",
      "\tat org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1965)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:155)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:100)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n",
      "\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n",
      "\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1728)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1710)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1676)\n",
      "\t... 6 more\n",
      "\n",
      "\n",
      "üîÑ Switching to pandas analysis...\n",
      "Re-reading data with pandas for analysis...\n",
      "Missing values per column:\n",
      "Invoice             0\n",
      "StockCode           0\n",
      "Description      4382\n",
      "Quantity            0\n",
      "InvoiceDate         0\n",
      "Price               0\n",
      "Customer ID    243007\n",
      "Country             0\n",
      "dtype: int64\n",
      "\n",
      "Total records: 1,067,371\n",
      "\n",
      "Missing values summary:\n",
      "Invoice: 0 (0.00%)\n",
      "  ‚Üí Invoice is complete (no missing values)\n",
      "StockCode: 0 (0.00%)\n",
      "  ‚Üí StockCode is complete (no missing values)\n",
      "Description: 4,382 (0.41%)\n",
      "  ‚Üí Description has some missing values\n",
      "Quantity: 0 (0.00%)\n",
      "  ‚Üí Quantity is complete (no missing values)\n",
      "InvoiceDate: 0 (0.00%)\n",
      "  ‚Üí InvoiceDate is complete (no missing values)\n",
      "Price: 0 (0.00%)\n",
      "  ‚Üí Price is complete (no missing values)\n",
      "Customer ID: 243,007 (22.77%)\n",
      "  ‚Üí CustomerID has many missing entries (243,007 records)\n",
      "Country: 0 (0.00%)\n",
      "  ‚Üí Country is complete (no missing values)\n",
      "‚úÖ Missing values analysis completed successfully with pandas!\n"
     ]
    }
   ],
   "source": [
    "# Count nulls in each column with robust error handling\n",
    "print(\"=== Missing Values Analysis ===\")\n",
    "\n",
    "# Try PySpark first, fallback to pandas if needed\n",
    "USE_SPARK = True\n",
    "\n",
    "try:\n",
    "    # Test Spark connection first\n",
    "    print(\"Testing Spark connection...\")\n",
    "    test_count = df.count()\n",
    "    print(f\"‚úÖ Spark connection successful! Dataset has {test_count:,} records\")\n",
    "    \n",
    "    # Count missing values in each column using PySpark\n",
    "    print(\"Counting missing values using PySpark...\")\n",
    "    missing_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "    \n",
    "    print(\"Missing values per column:\")\n",
    "    missing_counts.show()\n",
    "    \n",
    "    # Get total count for percentage calculation\n",
    "    total_records = df.count()\n",
    "    print(f\"\\nTotal records: {total_records:,}\")\n",
    "    \n",
    "    # Calculate and display missing percentages\n",
    "    print(\"\\nMissing values summary:\")\n",
    "    missing_data = missing_counts.collect()[0]\n",
    "    for col_name in df.columns:\n",
    "        missing_count = missing_data[col_name]\n",
    "        missing_pct = (missing_count / total_records) * 100\n",
    "        print(f\"{col_name}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "        \n",
    "        # Add specific comment for CustomerID\n",
    "        if col_name == \"Customer ID\" and missing_count > 0:\n",
    "            print(f\"  ‚Üí CustomerID has many missing entries ({missing_count:,} records)\")\n",
    "        elif missing_count == 0:\n",
    "            print(f\"  ‚Üí {col_name} is complete (no missing values)\")\n",
    "        else:\n",
    "            print(f\"  ‚Üí {col_name} has some missing values\")\n",
    "    \n",
    "    print(\"‚úÖ Missing values analysis completed successfully with PySpark!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå PySpark analysis failed: {e}\")\n",
    "    print(\"\\nüîÑ Switching to pandas analysis...\")\n",
    "    USE_SPARK = False\n",
    "    \n",
    "    # Fallback to pandas analysis\n",
    "    try:\n",
    "        # Re-read data with pandas for analysis\n",
    "        print(\"Re-reading data with pandas for analysis...\")\n",
    "        github_url = \"https://raw.githubusercontent.com/Hachi630/BDAS/main/online_retail_II.xlsx\"\n",
    "        excel_data = pd.read_excel(github_url, sheet_name=None)\n",
    "        sheet_2009_2010 = excel_data['Year 2009-2010']\n",
    "        sheet_2010_2011 = excel_data['Year 2010-2011']\n",
    "        pandas_df = pd.concat([sheet_2009_2010, sheet_2010_2011], ignore_index=True)\n",
    "        \n",
    "        print(\"Missing values per column:\")\n",
    "        missing_counts = pandas_df.isnull().sum()\n",
    "        print(missing_counts)\n",
    "        \n",
    "        total_records = len(pandas_df)\n",
    "        print(f\"\\nTotal records: {total_records:,}\")\n",
    "        \n",
    "        # Calculate and display missing percentages\n",
    "        print(\"\\nMissing values summary:\")\n",
    "        for col_name in missing_counts.index:\n",
    "            missing_count = missing_counts[col_name]\n",
    "            missing_pct = (missing_count / total_records) * 100\n",
    "            print(f\"{col_name}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "            \n",
    "            # Add specific comment for CustomerID\n",
    "            if col_name == \"Customer ID\" and missing_count > 0:\n",
    "                print(f\"  ‚Üí CustomerID has many missing entries ({missing_count:,} records)\")\n",
    "            elif missing_count == 0:\n",
    "                print(f\"  ‚Üí {col_name} is complete (no missing values)\")\n",
    "            else:\n",
    "                print(f\"  ‚Üí {col_name} has some missing values\")\n",
    "        \n",
    "        print(\"‚úÖ Missing values analysis completed successfully with pandas!\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Pandas analysis also failed: {e2}\")\n",
    "        print(\"Please check your internet connection and try again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd6a02",
   "metadata": {},
   "source": [
    "### 5.2 Numeric Values Validity Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c7af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify records with non-positive prices or zero quantity with error handling\n",
    "print(\"=== Numeric Values Validity Check ===\")\n",
    "\n",
    "if USE_SPARK:\n",
    "    try:\n",
    "        print(\"Using PySpark for numeric validity analysis...\")\n",
    "        \n",
    "        # Check for non-positive Price values\n",
    "        invalid_price_count = df.filter(F.col(\"Price\") <= 0).count()\n",
    "        print(f\"Records with Price <= 0: {invalid_price_count:,}\")\n",
    "        \n",
    "        if invalid_price_count > 0:\n",
    "            print(\"\\nSample records with invalid prices:\")\n",
    "            df.filter(F.col(\"Price\") <= 0).select(\"Invoice\", \"StockCode\", \"Quantity\", \"Price\").show(5)\n",
    "        \n",
    "        # Check for zero Quantity values\n",
    "        zero_quantity_count = df.filter(F.col(\"Quantity\") == 0).count()\n",
    "        print(f\"\\nRecords with Quantity = 0: {zero_quantity_count:,}\")\n",
    "        \n",
    "        if zero_quantity_count > 0:\n",
    "            print(\"\\nSample records with zero quantity:\")\n",
    "            df.filter(F.col(\"Quantity\") == 0).show(5)\n",
    "        \n",
    "        # Check for negative quantities (returns)\n",
    "        negative_quantity_count = df.filter(F.col(\"Quantity\") < 0).count()\n",
    "        print(f\"\\nRecords with Quantity < 0 (returns): {negative_quantity_count:,}\")\n",
    "        \n",
    "        print(\"‚úÖ PySpark numeric validity check completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå PySpark numeric check failed: {e}\")\n",
    "        print(\"üîÑ Switching to pandas analysis...\")\n",
    "        USE_SPARK = False\n",
    "\n",
    "if not USE_SPARK:\n",
    "    try:\n",
    "        print(\"Using pandas for numeric validity analysis...\")\n",
    "        \n",
    "        # Check for non-positive Price values\n",
    "        invalid_price_mask = pandas_df[\"Price\"] <= 0\n",
    "        invalid_price_count = invalid_price_mask.sum()\n",
    "        print(f\"Records with Price <= 0: {invalid_price_count:,}\")\n",
    "        \n",
    "        if invalid_price_count > 0:\n",
    "            print(\"\\nSample records with invalid prices:\")\n",
    "            print(pandas_df[invalid_price_mask][[\"Invoice\", \"StockCode\", \"Quantity\", \"Price\"]].head())\n",
    "        \n",
    "        # Check for zero Quantity values\n",
    "        zero_quantity_mask = pandas_df[\"Quantity\"] == 0\n",
    "        zero_quantity_count = zero_quantity_mask.sum()\n",
    "        print(f\"\\nRecords with Quantity = 0: {zero_quantity_count:,}\")\n",
    "        \n",
    "        if zero_quantity_count > 0:\n",
    "            print(\"\\nSample records with zero quantity:\")\n",
    "            print(pandas_df[zero_quantity_mask].head())\n",
    "        \n",
    "        # Check for negative quantities (returns)\n",
    "        negative_quantity_mask = pandas_df[\"Quantity\"] < 0\n",
    "        negative_quantity_count = negative_quantity_mask.sum()\n",
    "        print(f\"\\nRecords with Quantity < 0 (returns): {negative_quantity_count:,}\")\n",
    "        \n",
    "        print(\"‚úÖ Pandas numeric validity check completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Pandas numeric check failed: {e}\")\n",
    "\n",
    "# Summary comments\n",
    "print(\"\\n=== Validity Summary ===\")\n",
    "if invalid_price_count > 0:\n",
    "    print(f\"‚ö†Ô∏è  Found {invalid_price_count:,} records with non-positive prices (possibly freebies)\")\n",
    "else:\n",
    "    print(\"‚úÖ All prices are positive\")\n",
    "    \n",
    "if zero_quantity_count > 0:\n",
    "    print(f\"‚ö†Ô∏è  Found {zero_quantity_count:,} records with zero quantity (unusual - could indicate data entry errors)\")\n",
    "else:\n",
    "    print(\"‚úÖ No zero quantity records found\")\n",
    "    \n",
    "if negative_quantity_count > 0:\n",
    "    print(f\"‚ÑπÔ∏è  Found {negative_quantity_count:,} return transactions (negative quantities)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d3a64",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Perform comprehensive exploratory data analysis using PySpark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5369c8",
   "metadata": {},
   "source": [
    "### 6.1 Key Dataset Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of distinct products and customers\n",
    "print(\"=== Key Dataset Metrics ===\")\n",
    "\n",
    "# Calculate distinct counts using Spark\n",
    "unique_products = df.select(\"StockCode\").distinct().count()\n",
    "unique_customers = df.select(\"Customer ID\").distinct().count()\n",
    "\n",
    "# Calculate total revenue\n",
    "total_revenue = df.agg(F.sum(F.col(\"Quantity\") * F.col(\"Price\")).alias(\"TotalRevenue\")).collect()[0][\"TotalRevenue\"]\n",
    "\n",
    "# Calculate total transactions\n",
    "total_transactions = df.count()\n",
    "\n",
    "print(f\"Total Transactions: {total_transactions:,}\")\n",
    "print(f\"Unique Products (StockCode): {unique_products:,}\")\n",
    "print(f\"Unique Customers: {unique_customers:,}\")\n",
    "print(f\"Total Revenue: ¬£{total_revenue:,.2f}\")\n",
    "\n",
    "# Additional metrics\n",
    "avg_order_value = df.agg(F.avg(F.col(\"Quantity\") * F.col(\"Price\")).alias(\"AvgOrderValue\")).collect()[0][\"AvgOrderValue\"]\n",
    "print(f\"Average Order Value: ¬£{avg_order_value:.2f}\")\n",
    "\n",
    "# Data preview\n",
    "print(\"\\n=== Data Preview ===\")\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "print(\"\\n=== Data Schema ===\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f744a0b9",
   "metadata": {},
   "source": [
    "## 7. Data Quality Summary Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Summary Report\n",
    "print(\"=\" * 60)\n",
    "print(\"           DATA QUALITY ASSESSMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìä DATASET OVERVIEW:\")\n",
    "total_records = df.count()\n",
    "total_columns = len(df.columns)\n",
    "\n",
    "print(f\"   ‚Ä¢ Total Records: {total_records:,}\")\n",
    "print(f\"   ‚Ä¢ Total Columns: {total_columns}\")\n",
    "print(f\"   ‚Ä¢ Analysis Engine: PySpark\")\n",
    "\n",
    "print(\"\\nüîç DATA QUALITY ISSUES IDENTIFIED:\")\n",
    "\n",
    "# Missing Values Summary\n",
    "print(\"\\n1. MISSING VALUES:\")\n",
    "missing_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "missing_data = missing_counts.collect()[0]\n",
    "for col_name in df.columns:\n",
    "    missing_count = missing_data[col_name]\n",
    "    missing_pct = (missing_count / total_records) * 100\n",
    "    if missing_count > 0:\n",
    "        print(f\"   ‚Ä¢ {col_name}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "\n",
    "# Numeric Validity Summary\n",
    "print(\"\\n2. NUMERIC VALIDITY:\")\n",
    "invalid_price_count = df.filter(F.col(\"Price\") <= 0).count()\n",
    "zero_quantity_count = df.filter(F.col(\"Quantity\") == 0).count()\n",
    "negative_quantity_count = df.filter(F.col(\"Quantity\") < 0).count()\n",
    "\n",
    "if invalid_price_count > 0:\n",
    "    print(f\"   ‚Ä¢ Non-positive prices: {invalid_price_count:,}\")\n",
    "if zero_quantity_count > 0:\n",
    "    print(f\"   ‚Ä¢ Zero quantities: {zero_quantity_count:,}\")\n",
    "if negative_quantity_count > 0:\n",
    "    print(f\"   ‚Ä¢ Negative quantities (returns): {negative_quantity_count:,}\")\n",
    "\n",
    "# Returns and Cancellations Summary\n",
    "print(\"\\n3. RETURNS & CANCELLATIONS:\")\n",
    "num_cancelled = df.filter(F.col(\"Invoice\").startswith(\"C\")).count()\n",
    "num_returns = df.filter(F.col(\"Quantity\") < 0).count()\n",
    "\n",
    "print(f\"   ‚Ä¢ Cancelled invoices: {num_cancelled:,}\")\n",
    "print(f\"   ‚Ä¢ Return transactions: {num_returns:,}\")\n",
    "\n",
    "# Duplicates Summary\n",
    "print(\"\\n4. DUPLICATE RECORDS:\")\n",
    "unique_rows = df.dropDuplicates().count()\n",
    "num_duplicates = total_records - unique_rows\n",
    "if num_duplicates > 0:\n",
    "    print(f\"   ‚Ä¢ Duplicate records: {num_duplicates:,}\")\n",
    "else:\n",
    "    print(\"   ‚Ä¢ No duplicate records found\")\n",
    "\n",
    "# Date Range Summary\n",
    "print(\"\\n5. DATE RANGE CONSISTENCY:\")\n",
    "date_range = df.select(F.min(\"InvoiceDate\").alias(\"MinDate\"), F.max(\"InvoiceDate\").alias(\"MaxDate\"))\n",
    "date_info = date_range.collect()[0]\n",
    "min_date = date_info[\"MinDate\"]\n",
    "max_date = date_info[\"MaxDate\"]\n",
    "\n",
    "print(f\"   ‚Ä¢ Date range: {min_date} to {max_date}\")\n",
    "print(f\"   ‚Ä¢ Expected range: 2009-12-01 to 2011-12-09\")\n",
    "\n",
    "print(\"\\nüìã RECOMMENDATIONS:\")\n",
    "print(\"   ‚Ä¢ CustomerID missing values: Consider impact on customer analysis\")\n",
    "print(\"   ‚Ä¢ Return transactions: Account for net vs gross sales calculations\")\n",
    "print(\"   ‚Ä¢ Date range: Verify business context for partial years\")\n",
    "if num_duplicates > 0:\n",
    "    print(\"   ‚Ä¢ Duplicates: Consider removing for accurate analysis\")\n",
    "if invalid_price_count > 0 or zero_quantity_count > 0:\n",
    "    print(\"   ‚Ä¢ Invalid values: Review business rules for data cleaning\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"           END OF DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48274893",
   "metadata": {},
   "source": [
    "## 8. Troubleshooting: Restart Spark Session\n",
    "\n",
    "If you encounter connection timeout errors, run this cell to restart the Spark session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba03bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart Spark session if needed\n",
    "print(\"üîÑ Restarting Spark session...\")\n",
    "\n",
    "try:\n",
    "    # Stop current session\n",
    "    spark.stop()\n",
    "    print(\"‚úÖ Previous Spark session stopped\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è  No previous session to stop\")\n",
    "\n",
    "# Wait a moment\n",
    "import time\n",
    "time.sleep(2)\n",
    "\n",
    "# Restart with optimized configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OnlineRetailAnalysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"1800\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "    .config(\"spark.network.timeout\", \"1200s\") \\\n",
    "    .config(\"spark.rpc.askTimeout\", \"1200s\") \\\n",
    "    .config(\"spark.rpc.lookupTimeout\", \"1200s\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"1200s\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"‚úÖ New Spark session initialized!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# Test the connection\n",
    "try:\n",
    "    test_df = spark.range(10)\n",
    "    test_count = test_df.count()\n",
    "    print(f\"‚úÖ Connection test successful: {test_count} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection test failed: {e}\")\n",
    "    print(\"Please check your Java installation and try again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ddf6c6",
   "metadata": {},
   "source": [
    "## 9. Alternative: Pandas-Only Analysis\n",
    "\n",
    "If PySpark continues to have connection issues, use this pandas-only analysis as a complete alternative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af15180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete pandas-only analysis as fallback\n",
    "print(\"=== Complete Pandas Analysis ===\")\n",
    "print(\"This analysis uses pandas only, bypassing PySpark completely.\")\n",
    "\n",
    "# Load data with pandas\n",
    "print(\"Loading data with pandas...\")\n",
    "github_url = \"https://raw.githubusercontent.com/Hachi630/BDAS/main/online_retail_II.xlsx\"\n",
    "excel_data = pd.read_excel(github_url, sheet_name=None)\n",
    "sheet_2009_2010 = excel_data['Year 2009-2010']\n",
    "sheet_2010_2011 = excel_data['Year 2010-2011']\n",
    "df_pandas = pd.concat([sheet_2009_2010, sheet_2010_2011], ignore_index=True)\n",
    "\n",
    "print(f\"‚úÖ Data loaded successfully!\")\n",
    "print(f\"Dataset shape: {df_pandas.shape}\")\n",
    "print(f\"Columns: {list(df_pandas.columns)}\")\n",
    "\n",
    "# Basic info\n",
    "print(\"\\n=== Dataset Overview ===\")\n",
    "print(f\"Total records: {len(df_pandas):,}\")\n",
    "print(f\"Total columns: {len(df_pandas.columns)}\")\n",
    "print(f\"Memory usage: {df_pandas.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Data preview\n",
    "print(\"\\n=== Data Preview ===\")\n",
    "print(df_pandas.head())\n",
    "\n",
    "# Data types\n",
    "print(\"\\n=== Data Types ===\")\n",
    "print(df_pandas.dtypes)\n",
    "\n",
    "# Missing values\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "missing_counts = df_pandas.isnull().sum()\n",
    "missing_pct = (missing_counts / len(df_pandas)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Missing Percentage': missing_pct\n",
    "})\n",
    "print(missing_df)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n=== Basic Statistics ===\")\n",
    "print(df_pandas.describe())\n",
    "\n",
    "# Key metrics\n",
    "print(\"\\n=== Key Metrics ===\")\n",
    "unique_products = df_pandas['StockCode'].nunique()\n",
    "unique_customers = df_pandas['Customer ID'].nunique()\n",
    "total_revenue = (df_pandas['Quantity'] * df_pandas['Price']).sum()\n",
    "avg_order_value = (df_pandas['Quantity'] * df_pandas['Price']).mean()\n",
    "\n",
    "print(f\"Unique Products: {unique_products:,}\")\n",
    "print(f\"Unique Customers: {unique_customers:,}\")\n",
    "print(f\"Total Revenue: ¬£{total_revenue:,.2f}\")\n",
    "print(f\"Average Order Value: ¬£{avg_order_value:.2f}\")\n",
    "\n",
    "# Data quality checks\n",
    "print(\"\\n=== Data Quality Checks ===\")\n",
    "invalid_prices = (df_pandas['Price'] <= 0).sum()\n",
    "zero_quantities = (df_pandas['Quantity'] == 0).sum()\n",
    "negative_quantities = (df_pandas['Quantity'] < 0).sum()\n",
    "cancelled_invoices = df_pandas['Invoice'].str.startswith('C').sum()\n",
    "\n",
    "print(f\"Invalid prices (‚â§0): {invalid_prices:,}\")\n",
    "print(f\"Zero quantities: {zero_quantities:,}\")\n",
    "print(f\"Negative quantities (returns): {negative_quantities:,}\")\n",
    "print(f\"Cancelled invoices: {cancelled_invoices:,}\")\n",
    "\n",
    "# Date range\n",
    "print(\"\\n=== Date Range ===\")\n",
    "min_date = df_pandas['InvoiceDate'].min()\n",
    "max_date = df_pandas['InvoiceDate'].max()\n",
    "print(f\"Date range: {min_date} to {max_date}\")\n",
    "\n",
    "# Top countries\n",
    "print(\"\\n=== Top 10 Countries by Revenue ===\")\n",
    "country_revenue = df_pandas.groupby('Country').agg({\n",
    "    'Quantity': 'sum',\n",
    "    'Price': lambda x: (df_pandas.loc[x.index, 'Quantity'] * x).sum(),\n",
    "    'Invoice': 'count'\n",
    "}).rename(columns={'Price': 'TotalRevenue', 'Invoice': 'TransactionCount'})\n",
    "country_revenue = country_revenue.sort_values('TotalRevenue', ascending=False)\n",
    "print(country_revenue.head(10))\n",
    "\n",
    "print(\"\\n‚úÖ Complete pandas analysis finished!\")\n",
    "print(\"All analysis completed successfully using pandas.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
