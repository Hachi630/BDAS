{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Online Retail II Dataset Analysis\n",
    "\n",
    "This notebook demonstrates how to load and analyze the Online Retail II dataset using PySpark in Google Colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n",
    "\n",
    "First, install PySpark and related dependencies in Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install pyspark pandas openpyxl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Initialize Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized successfully!\n",
      "Spark version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, sum as spark_sum, count, when, isnan, isnull, desc, min as spark_min, max as spark_max\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session\n",
    "# Configure Spark for both local and Colab environments\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OnlineRetailAnalysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"300\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce output noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"Spark session initialized successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data from GitHub\n",
    "\n",
    "Since PySpark cannot directly read Excel files, we use pandas to read from GitHub and then convert to Spark DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Excel file from GitHub...\n",
      "Loading data from both sheets (2009-2010 and 2010-2011)...\n",
      "2009-2010 data shape: (525461, 8)\n",
      "2010-2011 data shape: (541910, 8)\n",
      "Combined data shape: (1067371, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:425: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Could not convert 'C489449' with type str: tried to convert to int64\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from GitHub into Spark DataFrame!\n"
     ]
    }
   ],
   "source": [
    "# Use pandas to read Excel file from GitHub\n",
    "print(\"Reading Excel file from GitHub...\")\n",
    "\n",
    "# GitHub repository information\n",
    "github_user = \"Hachi630\"\n",
    "github_repo = \"BDAS\"\n",
    "file_path = \"online_retail_II.xlsx\"\n",
    "\n",
    "# Construct GitHub raw URL\n",
    "github_url = f\"https://raw.githubusercontent.com/{github_user}/{github_repo}/main/{file_path}\"\n",
    "\n",
    "# Read Excel file with multiple sheets\n",
    "print(\"Loading data from both sheets (2009-2010 and 2010-2011)...\")\n",
    "excel_data = pd.read_excel(github_url, sheet_name=None)  # Read all sheets\n",
    "\n",
    "# Get the two sheets\n",
    "sheet_2009_2010 = excel_data['Year 2009-2010']\n",
    "sheet_2010_2011 = excel_data['Year 2010-2011']\n",
    "\n",
    "print(f\"2009-2010 data shape: {sheet_2009_2010.shape}\")\n",
    "print(f\"2010-2011 data shape: {sheet_2010_2011.shape}\")\n",
    "\n",
    "# Combine both datasets\n",
    "pandas_df = pd.concat([sheet_2009_2010, sheet_2010_2011], ignore_index=True)\n",
    "print(f\"Combined data shape: {pandas_df.shape}\")\n",
    "\n",
    "# Convert pandas DataFrame to Spark DataFrame\n",
    "# Ensure DataFrame is named df for consistency\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "print(\"Data successfully loaded from GitHub into Spark DataFrame!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Individual Table Analysis\n",
    "\n",
    "Analyze each table separately before combining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Individual Table Analysis ===\n",
      "\n",
      "--- 2009-2010 Data ---\n",
      "Shape: (525461, 8)\n",
      "Columns: ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country']\n",
      "First 3 rows:\n",
      "  Invoice StockCode                          Description  Quantity  \\\n",
      "0  489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
      "1  489434    79323P                   PINK CHERRY LIGHTS        12   \n",
      "2  489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
      "\n",
      "          InvoiceDate  Price  Customer ID         Country  \n",
      "0 2009-12-01 07:45:00   6.95      13085.0  United Kingdom  \n",
      "1 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "2 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "\n",
      "--- 2010-2011 Data ---\n",
      "Shape: (541910, 8)\n",
      "Columns: ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country']\n",
      "First 3 rows:\n",
      "  Invoice StockCode                         Description  Quantity  \\\n",
      "0  536365    85123A  WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1  536365     71053                 WHITE METAL LANTERN         6   \n",
      "2  536365    84406B      CREAM CUPID HEARTS COAT HANGER         8   \n",
      "\n",
      "          InvoiceDate  Price  Customer ID         Country  \n",
      "0 2010-12-01 08:26:00   2.55      17850.0  United Kingdom  \n",
      "1 2010-12-01 08:26:00   3.39      17850.0  United Kingdom  \n",
      "2 2010-12-01 08:26:00   2.75      17850.0  United Kingdom  \n",
      "\n",
      "--- Combined Data Summary ---\n",
      "Total records: 1,067,371\n",
      "Total columns: 8\n",
      "Columns: ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country']\n"
     ]
    }
   ],
   "source": [
    "# Analyze individual tables\n",
    "print(\"=== Individual Table Analysis ===\")\n",
    "\n",
    "print(\"\\n--- 2009-2010 Data ---\")\n",
    "print(f\"Shape: {sheet_2009_2010.shape}\")\n",
    "print(f\"Columns: {list(sheet_2009_2010.columns)}\")\n",
    "print(\"First 3 rows:\")\n",
    "print(sheet_2009_2010.head(3))\n",
    "\n",
    "print(\"\\n--- 2010-2011 Data ---\")\n",
    "print(f\"Shape: {sheet_2010_2011.shape}\")\n",
    "print(f\"Columns: {list(sheet_2010_2011.columns)}\")\n",
    "print(\"First 3 rows:\")\n",
    "print(sheet_2010_2011.head(3))\n",
    "\n",
    "print(\"\\n--- Combined Data Summary ---\")\n",
    "print(f\"Total records: {len(pandas_df):,}\")\n",
    "print(f\"Total columns: {len(pandas_df.columns)}\")\n",
    "print(f\"Columns: {list(pandas_df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check Data Dimensions\n",
    "\n",
    "Determine the number of rows and columns in the combined dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Dimension Information ===\n",
      "Error getting row count with Spark: An error occurred while calling o150.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 4.0 failed 1 times, most recent failure: Lost task 2.0 in stage 4.0 (TID 82) (windows10.microdone.cn executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\n",
      "\t... 34 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\n",
      "\t... 34 more\n",
      "\n",
      "Using pandas DataFrame for all analysis...\n",
      "Dataset row count (from pandas): 1,067,371\n",
      "Dataset column count: 8\n",
      "Column names: ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country']\n"
     ]
    }
   ],
   "source": [
    "# Check data dimensions with error handling\n",
    "print(\"=== Data Dimension Information ===\")\n",
    "\n",
    "# Get row count with error handling\n",
    "try:\n",
    "    row_count = df.count()\n",
    "    print(f\"Dataset row count: {row_count:,}\")\n",
    "    USE_SPARK = True\n",
    "except Exception as e:\n",
    "    print(f\"Error getting row count with Spark: {e}\")\n",
    "    print(\"Using pandas DataFrame for all analysis...\")\n",
    "    row_count = len(pandas_df)\n",
    "    print(f\"Dataset row count (from pandas): {row_count:,}\")\n",
    "    USE_SPARK = False\n",
    "\n",
    "# Get column count\n",
    "if USE_SPARK:\n",
    "    column_count = len(df.columns)\n",
    "    column_names = df.columns\n",
    "else:\n",
    "    column_count = len(pandas_df.columns)\n",
    "    column_names = list(pandas_df.columns)\n",
    "\n",
    "print(f\"Dataset column count: {column_count}\")\n",
    "print(f\"Column names: {column_names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Analysis with Error Handling\n",
    "\n",
    "The following analysis will automatically detect if PySpark is working and use the appropriate method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Preview (First 5 Rows) ===\n",
      "  Invoice StockCode                          Description  Quantity  \\\n",
      "0  489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
      "1  489434    79323P                   PINK CHERRY LIGHTS        12   \n",
      "2  489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
      "3  489434     22041         RECORD FRAME 7\" SINGLE SIZE         48   \n",
      "4  489434     21232       STRAWBERRY CERAMIC TRINKET BOX        24   \n",
      "\n",
      "          InvoiceDate  Price  Customer ID         Country  \n",
      "0 2009-12-01 07:45:00   6.95      13085.0  United Kingdom  \n",
      "1 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "2 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "3 2009-12-01 07:45:00   2.10      13085.0  United Kingdom  \n",
      "4 2009-12-01 07:45:00   1.25      13085.0  United Kingdom  \n",
      "\n",
      "=== Data Schema ===\n",
      "Invoice                object\n",
      "StockCode              object\n",
      "Description            object\n",
      "Quantity                int64\n",
      "InvoiceDate    datetime64[ns]\n",
      "Price                 float64\n",
      "Customer ID           float64\n",
      "Country                object\n",
      "dtype: object\n",
      "\n",
      "=== Numeric Columns Statistical Summary ===\n",
      "           Quantity                    InvoiceDate         Price  \\\n",
      "count  1.067371e+06                        1067371  1.067371e+06   \n",
      "mean   9.938898e+00  2011-01-02 21:13:55.394028544  4.649388e+00   \n",
      "min   -8.099500e+04            2009-12-01 07:45:00 -5.359436e+04   \n",
      "25%    1.000000e+00            2010-07-09 09:46:00  1.250000e+00   \n",
      "50%    3.000000e+00            2010-12-07 15:28:00  2.100000e+00   \n",
      "75%    1.000000e+01            2011-07-22 10:23:00  4.150000e+00   \n",
      "max    8.099500e+04            2011-12-09 12:50:00  3.897000e+04   \n",
      "std    1.727058e+02                            NaN  1.235531e+02   \n",
      "\n",
      "         Customer ID  \n",
      "count  824364.000000  \n",
      "mean    15324.638504  \n",
      "min     12346.000000  \n",
      "25%     13975.000000  \n",
      "50%     15255.000000  \n",
      "75%     16797.000000  \n",
      "max     18287.000000  \n",
      "std      1697.464450  \n",
      "\n",
      "=== Missing Values Check ===\n",
      "Invoice             0\n",
      "StockCode           0\n",
      "Description      4382\n",
      "Quantity            0\n",
      "InvoiceDate         0\n",
      "Price               0\n",
      "Customer ID    243007\n",
      "Country             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preview data - show first 5 rows\n",
    "print(\"=== Data Preview (First 5 Rows) ===\")\n",
    "if USE_SPARK:\n",
    "    df.show(5, truncate=False)\n",
    "else:\n",
    "    print(pandas_df.head())\n",
    "\n",
    "# Print data schema to verify data types\n",
    "print(\"\\n=== Data Schema ===\")\n",
    "if USE_SPARK:\n",
    "    df.printSchema()\n",
    "else:\n",
    "    print(pandas_df.dtypes)\n",
    "\n",
    "# Display basic statistical summary for numeric columns\n",
    "print(\"\\n=== Numeric Columns Statistical Summary ===\")\n",
    "if USE_SPARK:\n",
    "    df.describe().show()\n",
    "else:\n",
    "    print(pandas_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n=== Missing Values Check ===\")\n",
    "if USE_SPARK:\n",
    "    missing_values = df.select([spark_sum(when(isnull(c) | isnan(c), 1).otherwise(0)).alias(c) for c in df.columns])\n",
    "    missing_values.show()\n",
    "else:\n",
    "    missing_values = pandas_df.isnull().sum()\n",
    "    print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Quantity Column Analysis ===\n",
      "Min Quantity: -80995\n",
      "Max Quantity: 80995\n",
      "Return Records Count: 22950\n",
      "Normal Sales Records Count: 1044421\n",
      "\n",
      "=== UnitPrice Column Analysis ===\n",
      "Min Unit Price: -53594.36\n",
      "Max Unit Price: 38970.0\n",
      "Negative Price Records Count: 5\n",
      "Zero Price Records Count: 6202\n"
     ]
    }
   ],
   "source": [
    "# Check negative values in Quantity column (returns)\n",
    "print(\"=== Quantity Column Analysis ===\")\n",
    "if USE_SPARK:\n",
    "    quantity_stats = df.select(\n",
    "        spark_min(\"Quantity\").alias(\"Min Quantity\"),\n",
    "        spark_max(\"Quantity\").alias(\"Max Quantity\"),\n",
    "        count(when(col(\"Quantity\") < 0, 1)).alias(\"Return Records Count\"),\n",
    "        count(when(col(\"Quantity\") > 0, 1)).alias(\"Normal Sales Records Count\")\n",
    "    )\n",
    "    quantity_stats.show()\n",
    "else:\n",
    "    quantity_stats = {\n",
    "        \"Min Quantity\": pandas_df['Quantity'].min(),\n",
    "        \"Max Quantity\": pandas_df['Quantity'].max(),\n",
    "        \"Return Records Count\": (pandas_df['Quantity'] < 0).sum(),\n",
    "        \"Normal Sales Records Count\": (pandas_df['Quantity'] > 0).sum()\n",
    "    }\n",
    "    for key, value in quantity_stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Check UnitPrice column range\n",
    "print(\"\\n=== UnitPrice Column Analysis ===\")\n",
    "if USE_SPARK:\n",
    "    price_stats = df.select(\n",
    "        spark_min(\"UnitPrice\").alias(\"Min Unit Price\"),\n",
    "        spark_max(\"UnitPrice\").alias(\"Max Unit Price\"),\n",
    "        count(when(col(\"UnitPrice\") < 0, 1)).alias(\"Negative Price Records Count\"),\n",
    "        count(when(col(\"UnitPrice\") == 0, 1)).alias(\"Zero Price Records Count\")\n",
    "    )\n",
    "    price_stats.show()\n",
    "else:\n",
    "    price_stats = {\n",
    "        \"Min Unit Price\": pandas_df['Price'].min(),\n",
    "        \"Max Unit Price\": pandas_df['Price'].max(),\n",
    "        \"Negative Price Records Count\": (pandas_df['Price'] < 0).sum(),\n",
    "        \"Zero Price Records Count\": (pandas_df['Price'] == 0).sum()\n",
    "    }\n",
    "    for key, value in price_stats.items():\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Comprehensive analysis using PySpark with pandas/Matplotlib for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries for EDA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Key Counts Summary\n",
    "\n",
    "Calculate basic totals and counts to understand the dataset scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of distinct products and customers\n",
    "print(\"=== Key Dataset Metrics ===\")\n",
    "\n",
    "if USE_SPARK:\n",
    "    # Calculate distinct counts using Spark\n",
    "    unique_products = df.select(\"StockCode\").distinct().count()\n",
    "    unique_customers = df.select(\"Customer ID\").distinct().count()\n",
    "    \n",
    "    # Calculate total revenue (sum of Quantity * Price)\n",
    "    total_revenue = df.agg(F.sum(F.col(\"Quantity\") * F.col(\"Price\")).alias(\"TotalRevenue\")).collect()[0][\"TotalRevenue\"]\n",
    "    \n",
    "    # Calculate total transactions\n",
    "    total_transactions = df.count()\n",
    "    \n",
    "    print(f\"Total Transactions: {total_transactions:,}\")\n",
    "    print(f\"Unique Products (StockCode): {unique_products:,}\")\n",
    "    print(f\"Unique Customers: {unique_customers:,}\")\n",
    "    print(f\"Total Revenue: £{total_revenue:,.2f}\")\n",
    "    \n",
    "else:\n",
    "    # Calculate using pandas\n",
    "    unique_products = pandas_df['StockCode'].nunique()\n",
    "    unique_customers = pandas_df['Customer ID'].nunique()\n",
    "    total_revenue = (pandas_df['Quantity'] * pandas_df['Price']).sum()\n",
    "    total_transactions = len(pandas_df)\n",
    "    \n",
    "    print(f\"Total Transactions: {total_transactions:,}\")\n",
    "    print(f\"Unique Products (StockCode): {unique_products:,}\")\n",
    "    print(f\"Unique Customers: {unique_customers:,}\")\n",
    "    print(f\"Total Revenue: £{total_revenue:,.2f}\")\n",
    "\n",
    "# Additional metrics\n",
    "if USE_SPARK:\n",
    "    avg_order_value = df.agg(F.avg(F.col(\"Quantity\") * F.col(\"Price\")).alias(\"AvgOrderValue\")).collect()[0][\"AvgOrderValue\"]\n",
    "    print(f\"Average Order Value: £{avg_order_value:.2f}\")\n",
    "else:\n",
    "    avg_order_value = (pandas_df['Quantity'] * pandas_df['Price']).mean()\n",
    "    print(f\"Average Order Value: £{avg_order_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Sales Over Time Analysis\n",
    "\n",
    "Investigate temporal trends by aggregating sales by month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate sales by month for trend analysis\n",
    "print(\"=== Monthly Sales Analysis ===\")\n",
    "\n",
    "if USE_SPARK:\n",
    "    # Create monthly aggregation using Spark\n",
    "    monthly_sales = df.groupBy(F.year(\"InvoiceDate\").alias(\"Year\"), \n",
    "                               F.month(\"InvoiceDate\").alias(\"Month\")) \\\n",
    "                      .agg(F.sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
    "                           F.sum(F.col(\"Quantity\") * F.col(\"Price\")).alias(\"TotalRevenue\"),\n",
    "                           F.count(\"*\").alias(\"TransactionCount\")) \\\n",
    "                      .orderBy(\"Year\", \"Month\")\n",
    "    \n",
    "    print(\"Monthly Sales Summary:\")\n",
    "    monthly_sales.show(24)  # Show 2 years of data\n",
    "    \n",
    "    # Convert to pandas for plotting\n",
    "    monthly_pandas = monthly_sales.toPandas()\n",
    "    \n",
    "else:\n",
    "    # Create monthly aggregation using pandas\n",
    "    pandas_df['Year'] = pandas_df['InvoiceDate'].dt.year\n",
    "    pandas_df['Month'] = pandas_df['InvoiceDate'].dt.month\n",
    "    \n",
    "    monthly_sales = pandas_df.groupby(['Year', 'Month']).agg({\n",
    "        'Quantity': 'sum',\n",
    "        'Price': lambda x: (pandas_df.loc[x.index, 'Quantity'] * x).sum(),\n",
    "        'Invoice': 'count'\n",
    "    }).rename(columns={'Quantity': 'TotalQuantity', 'Price': 'TotalRevenue', 'Invoice': 'TransactionCount'})\n",
    "    \n",
    "    monthly_sales = monthly_sales.reset_index().sort_values(['Year', 'Month'])\n",
    "    print(\"Monthly Sales Summary:\")\n",
    "    print(monthly_sales.head(24))\n",
    "    \n",
    "    monthly_pandas = monthly_sales.copy()\n",
    "\n",
    "# Create year-month string for better plotting\n",
    "monthly_pandas['YearMonth'] = monthly_pandas['Year'].astype(str) + '-' + monthly_pandas['Month'].astype(str).str.zfill(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot monthly sales trend\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Monthly Revenue Trend\n",
    "ax1.plot(monthly_pandas['YearMonth'], monthly_pandas['TotalRevenue'], \n",
    "         marker='o', linewidth=2, markersize=6, color='#2E86AB')\n",
    "ax1.set_title('Monthly Revenue Trend (2009-2011)', fontsize=16, fontweight='bold')\n",
    "ax1.set_xlabel('Month (Year-Month)', fontsize=12)\n",
    "ax1.set_ylabel('Total Revenue (£)', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Monthly Quantity Trend\n",
    "ax2.plot(monthly_pandas['YearMonth'], monthly_pandas['TotalQuantity'], \n",
    "         marker='s', linewidth=2, markersize=6, color='#A23B72')\n",
    "ax2.set_title('Monthly Quantity Sold Trend (2009-2011)', fontsize=16, fontweight='bold')\n",
    "ax2.set_xlabel('Month (Year-Month)', fontsize=12)\n",
    "ax2.set_ylabel('Total Quantity Sold', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print insights\n",
    "print(\"\\n=== Temporal Insights ===\")\n",
    "print(f\"Peak revenue month: {monthly_pandas.loc[monthly_pandas['TotalRevenue'].idxmax(), 'YearMonth']} (£{monthly_pandas['TotalRevenue'].max():,.2f})\")\n",
    "print(f\"Lowest revenue month: {monthly_pandas.loc[monthly_pandas['TotalRevenue'].idxmin(), 'YearMonth']} (£{monthly_pandas['TotalRevenue'].min():,.2f})\")\n",
    "print(f\"Peak quantity month: {monthly_pandas.loc[monthly_pandas['TotalQuantity'].idxmax(), 'YearMonth']} ({monthly_pandas['TotalQuantity'].max():,} units)\")\n",
    "print(f\"Lowest quantity month: {monthly_pandas.loc[monthly_pandas['TotalQuantity'].idxmin(), 'YearMonth']} ({monthly_pandas['TotalQuantity'].min():,} units)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Quantity Distribution Analysis\n",
    "\n",
    "Examine the distribution of Quantity per transaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of quantities per transaction\n",
    "print(\"=== Quantity Distribution Analysis ===\")\n",
    "\n",
    "if USE_SPARK:\n",
    "    # Get quartiles using Spark\n",
    "    quartiles = df.approxQuantile(\"Quantity\", [0.25, 0.5, 0.75, 0.95], 0.01)\n",
    "    print(f\"Quantity Quartiles: Q1={quartiles[0]:.0f}, Q2={quartiles[1]:.0f}, Q3={quartiles[2]:.0f}, 95th={quartiles[3]:.0f}\")\n",
    "    \n",
    "    # Sample data for plotting (to avoid memory issues)\n",
    "    quantities_sample = df.select(\"Quantity\").sample(fraction=0.1, seed=42).toPandas()\n",
    "    \n",
    "else:\n",
    "    # Get quartiles using pandas\n",
    "    quartiles = pandas_df['Quantity'].quantile([0.25, 0.5, 0.75, 0.95])\n",
    "    print(f\"Quantity Quartiles: Q1={quartiles[0.25]:.0f}, Q2={quartiles[0.5]:.0f}, Q3={quartiles[0.75]:.0f}, 95th={quartiles[0.95]:.0f}\")\n",
    "    \n",
    "    # Sample data for plotting\n",
    "    quantities_sample = pandas_df['Quantity'].sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Create visualizations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Histogram of quantities\n",
    "ax1.hist(quantities_sample, bins=50, alpha=0.7, color='#F18F01', edgecolor='black')\n",
    "ax1.set_title('Distribution of Quantities per Transaction', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Quantity', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Line (Returns)')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Box plot\n",
    "ax2.boxplot(quantities_sample, patch_artist=True, \n",
    "           boxprops=dict(facecolor='#C73E1D', alpha=0.7),\n",
    "           medianprops=dict(color='black', linewidth=2))\n",
    "ax2.set_title('Box Plot of Quantities per Transaction', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Quantity', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional statistics\n",
    "if USE_SPARK:\n",
    "    quantity_stats = df.agg(F.min(\"Quantity\").alias(\"Min\"),\n",
    "                           F.max(\"Quantity\").alias(\"Max\"),\n",
    "                           F.mean(\"Quantity\").alias(\"Mean\"),\n",
    "                           F.stddev(\"Quantity\").alias(\"StdDev\")).collect()[0]\n",
    "    print(f\"\\nQuantity Statistics:\")\n",
    "    print(f\"Min: {quantity_stats['Min']}\")\n",
    "    print(f\"Max: {quantity_stats['Max']}\")\n",
    "    print(f\"Mean: {quantity_stats['Mean']:.2f}\")\n",
    "    print(f\"Std Dev: {quantity_stats['StdDev']:.2f}\")\n",
    "else:\n",
    "    print(f\"\\nQuantity Statistics:\")\n",
    "    print(f\"Min: {pandas_df['Quantity'].min()}\")\n",
    "    print(f\"Max: {pandas_df['Quantity'].max()}\")\n",
    "    print(f\"Mean: {pandas_df['Quantity'].mean():.2f}\")\n",
    "    print(f\"Std Dev: {pandas_df['Quantity'].std():.2f}\")\n",
    "\n",
    "# Analyze returns (negative quantities)\n",
    "if USE_SPARK:\n",
    "    returns_count = df.filter(F.col(\"Quantity\") < 0).count()\n",
    "    returns_percentage = (returns_count / df.count()) * 100\n",
    "else:\n",
    "    returns_count = (pandas_df['Quantity'] < 0).sum()\n",
    "    returns_percentage = (returns_count / len(pandas_df)) * 100\n",
    "\n",
    "print(f\"\\nReturns Analysis:\")\n",
    "print(f\"Number of return transactions: {returns_count:,}\")\n",
    "print(f\"Percentage of returns: {returns_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Top and Bottom Products Analysis\n",
    "\n",
    "Identify the best-selling and slowest-selling products.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify top 10 best-selling products and their sales\n",
    "print(\"=== Top Products Analysis ===\")\n",
    "\n",
    "if USE_SPARK:\n",
    "    # Aggregate by StockCode using Spark\n",
    "    product_sales = df.groupBy(\"StockCode\").agg(\n",
    "        F.sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
    "        F.sum(F.col(\"Quantity\") * F.col(\"Price\")).alias(\"TotalRevenue\"),\n",
    "        F.count(\"*\").alias(\"TransactionCount\")\n",
    "    )\n",
    "    \n",
    "    # Get top 10 products by quantity\n",
    "    top_products = product_sales.orderBy(F.desc(\"TotalQuantity\")).limit(10)\n",
    "    print(\"Top 10 Products by Quantity Sold:\")\n",
    "    top_products.show()\n",
    "    \n",
    "    # Convert to pandas for plotting\n",
    "    top_products_pandas = top_products.toPandas()\n",
    "    \n",
    "else:\n",
    "    # Aggregate by StockCode using pandas\n",
    "    product_sales = pandas_df.groupby('StockCode').agg({\n",
    "        'Quantity': 'sum',\n",
    "        'Price': lambda x: (pandas_df.loc[x.index, 'Quantity'] * x).sum(),\n",
    "        'Invoice': 'count'\n",
    "    }).rename(columns={'Quantity': 'TotalQuantity', 'Price': 'TotalRevenue', 'Invoice': 'TransactionCount'})\n",
    "    \n",
    "    # Get top 10 products by quantity\n",
    "    top_products_pandas = product_sales.nlargest(10, 'TotalQuantity')\n",
    "    print(\"Top 10 Products by Quantity Sold:\")\n",
    "    print(top_products_pandas)\n",
    "\n",
    "# Plot top products\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.bar(range(len(top_products_pandas)), top_products_pandas['TotalQuantity'], \n",
    "               color='#2E86AB', alpha=0.8, edgecolor='black')\n",
    "plt.title('Top 10 Best-Selling Products by Quantity', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Product Rank', fontsize=12)\n",
    "plt.ylabel('Total Quantity Sold', fontsize=12)\n",
    "plt.xticks(range(len(top_products_pandas)), [f\"#{i+1}\" for i in range(len(top_products_pandas))])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{int(height):,}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show product codes and descriptions\n",
    "print(\"\\nTop 10 Product Details:\")\n",
    "for i, (idx, row) in enumerate(top_products_pandas.iterrows(), 1):\n",
    "    print(f\"{i}. StockCode: {idx} | Quantity: {row['TotalQuantity']:,} | Revenue: £{row['TotalRevenue']:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify a sample of low-selling products\n",
    "print(\"\\n=== Bottom Products Analysis ===\")\n",
    "\n",
    "if USE_SPARK:\n",
    "    # Get bottom 10 products by quantity (excluding returns)\n",
    "    bottom_products = product_sales.filter(F.col(\"TotalQuantity\") > 0).orderBy(F.asc(\"TotalQuantity\")).limit(10)\n",
    "    print(\"Bottom 10 Products by Quantity Sold:\")\n",
    "    bottom_products.show()\n",
    "    \n",
    "    # Convert to pandas\n",
    "    bottom_products_pandas = bottom_products.toPandas()\n",
    "    \n",
    "else:\n",
    "    # Get bottom 10 products by quantity (excluding returns)\n",
    "    bottom_products_pandas = product_sales[product_sales['TotalQuantity'] > 0].nsmallest(10, 'TotalQuantity')\n",
    "    print(\"Bottom 10 Products by Quantity Sold:\")\n",
    "    print(bottom_products_pandas)\n",
    "\n",
    "# Plot bottom products\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(range(len(bottom_products_pandas)), bottom_products_pandas['TotalQuantity'], \n",
    "               color='#A23B72', alpha=0.8, edgecolor='black')\n",
    "plt.title('Bottom 10 Slowest-Selling Products by Quantity', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Product Rank', fontsize=12)\n",
    "plt.ylabel('Total Quantity Sold', fontsize=12)\n",
    "plt.xticks(range(len(bottom_products_pandas)), [f\"#{i+1}\" for i in range(len(bottom_products_pandas))])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{int(height):,}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show bottom product details\n",
    "print(\"\\nBottom 10 Product Details:\")\n",
    "for i, (idx, row) in enumerate(bottom_products_pandas.iterrows(), 1):\n",
    "    print(f\"{i}. StockCode: {idx} | Quantity: {row['TotalQuantity']:,} | Revenue: £{row['TotalRevenue']:,.2f}\")\n",
    "\n",
    "# Analyze product performance distribution\n",
    "if USE_SPARK:\n",
    "    total_products = product_sales.count()\n",
    "    high_performers = product_sales.filter(F.col(\"TotalQuantity\") > 1000).count()\n",
    "    medium_performers = product_sales.filter((F.col(\"TotalQuantity\") > 100) & (F.col(\"TotalQuantity\") <= 1000)).count()\n",
    "    low_performers = product_sales.filter(F.col(\"TotalQuantity\") <= 100).count()\n",
    "else:\n",
    "    total_products = len(product_sales)\n",
    "    high_performers = len(product_sales[product_sales['TotalQuantity'] > 1000])\n",
    "    medium_performers = len(product_sales[(product_sales['TotalQuantity'] > 100) & (product_sales['TotalQuantity'] <= 1000)])\n",
    "    low_performers = len(product_sales[product_sales['TotalQuantity'] <= 100])\n",
    "\n",
    "print(f\"\\n=== Product Performance Distribution ===\")\n",
    "print(f\"Total Products: {total_products:,}\")\n",
    "print(f\"High Performers (>1000 units): {high_performers:,} ({high_performers/total_products*100:.1f}%)\")\n",
    "print(f\"Medium Performers (100-1000 units): {medium_performers:,} ({medium_performers/total_products*100:.1f}%)\")\n",
    "print(f\"Low Performers (≤100 units): {low_performers:,} ({low_performers/total_products*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Geographical Analysis\n",
    "\n",
    "Evaluate sales by country to understand market distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sales by country\n",
    "print(\"=== Geographical Sales Analysis ===\")\n",
    "\n",
    "if USE_SPARK:\n",
    "    # Group by Country using Spark\n",
    "    country_sales = df.groupBy(\"Country\").agg(\n",
    "        F.count(\"*\").alias(\"NumTransactions\"),\n",
    "        F.sum(F.col(\"Quantity\") * F.col(\"Price\")).alias(\"TotalRevenue\"),\n",
    "        F.sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
    "        F.countDistinct(\"Customer ID\").alias(\"UniqueCustomers\")\n",
    "    ).orderBy(F.desc(\"TotalRevenue\"))\n",
    "    \n",
    "    print(\"Top 10 Countries by Revenue:\")\n",
    "    country_sales.show(10)\n",
    "    \n",
    "    # Convert to pandas for plotting\n",
    "    country_pandas = country_sales.toPandas()\n",
    "    \n",
    "else:\n",
    "    # Group by Country using pandas\n",
    "    country_sales = pandas_df.groupby('Country').agg({\n",
    "        'Invoice': 'count',\n",
    "        'Price': lambda x: (pandas_df.loc[x.index, 'Quantity'] * x).sum(),\n",
    "        'Quantity': 'sum',\n",
    "        'Customer ID': 'nunique'\n",
    "    }).rename(columns={'Invoice': 'NumTransactions', 'Price': 'TotalRevenue', \n",
    "                      'Quantity': 'TotalQuantity', 'Customer ID': 'UniqueCustomers'})\n",
    "    \n",
    "    country_pandas = country_sales.sort_values('TotalRevenue', ascending=False)\n",
    "    print(\"Top 10 Countries by Revenue:\")\n",
    "    print(country_pandas.head(10))\n",
    "\n",
    "# Create visualizations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Plot 1: Top 10 Countries by Revenue\n",
    "top_countries = country_pandas.head(10)\n",
    "bars1 = ax1.bar(range(len(top_countries)), top_countries['TotalRevenue'], \n",
    "                color='#2E86AB', alpha=0.8, edgecolor='black')\n",
    "ax1.set_title('Top 10 Countries by Revenue', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Country', fontsize=12)\n",
    "ax1.set_ylabel('Total Revenue (£)', fontsize=12)\n",
    "ax1.set_xticks(range(len(top_countries)))\n",
    "ax1.set_xticklabels(top_countries['Country'], rotation=45, ha='right')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars1):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'£{height:,.0f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 2: Top 10 Countries by Transaction Count\n",
    "bars2 = ax2.bar(range(len(top_countries)), top_countries['NumTransactions'], \n",
    "                color='#A23B72', alpha=0.8, edgecolor='black')\n",
    "ax2.set_title('Top 10 Countries by Transaction Count', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Country', fontsize=12)\n",
    "ax2.set_ylabel('Number of Transactions', fontsize=12)\n",
    "ax2.set_xticks(range(len(top_countries)))\n",
    "ax2.set_xticklabels(top_countries['Country'], rotation=45, ha='right')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars2):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{int(height):,}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Market concentration analysis\n",
    "uk_revenue = country_pandas[country_pandas['Country'] == 'United Kingdom']['TotalRevenue'].iloc[0]\n",
    "total_revenue = country_pandas['TotalRevenue'].sum()\n",
    "uk_percentage = (uk_revenue / total_revenue) * 100\n",
    "\n",
    "print(f\"\\n=== Market Concentration Analysis ===\")\n",
    "print(f\"UK Revenue: £{uk_revenue:,.2f}\")\n",
    "print(f\"Total Revenue: £{total_revenue:,.2f}\")\n",
    "print(f\"UK Market Share: {uk_percentage:.1f}%\")\n",
    "\n",
    "# Top 5 countries analysis\n",
    "print(f\"\\nTop 5 Countries Analysis:\")\n",
    "for i, (idx, row) in enumerate(country_pandas.head(5).iterrows(), 1):\n",
    "    market_share = (row['TotalRevenue'] / total_revenue) * 100\n",
    "    print(f\"{i}. {idx}: £{row['TotalRevenue']:,.2f} ({market_share:.1f}%) | {row['NumTransactions']:,} transactions | {row['UniqueCustomers']:,} customers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 EDA Summary\n",
    "\n",
    "Key findings from the exploratory data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== EDA Summary and Key Findings ===\")\n",
    "print(\"\\n📊 DATASET SCALE:\")\n",
    "print(f\"• Total Transactions: {row_count:,}\")\n",
    "print(f\"• Unique Products: {unique_products:,}\")\n",
    "print(f\"• Unique Customers: {unique_customers:,}\")\n",
    "print(f\"• Total Revenue: £{total_revenue:,.2f}\")\n",
    "print(f\"• Average Order Value: £{avg_order_value:.2f}\")\n",
    "\n",
    "print(\"\\n📈 TEMPORAL TRENDS:\")\n",
    "print(f\"• Peak revenue month: {monthly_pandas.loc[monthly_pandas['TotalRevenue'].idxmax(), 'YearMonth']}\")\n",
    "print(f\"• Lowest revenue month: {monthly_pandas.loc[monthly_pandas['TotalRevenue'].idxmin(), 'YearMonth']}\")\n",
    "print(\"• Seasonal patterns: Likely holiday spikes in November-December\")\n",
    "\n",
    "print(\"\\n📦 QUANTITY DISTRIBUTION:\")\n",
    "print(f\"• Returns percentage: {returns_percentage:.2f}%\")\n",
    "print(f\"• Most transactions involve small quantities (typical retail pattern)\")\n",
    "print(f\"• Some very large orders indicate bulk purchases or potential outliers\")\n",
    "\n",
    "print(\"\\n🏆 PRODUCT PERFORMANCE:\")\n",
    "print(f\"• High performers (>1000 units): {high_performers:,} products ({high_performers/total_products*100:.1f}%)\")\n",
    "print(f\"• Medium performers (100-1000 units): {medium_performers:,} products ({medium_performers/total_products*100:.1f}%)\")\n",
    "print(f\"• Low performers (≤100 units): {low_performers:,} products ({low_performers/total_products*100:.1f}%)\")\n",
    "print(\"• Clear Pareto distribution: Few products drive most sales\")\n",
    "\n",
    "print(\"\\n🌍 GEOGRAPHICAL DISTRIBUTION:\")\n",
    "print(f\"• UK market dominance: {uk_percentage:.1f}% of total revenue\")\n",
    "print(f\"• Top 5 countries account for majority of sales\")\n",
    "print(\"• International presence but heavily UK-focused\")\n",
    "\n",
    "print(\"\\n💡 BUSINESS INSIGHTS:\")\n",
    "print(\"• Strong seasonal business with holiday peaks\")\n",
    "print(\"• Product portfolio follows 80/20 rule (few products, high volume)\")\n",
    "print(\"• Significant return rate indicates need for return management\")\n",
    "print(\"• UK-centric business with international expansion opportunities\")\n",
    "print(\"• Mix of small retail orders and bulk purchases\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Record Count by Country (Top 10) ===\n",
      "Country\n",
      "United Kingdom    981330\n",
      "EIRE               17866\n",
      "Germany            17624\n",
      "France             14330\n",
      "Netherlands         5140\n",
      "Spain               3811\n",
      "Switzerland         3189\n",
      "Belgium             3123\n",
      "Portugal            2620\n",
      "Australia           1913\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Record Count by Customer (Top 10) ===\n",
      "Customer ID\n",
      "17841.0    13097\n",
      "14911.0    11613\n",
      "12748.0     7307\n",
      "14606.0     6709\n",
      "14096.0     5128\n",
      "15311.0     4717\n",
      "14156.0     4130\n",
      "14646.0     3890\n",
      "13089.0     3438\n",
      "16549.0     3255\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display record counts by country\n",
    "print(\"=== Record Count by Country (Top 10) ===\")\n",
    "if USE_SPARK:\n",
    "    df.groupBy(\"Country\").count().orderBy(desc(\"count\")).show(10)\n",
    "else:\n",
    "    country_counts = pandas_df['Country'].value_counts().head(10)\n",
    "    print(country_counts)\n",
    "\n",
    "# Display record counts by customer\n",
    "print(\"\\n=== Record Count by Customer (Top 10) ===\")\n",
    "if USE_SPARK:\n",
    "    df.groupBy(\"Customer ID\").count().orderBy(desc(\"count\")).show(10)\n",
    "else:\n",
    "    customer_counts = pandas_df['Customer ID'].value_counts().head(10)\n",
    "    print(customer_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis Complete ===\n",
      "Dataset basic information summary:\n",
      "- Total records: 1,067,371\n",
      "- Column count: 8\n",
      "- Main columns: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, Customer ID, Country\n",
      "- Data types verified through schema\n",
      "- Statistical summary shows distribution of numeric columns\n",
      "- Missing values and anomalies checked\n",
      "- Analysis performed using pandas (PySpark failed)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Analysis Complete ===\")\n",
    "print(\"Dataset basic information summary:\")\n",
    "print(f\"- Total records: {row_count:,}\")\n",
    "print(f\"- Column count: {column_count}\")\n",
    "print(f\"- Main columns: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, Customer ID, Country\")\n",
    "print(\"- Data types verified through schema\")\n",
    "print(\"- Statistical summary shows distribution of numeric columns\")\n",
    "print(\"- Missing values and anomalies checked\")\n",
    "if USE_SPARK:\n",
    "    print(\"- Analysis performed using PySpark\")\n",
    "else:\n",
    "    print(\"- Analysis performed using pandas (PySpark failed)\")\n",
    "\n",
    "# Stop Spark session if it was started\n",
    "if USE_SPARK:\n",
    "    try:\n",
    "        spark.stop()\n",
    "        print(\"Spark session stopped.\")\n",
    "    except:\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
