{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Online Retail II Dataset Analysis\n",
    "\n",
    "This notebook demonstrates how to load and analyze the Online Retail II dataset using PySpark in Google Colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n",
    "\n",
    "First, install PySpark and related dependencies in Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install pyspark pandas openpyxl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Initialize Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized successfully!\n",
      "Spark version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, sum as spark_sum, count, when, isnan, isnull, desc, min as spark_min, max as spark_max\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session\n",
    "# Configure Spark for both local and Colab environments\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OnlineRetailAnalysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"300\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce output noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"Spark session initialized successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data from GitHub\n",
    "\n",
    "Since PySpark cannot directly read Excel files, we use pandas to read from GitHub and then convert to Spark DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Excel file from GitHub...\n",
      "Loading data from both sheets (2009-2010 and 2010-2011)...\n",
      "2009-2010 data shape: (525461, 8)\n",
      "2010-2011 data shape: (541910, 8)\n",
      "Combined data shape: (1067371, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:425: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Could not convert 'C489449' with type str: tried to convert to int64\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from GitHub into Spark DataFrame!\n"
     ]
    }
   ],
   "source": [
    "# Use pandas to read Excel file from GitHub\n",
    "print(\"Reading Excel file from GitHub...\")\n",
    "\n",
    "# GitHub repository information\n",
    "github_user = \"Hachi630\"\n",
    "github_repo = \"BDAS\"\n",
    "file_path = \"online_retail_II.xlsx\"\n",
    "\n",
    "# Construct GitHub raw URL\n",
    "github_url = f\"https://raw.githubusercontent.com/{github_user}/{github_repo}/main/{file_path}\"\n",
    "\n",
    "# Read Excel file with multiple sheets\n",
    "print(\"Loading data from both sheets (2009-2010 and 2010-2011)...\")\n",
    "excel_data = pd.read_excel(github_url, sheet_name=None)  # Read all sheets\n",
    "\n",
    "# Get the two sheets\n",
    "sheet_2009_2010 = excel_data['Year 2009-2010']\n",
    "sheet_2010_2011 = excel_data['Year 2010-2011']\n",
    "\n",
    "print(f\"2009-2010 data shape: {sheet_2009_2010.shape}\")\n",
    "print(f\"2010-2011 data shape: {sheet_2010_2011.shape}\")\n",
    "\n",
    "# Combine both datasets\n",
    "pandas_df = pd.concat([sheet_2009_2010, sheet_2010_2011], ignore_index=True)\n",
    "print(f\"Combined data shape: {pandas_df.shape}\")\n",
    "\n",
    "# Convert pandas DataFrame to Spark DataFrame\n",
    "# Ensure DataFrame is named df for consistency\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "print(\"Data successfully loaded from GitHub into Spark DataFrame!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Individual Table Analysis\n",
    "\n",
    "Analyze each table separately before combining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Individual Table Analysis ===\n",
      "\n",
      "--- 2009-2010 Data ---\n",
      "Shape: (525461, 8)\n",
      "Columns: ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country']\n",
      "First 3 rows:\n",
      "  Invoice StockCode                          Description  Quantity  \\\n",
      "0  489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
      "1  489434    79323P                   PINK CHERRY LIGHTS        12   \n",
      "2  489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
      "\n",
      "          InvoiceDate  Price  Customer ID         Country  \n",
      "0 2009-12-01 07:45:00   6.95      13085.0  United Kingdom  \n",
      "1 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "2 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "\n",
      "--- 2010-2011 Data ---\n",
      "Shape: (541910, 8)\n",
      "Columns: ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country']\n",
      "First 3 rows:\n",
      "  Invoice StockCode                         Description  Quantity  \\\n",
      "0  536365    85123A  WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1  536365     71053                 WHITE METAL LANTERN         6   \n",
      "2  536365    84406B      CREAM CUPID HEARTS COAT HANGER         8   \n",
      "\n",
      "          InvoiceDate  Price  Customer ID         Country  \n",
      "0 2010-12-01 08:26:00   2.55      17850.0  United Kingdom  \n",
      "1 2010-12-01 08:26:00   3.39      17850.0  United Kingdom  \n",
      "2 2010-12-01 08:26:00   2.75      17850.0  United Kingdom  \n",
      "\n",
      "--- Combined Data Summary ---\n",
      "Total records: 1,067,371\n",
      "Total columns: 8\n",
      "Columns: ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country']\n"
     ]
    }
   ],
   "source": [
    "# Analyze individual tables\n",
    "print(\"=== Individual Table Analysis ===\")\n",
    "\n",
    "print(\"\\n--- 2009-2010 Data ---\")\n",
    "print(f\"Shape: {sheet_2009_2010.shape}\")\n",
    "print(f\"Columns: {list(sheet_2009_2010.columns)}\")\n",
    "print(\"First 3 rows:\")\n",
    "print(sheet_2009_2010.head(3))\n",
    "\n",
    "print(\"\\n--- 2010-2011 Data ---\")\n",
    "print(f\"Shape: {sheet_2010_2011.shape}\")\n",
    "print(f\"Columns: {list(sheet_2010_2011.columns)}\")\n",
    "print(\"First 3 rows:\")\n",
    "print(sheet_2010_2011.head(3))\n",
    "\n",
    "print(\"\\n--- Combined Data Summary ---\")\n",
    "print(f\"Total records: {len(pandas_df):,}\")\n",
    "print(f\"Total columns: {len(pandas_df.columns)}\")\n",
    "print(f\"Columns: {list(pandas_df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check Data Dimensions\n",
    "\n",
    "Determine the number of rows and columns in the combined dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Dimension Information ===\n",
      "Error getting row count with Spark: An error occurred while calling o150.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 4.0 failed 1 times, most recent failure: Lost task 2.0 in stage 4.0 (TID 82) (windows10.microdone.cn executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\n",
      "\t... 34 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\n",
      "\t... 34 more\n",
      "\n",
      "Using pandas DataFrame for all analysis...\n",
      "Dataset row count (from pandas): 1,067,371\n",
      "Dataset column count: 8\n",
      "Column names: ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country']\n"
     ]
    }
   ],
   "source": [
    "# Check data dimensions with error handling\n",
    "print(\"=== Data Dimension Information ===\")\n",
    "\n",
    "# Get row count with error handling\n",
    "try:\n",
    "    row_count = df.count()\n",
    "    print(f\"Dataset row count: {row_count:,}\")\n",
    "    USE_SPARK = True\n",
    "except Exception as e:\n",
    "    print(f\"Error getting row count with Spark: {e}\")\n",
    "    print(\"Using pandas DataFrame for all analysis...\")\n",
    "    row_count = len(pandas_df)\n",
    "    print(f\"Dataset row count (from pandas): {row_count:,}\")\n",
    "    USE_SPARK = False\n",
    "\n",
    "# Get column count\n",
    "if USE_SPARK:\n",
    "    column_count = len(df.columns)\n",
    "    column_names = df.columns\n",
    "else:\n",
    "    column_count = len(pandas_df.columns)\n",
    "    column_names = list(pandas_df.columns)\n",
    "\n",
    "print(f\"Dataset column count: {column_count}\")\n",
    "print(f\"Column names: {column_names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Analysis with Error Handling\n",
    "\n",
    "The following analysis will automatically detect if PySpark is working and use the appropriate method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Preview (First 5 Rows) ===\n",
      "  Invoice StockCode                          Description  Quantity  \\\n",
      "0  489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
      "1  489434    79323P                   PINK CHERRY LIGHTS        12   \n",
      "2  489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
      "3  489434     22041         RECORD FRAME 7\" SINGLE SIZE         48   \n",
      "4  489434     21232       STRAWBERRY CERAMIC TRINKET BOX        24   \n",
      "\n",
      "          InvoiceDate  Price  Customer ID         Country  \n",
      "0 2009-12-01 07:45:00   6.95      13085.0  United Kingdom  \n",
      "1 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "2 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "3 2009-12-01 07:45:00   2.10      13085.0  United Kingdom  \n",
      "4 2009-12-01 07:45:00   1.25      13085.0  United Kingdom  \n",
      "\n",
      "=== Data Schema ===\n",
      "Invoice                object\n",
      "StockCode              object\n",
      "Description            object\n",
      "Quantity                int64\n",
      "InvoiceDate    datetime64[ns]\n",
      "Price                 float64\n",
      "Customer ID           float64\n",
      "Country                object\n",
      "dtype: object\n",
      "\n",
      "=== Numeric Columns Statistical Summary ===\n",
      "           Quantity                    InvoiceDate         Price  \\\n",
      "count  1.067371e+06                        1067371  1.067371e+06   \n",
      "mean   9.938898e+00  2011-01-02 21:13:55.394028544  4.649388e+00   \n",
      "min   -8.099500e+04            2009-12-01 07:45:00 -5.359436e+04   \n",
      "25%    1.000000e+00            2010-07-09 09:46:00  1.250000e+00   \n",
      "50%    3.000000e+00            2010-12-07 15:28:00  2.100000e+00   \n",
      "75%    1.000000e+01            2011-07-22 10:23:00  4.150000e+00   \n",
      "max    8.099500e+04            2011-12-09 12:50:00  3.897000e+04   \n",
      "std    1.727058e+02                            NaN  1.235531e+02   \n",
      "\n",
      "         Customer ID  \n",
      "count  824364.000000  \n",
      "mean    15324.638504  \n",
      "min     12346.000000  \n",
      "25%     13975.000000  \n",
      "50%     15255.000000  \n",
      "75%     16797.000000  \n",
      "max     18287.000000  \n",
      "std      1697.464450  \n",
      "\n",
      "=== Missing Values Check ===\n",
      "Invoice             0\n",
      "StockCode           0\n",
      "Description      4382\n",
      "Quantity            0\n",
      "InvoiceDate         0\n",
      "Price               0\n",
      "Customer ID    243007\n",
      "Country             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preview data - show first 5 rows\n",
    "print(\"=== Data Preview (First 5 Rows) ===\")\n",
    "if USE_SPARK:\n",
    "    df.show(5, truncate=False)\n",
    "else:\n",
    "    print(pandas_df.head())\n",
    "\n",
    "# Print data schema to verify data types\n",
    "print(\"\\n=== Data Schema ===\")\n",
    "if USE_SPARK:\n",
    "    df.printSchema()\n",
    "else:\n",
    "    print(pandas_df.dtypes)\n",
    "\n",
    "# Display basic statistical summary for numeric columns\n",
    "print(\"\\n=== Numeric Columns Statistical Summary ===\")\n",
    "if USE_SPARK:\n",
    "    df.describe().show()\n",
    "else:\n",
    "    print(pandas_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n=== Missing Values Check ===\")\n",
    "if USE_SPARK:\n",
    "    missing_values = df.select([spark_sum(when(isnull(c) | isnan(c), 1).otherwise(0)).alias(c) for c in df.columns])\n",
    "    missing_values.show()\n",
    "else:\n",
    "    missing_values = pandas_df.isnull().sum()\n",
    "    print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Quantity Column Analysis ===\n",
      "Min Quantity: -80995\n",
      "Max Quantity: 80995\n",
      "Return Records Count: 22950\n",
      "Normal Sales Records Count: 1044421\n",
      "\n",
      "=== UnitPrice Column Analysis ===\n",
      "Min Unit Price: -53594.36\n",
      "Max Unit Price: 38970.0\n",
      "Negative Price Records Count: 5\n",
      "Zero Price Records Count: 6202\n"
     ]
    }
   ],
   "source": [
    "# Check negative values in Quantity column (returns)\n",
    "print(\"=== Quantity Column Analysis ===\")\n",
    "if USE_SPARK:\n",
    "    quantity_stats = df.select(\n",
    "        spark_min(\"Quantity\").alias(\"Min Quantity\"),\n",
    "        spark_max(\"Quantity\").alias(\"Max Quantity\"),\n",
    "        count(when(col(\"Quantity\") < 0, 1)).alias(\"Return Records Count\"),\n",
    "        count(when(col(\"Quantity\") > 0, 1)).alias(\"Normal Sales Records Count\")\n",
    "    )\n",
    "    quantity_stats.show()\n",
    "else:\n",
    "    quantity_stats = {\n",
    "        \"Min Quantity\": pandas_df['Quantity'].min(),\n",
    "        \"Max Quantity\": pandas_df['Quantity'].max(),\n",
    "        \"Return Records Count\": (pandas_df['Quantity'] < 0).sum(),\n",
    "        \"Normal Sales Records Count\": (pandas_df['Quantity'] > 0).sum()\n",
    "    }\n",
    "    for key, value in quantity_stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Check UnitPrice column range\n",
    "print(\"\\n=== UnitPrice Column Analysis ===\")\n",
    "if USE_SPARK:\n",
    "    price_stats = df.select(\n",
    "        spark_min(\"UnitPrice\").alias(\"Min Unit Price\"),\n",
    "        spark_max(\"UnitPrice\").alias(\"Max Unit Price\"),\n",
    "        count(when(col(\"UnitPrice\") < 0, 1)).alias(\"Negative Price Records Count\"),\n",
    "        count(when(col(\"UnitPrice\") == 0, 1)).alias(\"Zero Price Records Count\")\n",
    "    )\n",
    "    price_stats.show()\n",
    "else:\n",
    "    price_stats = {\n",
    "        \"Min Unit Price\": pandas_df['Price'].min(),\n",
    "        \"Max Unit Price\": pandas_df['Price'].max(),\n",
    "        \"Negative Price Records Count\": (pandas_df['Price'] < 0).sum(),\n",
    "        \"Zero Price Records Count\": (pandas_df['Price'] == 0).sum()\n",
    "    }\n",
    "    for key, value in price_stats.items():\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Record Count by Country (Top 10) ===\n",
      "Country\n",
      "United Kingdom    981330\n",
      "EIRE               17866\n",
      "Germany            17624\n",
      "France             14330\n",
      "Netherlands         5140\n",
      "Spain               3811\n",
      "Switzerland         3189\n",
      "Belgium             3123\n",
      "Portugal            2620\n",
      "Australia           1913\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Record Count by Customer (Top 10) ===\n",
      "Customer ID\n",
      "17841.0    13097\n",
      "14911.0    11613\n",
      "12748.0     7307\n",
      "14606.0     6709\n",
      "14096.0     5128\n",
      "15311.0     4717\n",
      "14156.0     4130\n",
      "14646.0     3890\n",
      "13089.0     3438\n",
      "16549.0     3255\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display record counts by country\n",
    "print(\"=== Record Count by Country (Top 10) ===\")\n",
    "if USE_SPARK:\n",
    "    df.groupBy(\"Country\").count().orderBy(desc(\"count\")).show(10)\n",
    "else:\n",
    "    country_counts = pandas_df['Country'].value_counts().head(10)\n",
    "    print(country_counts)\n",
    "\n",
    "# Display record counts by customer\n",
    "print(\"\\n=== Record Count by Customer (Top 10) ===\")\n",
    "if USE_SPARK:\n",
    "    df.groupBy(\"Customer ID\").count().orderBy(desc(\"count\")).show(10)\n",
    "else:\n",
    "    customer_counts = pandas_df['Customer ID'].value_counts().head(10)\n",
    "    print(customer_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis Complete ===\n",
      "Dataset basic information summary:\n",
      "- Total records: 1,067,371\n",
      "- Column count: 8\n",
      "- Main columns: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, Customer ID, Country\n",
      "- Data types verified through schema\n",
      "- Statistical summary shows distribution of numeric columns\n",
      "- Missing values and anomalies checked\n",
      "- Analysis performed using pandas (PySpark failed)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Analysis Complete ===\")\n",
    "print(\"Dataset basic information summary:\")\n",
    "print(f\"- Total records: {row_count:,}\")\n",
    "print(f\"- Column count: {column_count}\")\n",
    "print(f\"- Main columns: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, Customer ID, Country\")\n",
    "print(\"- Data types verified through schema\")\n",
    "print(\"- Statistical summary shows distribution of numeric columns\")\n",
    "print(\"- Missing values and anomalies checked\")\n",
    "if USE_SPARK:\n",
    "    print(\"- Analysis performed using PySpark\")\n",
    "else:\n",
    "    print(\"- Analysis performed using pandas (PySpark failed)\")\n",
    "\n",
    "# Stop Spark session if it was started\n",
    "if USE_SPARK:\n",
    "    try:\n",
    "        spark.stop()\n",
    "        print(\"Spark session stopped.\")\n",
    "    except:\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
